{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "# 特征字典，后面用到了好多次，干脆当全局变量了\n",
    "featureDic = {\n",
    "    '色泽': ['浅白', '青绿', '乌黑'],\n",
    "    '根蒂': ['硬挺', '蜷缩', '稍蜷'],\n",
    "    '敲声': ['沉闷', '浊响', '清脆'],\n",
    "    '纹理': ['清晰', '模糊', '稍糊'],\n",
    "    '脐部': ['凹陷', '平坦', '稍凹'],\n",
    "    '触感': ['硬滑', '软粘']}\n",
    "\n",
    "\n",
    "def getDataSet():\n",
    "    \"\"\"\n",
    "    get watermelon data set 3.0 alpha.\n",
    "    :return: 编码好的数据集以及特征的字典。\n",
    "    \"\"\"\n",
    "    dataSet = [\n",
    "        ['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460, 1],\n",
    "        ['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376, 1],\n",
    "        ['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.634, 0.264, 1],\n",
    "        ['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.608, 0.318, 1],\n",
    "        ['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.556, 0.215, 1],\n",
    "        ['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘', 0.403, 0.237, 1],\n",
    "        ['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘', 0.481, 0.149, 1],\n",
    "        ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑', 0.437, 0.211, 1],\n",
    "        ['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', 0.666, 0.091, 0],\n",
    "        ['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘', 0.243, 0.267, 0],\n",
    "        ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057, 0],\n",
    "        ['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘', 0.343, 0.099, 0],\n",
    "        ['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑', 0.639, 0.161, 0],\n",
    "        ['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', 0.657, 0.198, 0],\n",
    "        ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘', 0.360, 0.370, 0],\n",
    "        ['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑', 0.593, 0.042, 0],\n",
    "        ['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑', 0.719, 0.103, 0]\n",
    "    ]\n",
    "\n",
    "    features = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖量']\n",
    "    # features = ['color', 'root', 'knocks', 'texture', 'navel', 'touch', 'density', 'sugar']\n",
    "\n",
    "    # #得到特征值字典，本来用这个生成的特征字典，还是直接当全局变量方便\n",
    "    # featureDic = {}\n",
    "    # for i in range(len(features)):\n",
    "    #     featureList = [example[i] for example in dataSet]\n",
    "    #     uniqueFeature = list(set(featureList))\n",
    "    #     featureDic[features[i]] = uniqueFeature\n",
    "\n",
    "    # 每种特征的属性个数\n",
    "    numList = []  # [3, 3, 3, 3, 3, 2]\n",
    "    for i in range(len(features) - 2):\n",
    "        numList.append(len(featureDic[features[i]]))\n",
    "\n",
    "    dataSet = np.array(dataSet)\n",
    "    return dataSet[:, :-1], dataSet[:, -1], features\n",
    "\n",
    "\n",
    "# data, classLabel, feature = getDataSet()\n",
    "# print(data)\n",
    "# print(classLabel)\n",
    "# print(feature)\n",
    "\n",
    "\n",
    "def newData():\n",
    "    \"\"\"\n",
    "    利用pandas将分类变量转化为数值变量。将分类变量进行one-hot编码。\n",
    "    :return: 变量全为数值的变量，以及新的特征标签。\n",
    "    \"\"\"\n",
    "    dataSet, classLabel, features = getDataSet()\n",
    "    df = pd.DataFrame(dataSet)\n",
    "    df.columns = features\n",
    "    # 类别变量转化为数字变量\n",
    "    # features = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖量']\n",
    "    # features = ['color', 'root', 'knocks', 'texture', 'navel', 'touch', 'density', 'sugar']\n",
    "    # 色泽\n",
    "    color = pd.get_dummies(df.色泽, prefix=\"色泽\")\n",
    "    # 根蒂\n",
    "    root = pd.get_dummies(df.根蒂, prefix=\"根蒂\")\n",
    "    # 敲声\n",
    "    knocks = pd.get_dummies(df.敲声, prefix=\"敲声\")\n",
    "    # 纹理\n",
    "    texture = pd.get_dummies(df.纹理, prefix=\"纹理\")\n",
    "    # 脐部\n",
    "    navel = pd.get_dummies(df.脐部, prefix=\"脐部\")\n",
    "    # 触感\n",
    "    touch = pd.get_dummies(df.触感, prefix=\"触感\")\n",
    "    # 密度和含糖量\n",
    "    densityAndsugar = pd.DataFrame()\n",
    "    densityAndsugar[\"密度\"] = df.密度\n",
    "    densityAndsugar[\"含糖量\"] = df.含糖量\n",
    "    # 融合\n",
    "    newData = pd.concat([color, root, knocks, texture, navel, touch, densityAndsugar], axis=1)\n",
    "    # print(\"newData\", newData)\n",
    "    newFeatures = list(newData.columns)\n",
    "    newData = np.asarray(newData, dtype=\"float64\")\n",
    "    classLabel = np.asarray(classLabel, dtype=\"int\").reshape(-1, 1)\n",
    "\n",
    "    # 新的特征数据和类融合\n",
    "    newDataSet = np.concatenate((newData, classLabel), axis=1)\n",
    "    # # 在第一列添加1\n",
    "    # newDataSet = np.insert(newDataSet, 0,\n",
    "    #                        np.ones(dataSet.shape[0]),\n",
    "    #                        axis=1)\n",
    "\n",
    "    return newDataSet, newFeatures\n",
    "\n",
    "\n",
    "# Sigmoid 函数\n",
    "def sigmoid(Z):\n",
    "    return 1.0 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "# 神经网络累计BP\n",
    "def NNetworkBP(dataSet, eta, thresh):\n",
    "    \"\"\"\n",
    "    :param dataSet: 数据集. m x n\n",
    "    :param eta: 学习率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    errHistory = []     # 记录每轮迭代的均方误差\n",
    "    y = dataSet[:, -1].reshape(-1, 1)\n",
    "    x = dataSet[:, :-1]\n",
    "    m, n = x.shape\n",
    "    # 隐层参数\n",
    "    v = np.random.randn(n, n + 1)\n",
    "    # 输出层参数\n",
    "    w = np.random.randn(n + 1, 1)\n",
    "    # 隐层阈值\n",
    "    gamma = np.random.randn(1, n + 1)\n",
    "    # 输出值\n",
    "    theta = np.random.random(1)\n",
    "\n",
    "    err = errOfMeanSqur(dataSet, v, gamma, w, theta)\n",
    "    while err > thresh:\n",
    "        b = sigmoid(np.dot(x, v) - gamma)  # m x (n+1)\n",
    "        beta = np.dot(b, w)     # m x 1\n",
    "        # 预测值\n",
    "        yHat = sigmoid(beta - theta)    # m x 1\n",
    "        # 输出层神经元梯度项\n",
    "        g = yHat * (1 - yHat) * (y - yHat)  # m x 1\n",
    "        # 隐层神经元梯度向\n",
    "        e = b * (1 - b) * np.dot(g, w.T)    # m x (n+1)\n",
    "        # 更新w, v, theta, gamma\n",
    "        w += eta * np.dot(b.T, g)\n",
    "        v += eta * np.dot(x.T, e)\n",
    "        theta -= eta * g.sum()\n",
    "        gamma -= eta * e.sum(axis=0)\n",
    "\n",
    "        err = errOfMeanSqur(dataSet, v, gamma, w, theta)\n",
    "        errHistory.append(err)\n",
    "\n",
    "    return v, gamma, w, theta, errHistory\n",
    "\n",
    "\n",
    "# 神经网络累计BP\n",
    "def NNetworkABP(dataSet, eta, thresh):\n",
    "    \"\"\"\n",
    "    :param dataSet: 数据集. m x n\n",
    "    :param eta: 学习率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    errHistory = []     # 记录每轮迭代的均方误差\n",
    "    y = dataSet[:, -1].reshape(-1, 1)\n",
    "    x = dataSet[:, :-1]\n",
    "    m, n = x.shape\n",
    "    # 隐层参数\n",
    "    v = np.random.randn(n, n + 1)\n",
    "    # 输出层参数\n",
    "    w = np.random.randn(n + 1, 1)\n",
    "    # 隐层阈值\n",
    "    gamma = np.random.randn(1, n + 1)\n",
    "    # 输出值\n",
    "    theta = np.random.random(1)\n",
    "    epoch = 0\n",
    "\n",
    "    err = errOfMeanSqur(dataSet, v, gamma, w, theta)\n",
    "    while err > thresh:\n",
    "        for i in range(m):\n",
    "            b = sigmoid(np.dot(x[i], v) - gamma)  # 1 x (n+1)\n",
    "            beta = np.dot(b, w)[0]     # 1\n",
    "            # 预测值\n",
    "            yHat = sigmoid(beta - theta)  # 1\n",
    "            # 输出层神经元梯度项\n",
    "            g = yHat * (1 - yHat) * (y[i] - yHat)  # 1\n",
    "            print(\"g = \", g)\n",
    "            # 隐层神经元梯度向\n",
    "            e = b * (1 - b) * g * w.T.sum()   # 1 x (n+1)\n",
    "            # 更新w, v, theta, gamma\n",
    "            w += eta * b.T * g\n",
    "            v += eta * np.dot(x[i].reshape(n, -1), e)\n",
    "            theta -= eta * g\n",
    "            gamma -= eta * e\n",
    "            epoch += 1\n",
    "            err = errOfMeanSqur(dataSet, v, gamma, w, theta)\n",
    "            errHistory.append(err)\n",
    "\n",
    "    return v, gamma, w, theta, errHistory\n",
    "\n",
    "\n",
    "def classify(data, v, gamma, w, theta):\n",
    "    b = sigmoid(np.dot(data, v) - gamma)\n",
    "    beta = np.dot(b, w)\n",
    "    yHat = sigmoid(beta - theta)\n",
    "    return yHat[0][0]\n",
    "\n",
    "\n",
    "def errOfMeanSqur(dataSet, v, gamma, w, theta):\n",
    "    x = dataSet[:, :-1]\n",
    "    y = dataSet[:, -1]\n",
    "    num = x.shape[0]\n",
    "    err = 0.0\n",
    "    for i in range(num):\n",
    "        yPre = classify(dataSet[i][:-1], v, gamma, w, theta)\n",
    "        err += ((y[i] - yPre) ** 2) / 2.0\n",
    "\n",
    "    return err / float(num)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # # test NNetwork\n",
    "    dataSet, _ = newData()\n",
    "    print(dataSet)\n",
    "    v1, gamma1, w1, theta1, errHistory1 = NNetworkBP(dataSet, 0.1, 0.001)\n",
    "    # 画图\n",
    "    plt.plot(np.arange(len(errHistory1)), errHistory1)\n",
    "    plt.show()\n",
    "    # test NNetworkABP(dataSet, eta, thresh)\n",
    "    v2, gamma2, w2, theta2, errHistory2 = NNetworkABP(dataSet, 0.1, 0.001)\n",
    "    plt.plot(np.arange(len(errHistory2)), errHistory2)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#西瓜数据集 每一列为一条数据\n",
    "features=np.array([\n",
    "    [1,2,2,1,0,1,2,2,2,1,0,0,1,0,2,0,1],\n",
    "    [2,2,2,2,2,1,1,1,1,0,0,2,1,1,1,2,2],\n",
    "    [1,0,1,0,1,1,1,1,0,2,2,1,1,0,1,1,0],\n",
    "    [0,0,0,0,0,0,1,0,1,0,2,2,1,1,0,2,1],\n",
    "    [2,2,2,2,2,1,1,1,1,0,0,0,2,2,1,0,1],\n",
    "    [1,1,1,1,1,0,0,1,1,0,1,0,1,1,0,1,1],\n",
    "    [0.697,0.774,0.634,0.608,0.556,0.403,0.481,0.437,0.666,0.243,0.245,0.343,0.639,0.657,0.360,0.593,0.719],\n",
    "    [0.460,0.376,0.264,0.318,0.215,0.237,0.149,0.211,0.091,0.267,0.057,0.099,0.161,0.198,0.370,0.042,0.103]\n",
    "])\n",
    "labels=np.array([\n",
    "    [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0]\n",
    "])\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1./(1+np.exp(-X))\n",
    "class Net():\n",
    "    def __init__(self,num_input=8,num_hidden=10,num_output=1):\n",
    "        #隐含层和输出层的权重和偏置\n",
    "        self.W1=np.random.randn(num_hidden,num_input)\n",
    "        self.b1=np.zeros(num_hidden).reshape(-1,1)\n",
    "        self.W2=np.random.randn(num_output,num_hidden)\n",
    "        self.b2=np.zeros(num_output).reshape(-1,1)\n",
    "        #隐含层和输出层的输出\n",
    "        self.o1=np.zeros(num_hidden).reshape(-1,1)\n",
    "        self.o2=np.zeros(num_output).reshape(-1,1)\n",
    "        #梯度存储变量\n",
    "        self.do2=np.zeros(self.o2.shape)\n",
    "        self.dW2=np.zeros(self.W2.shape)\n",
    "        self.db2=np.zeros(self.b2.shape)\n",
    "        self.do1=np.zeros(self.o1.shape)\n",
    "        self.dW1=np.zeros(self.W1.shape)\n",
    "        self.db1=np.zeros(self.b1.shape)\n",
    "    def forward(self,X):#前向传播\n",
    "        if X.shape[0] != self.W1.shape[1]:\n",
    "            print(\"输入数据格式错误！\")\n",
    "            return\n",
    "        self.input=X\n",
    "        #使用sigmoid函数为激活函数\n",
    "        self.o1=sigmoid(np.matmul(self.W1,self.input)+self.b1)\n",
    "        self.o2=sigmoid(np.matmul(self.W2,self.o1)+self.b2)\n",
    "        return self.o2\n",
    "    def standard_BP(self,label,lr=0.2):#标准BP 使用均方误差为损失函数\n",
    "        #求梯度\n",
    "        self.do2=self.o2-label\n",
    "        self.dW2=np.matmul(self.do2*self.o2*(1-self.o2),self.o1.reshape(1,-1))\n",
    "        self.db2=self.do2*self.o2*(1-self.o2)\n",
    "        self.do1=np.matmul(self.W2.transpose(),self.do2*self.o2*(1-self.o2))\n",
    "        self.dW1=np.matmul(self.do1*self.o1*(1-self.o1),self.input.reshape(1,-1))\n",
    "        self.db1=self.do1*self.o1*(1-self.o1)\n",
    "        #更新参数\n",
    "        self.W2-=self.dW2*lr\n",
    "        self.b2-=self.db2*lr\n",
    "        self.W1-=self.dW1*lr\n",
    "        self.b1-=self.db1*lr\n",
    "    def accumulate_BP(self,labels,lr=0.2):#累积BP 使用均方误差为损失函数\n",
    "        num=labels.shape[1]#样本数量\n",
    "        #求梯度\n",
    "        self.do2=(self.o2-labels)/num\n",
    "        self.dW2=np.matmul(self.do2*self.o2*(1-self.o2),self.o1.transpose())\n",
    "        self.db2=(self.do2*self.o2*(1-self.o2)).sum(axis=1).reshape(-1,1)\n",
    "        self.do1=np.matmul(self.W2.transpose(),self.do2*self.o2*(1-self.o2))\n",
    "        self.dW1=np.matmul(self.do1*self.o1*(1-self.o1),self.input.transpose())\n",
    "        self.db1=(self.do1*self.o1*(1-self.o1)).sum(axis=1).reshape(-1,1)\n",
    "        #更新参数\n",
    "        self.W2-=self.dW2*lr\n",
    "        self.b2-=self.db2*lr\n",
    "        self.W1-=self.dW1*lr\n",
    "        self.b1-=self.db1*lr\n",
    "        \n",
    "def train_standard_BP(features,labels,lr):\n",
    "    net=Net()\n",
    "    epoch=0\n",
    "    loss=1\n",
    "    all_loss=[]\n",
    "    while loss>0.01:#停止条件\n",
    "        for i in range(features.shape[1]):\n",
    "            X=features[:,i]\n",
    "            Y=labels[0,i]\n",
    "            net.forward(X.reshape(-1,1))\n",
    "            net.standard_BP(Y,lr)\n",
    "        output=net.forward(features)\n",
    "        loss=0.5*((output-labels)**2).sum()\n",
    "        epoch+=1\n",
    "        all_loss.append(loss)\n",
    "    print(\"标准BP\",\"学习率：\",lr,\"\\n终止epoch：\",epoch,\"loss: \",loss)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(all_loss)\n",
    "    plt.show()\n",
    "    \n",
    "def train_accumulate_BP(features,labels,lr=0.2):\n",
    "    net=Net()\n",
    "    epoch=0\n",
    "    loss=1\n",
    "    all_loss=[]\n",
    "    while loss>0.01:#停止条件\n",
    "        output=net.forward(features)\n",
    "        net.accumulate_BP(labels,lr)\n",
    "        loss=0.5*((output-labels)**2).sum()/labels.shape[1]\n",
    "        epoch+=1\n",
    "        all_loss.append(loss)\n",
    "    print(\"累积BP\",\"学习率：\",lr,\"\\n终止epoch：\",epoch,\"loss: \",loss)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(all_loss)\n",
    "    plt.show()\n",
    "\n",
    "train_standard_BP(features,labels,0.1)\n",
    "train_accumulate_BP(features,labels,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, err = 0.24035770334189063\n",
      "iteration 2, err = 0.21314513140172167\n",
      "iteration 3, err = 0.9209665675825492\n",
      "iteration 4, err = 1.1325482127620916\n",
      "iteration 5, err = 3.354371285374258\n",
      "iteration 6, err = 0.44841301704241027\n",
      "iteration 7, err = 0.14678920068668103\n",
      "iteration 8, err = 0.2612877258604261\n",
      "iteration 9, err = 0.4468169688658819\n",
      "iteration 10, err = 0.3730392266396577\n",
      "iteration 11, err = 0.3460023878634557\n",
      "iteration 12, err = 0.38254500554208115\n",
      "iteration 13, err = 0.6725283403742343\n",
      "iteration 14, err = 0.15953517985266605\n",
      "iteration 15, err = 0.3998241138887494\n",
      "iteration 16, err = 0.38122613117530313\n",
      "iteration 17, err = 0.5557053098928897\n",
      "iteration 18, err = 0.39483027287540273\n",
      "iteration 19, err = 0.37676356957620905\n",
      "iteration 20, err = 0.1646766482111876\n",
      "iteration 21, err = 0.2579386190689702\n",
      "iteration 22, err = 0.3955746666336729\n",
      "iteration 23, err = 0.36952187752322024\n",
      "iteration 24, err = 0.3703611322811474\n",
      "iteration 25, err = 0.2134955938884238\n",
      "iteration 26, err = 0.38562849029734153\n",
      "iteration 27, err = 0.4969238000338829\n",
      "iteration 28, err = 0.16821114653459368\n",
      "iteration 29, err = 0.3636155344958656\n",
      "iteration 30, err = 0.3680992125064419\n",
      "iteration 31, err = 0.4325443106610063\n",
      "iteration 32, err = 0.3638259338744924\n",
      "iteration 33, err = 0.3433311128229119\n",
      "iteration 34, err = 0.3558752946249978\n",
      "iteration 35, err = 0.32974158234244866\n",
      "iteration 36, err = 0.3552531378487022\n",
      "iteration 37, err = 0.3795410072245183\n",
      "iteration 38, err = 0.30981175535687044\n",
      "iteration 39, err = 0.3198456011662404\n",
      "iteration 40, err = 0.2763117679099999\n",
      "iteration 41, err = 0.27832048239003737\n",
      "iteration 42, err = 0.257725965552716\n",
      "iteration 43, err = 0.27506661955429335\n",
      "iteration 44, err = 0.28569278194876985\n",
      "iteration 45, err = 0.31470690828813197\n",
      "iteration 46, err = 0.2776639248768048\n",
      "iteration 47, err = 0.45608743351936504\n",
      "iteration 48, err = 0.1456698526822497\n",
      "iteration 49, err = 0.2630413476118902\n",
      "iteration 50, err = 0.2777869271388392\n",
      "iteration 51, err = 0.2786579753027437\n",
      "iteration 52, err = 0.25434620621342247\n",
      "iteration 53, err = 0.16218080319093514\n",
      "iteration 54, err = 0.18530885260337765\n",
      "iteration 55, err = 0.30523309614618865\n",
      "iteration 56, err = 0.34994137307262024\n",
      "iteration 57, err = 0.35413674297761133\n",
      "iteration 58, err = 0.2310236349030139\n",
      "iteration 59, err = 0.3527277654228594\n",
      "iteration 60, err = 0.17769124491581903\n",
      "iteration 61, err = 0.2079007921086733\n",
      "iteration 62, err = 0.32382459490022647\n",
      "iteration 63, err = 0.1922013774073979\n",
      "iteration 64, err = 0.30066679417185205\n",
      "iteration 65, err = 0.25700251080061953\n",
      "iteration 66, err = 0.3507853074119668\n",
      "iteration 67, err = 0.36389187107101534\n",
      "iteration 68, err = 0.26295132621546324\n",
      "iteration 69, err = 0.1905964482578365\n",
      "iteration 70, err = 0.28419299644861173\n",
      "iteration 71, err = 0.23804026474919682\n",
      "iteration 72, err = 0.23459195412149733\n",
      "iteration 73, err = 0.2305973102830176\n",
      "iteration 74, err = 0.1971933165048992\n",
      "iteration 75, err = 0.3136381340087813\n",
      "iteration 76, err = 0.23069020658597023\n",
      "iteration 77, err = 0.22412970282041778\n",
      "iteration 78, err = 0.19005558922589622\n",
      "iteration 79, err = 0.19466997558366572\n",
      "iteration 80, err = 0.2701246257299927\n",
      "iteration 81, err = 0.3424265117455419\n",
      "iteration 82, err = 0.21943411408157806\n",
      "iteration 83, err = 0.3604231378276199\n",
      "iteration 84, err = 0.4044303484272189\n",
      "iteration 85, err = 0.41527389092872885\n",
      "iteration 86, err = 0.2066288360259393\n",
      "iteration 87, err = 0.25994249298908717\n",
      "iteration 88, err = 0.2025052768021458\n",
      "iteration 89, err = 0.36868508661749944\n",
      "iteration 90, err = 0.17180322532798187\n",
      "iteration 91, err = 0.18851266492761132\n",
      "iteration 92, err = 0.33398491019824283\n",
      "iteration 93, err = 0.23249608173886507\n",
      "iteration 94, err = 0.19746801121271254\n",
      "iteration 95, err = 0.24875525482894426\n",
      "iteration 96, err = 0.32730260699237673\n",
      "iteration 97, err = 0.3437704330102418\n",
      "iteration 98, err = 0.19103048155347388\n",
      "iteration 99, err = 0.2090691527857253\n",
      "iteration 100, err = 0.3148117404015911\n",
      "iteration 101, err = 0.212291124263519\n",
      "iteration 102, err = 0.33928484747365656\n",
      "iteration 103, err = 0.18442731062957718\n",
      "iteration 104, err = 0.24162071601938817\n",
      "iteration 105, err = 0.18748764611809418\n",
      "iteration 106, err = 0.3227147610464251\n",
      "iteration 107, err = 0.18609476778801323\n",
      "iteration 108, err = 0.3054780200107617\n",
      "iteration 109, err = 0.19568907528697288\n",
      "iteration 110, err = 0.30733742339192194\n",
      "iteration 111, err = 0.27986558818834095\n",
      "iteration 112, err = 0.33177223307338183\n",
      "iteration 113, err = 0.18182138422979194\n",
      "iteration 114, err = 0.1818142328801081\n",
      "iteration 115, err = 0.23163279710067156\n",
      "iteration 116, err = 0.3123586716980763\n",
      "iteration 117, err = 0.17291109182215983\n",
      "iteration 118, err = 0.32655307867138106\n",
      "iteration 119, err = 0.3891210295818157\n",
      "iteration 120, err = 0.18162250908998798\n",
      "iteration 121, err = 0.18303230080577143\n",
      "iteration 122, err = 0.22418059723991413\n",
      "iteration 123, err = 0.305891313446466\n",
      "iteration 124, err = 0.32765425239716545\n",
      "iteration 125, err = 0.17960804868223856\n",
      "iteration 126, err = 0.22185261244250543\n",
      "iteration 127, err = 0.17582602807752376\n",
      "iteration 128, err = 0.18360863733523\n",
      "iteration 129, err = 0.21475131115481777\n",
      "iteration 130, err = 0.2952445072263317\n",
      "iteration 131, err = 0.1656452377433136\n",
      "iteration 132, err = 0.2991579975816056\n",
      "iteration 133, err = 0.18564421242933815\n",
      "iteration 134, err = 0.27837252682191055\n",
      "iteration 135, err = 0.27506412411072306\n",
      "iteration 136, err = 0.3057941862759303\n",
      "iteration 137, err = 0.15871553960357698\n",
      "iteration 138, err = 0.197478091311358\n",
      "iteration 139, err = 0.15624483297247116\n",
      "iteration 140, err = 0.1946591895633853\n",
      "iteration 141, err = 0.24186889954931864\n",
      "iteration 142, err = 0.20379217745289244\n",
      "iteration 143, err = 0.182387547512114\n",
      "iteration 144, err = 0.15573333797092664\n",
      "iteration 145, err = 0.2099997277583797\n",
      "iteration 146, err = 0.15102465936583098\n",
      "iteration 147, err = 0.31016506036306773\n",
      "iteration 148, err = 0.1716445266586076\n",
      "iteration 149, err = 0.18445951798893753\n",
      "iteration 150, err = 0.23614575149955389\n",
      "iteration 151, err = 0.25018763401711935\n",
      "iteration 152, err = 0.14857477196639127\n",
      "iteration 153, err = 0.15858525934225706\n",
      "iteration 154, err = 0.1594815017840602\n",
      "iteration 155, err = 0.20436907533506138\n",
      "iteration 156, err = 0.2778813699568238\n",
      "iteration 157, err = 0.30153100852827186\n",
      "iteration 158, err = 0.18598139966287394\n",
      "iteration 159, err = 0.31213167572985284\n",
      "iteration 160, err = 0.17974772198415734\n",
      "iteration 161, err = 0.17204612885100146\n",
      "iteration 162, err = 0.28956629604010253\n",
      "iteration 163, err = 0.18562884593663173\n",
      "iteration 164, err = 0.28694398326417003\n",
      "iteration 165, err = 0.1658183838319731\n",
      "iteration 166, err = 0.26381115378191017\n",
      "iteration 167, err = 0.3017546958237773\n",
      "iteration 168, err = 0.14766235975117945\n",
      "iteration 169, err = 0.16584712661405113\n",
      "iteration 170, err = 0.1925146304791406\n",
      "iteration 171, err = 0.14714562273140422\n",
      "iteration 172, err = 0.18369486715536498\n",
      "iteration 173, err = 0.1999967001877483\n",
      "iteration 174, err = 0.1728930468304761\n",
      "iteration 175, err = 0.28588675883778764\n",
      "iteration 176, err = 0.1581049429933254\n",
      "iteration 177, err = 0.15045636038167287\n",
      "iteration 178, err = 0.16355717151772223\n",
      "iteration 179, err = 0.16549511347218362\n",
      "iteration 180, err = 0.18789689177634133\n",
      "iteration 181, err = 0.2660901174520172\n",
      "iteration 182, err = 0.13961490183647643\n",
      "iteration 183, err = 0.27374573503630495\n",
      "iteration 184, err = 0.3478189833813612\n",
      "iteration 185, err = 0.3737538105922604\n",
      "iteration 186, err = 0.14553184031858402\n",
      "iteration 187, err = 0.2112362362558123\n",
      "iteration 188, err = 0.14008078368295088\n",
      "iteration 189, err = 0.2751104939817422\n",
      "iteration 190, err = 0.17515052944913806\n",
      "iteration 191, err = 0.1651802895706989\n",
      "iteration 192, err = 0.28525008551552383\n",
      "iteration 193, err = 0.15593653310033484\n",
      "iteration 194, err = 0.14374859370027152\n",
      "iteration 195, err = 0.18865523391577135\n",
      "iteration 196, err = 0.2600288868499387\n",
      "iteration 197, err = 0.287109049404453\n",
      "iteration 198, err = 0.13346827884329782\n",
      "iteration 199, err = 0.17868648508842433\n",
      "iteration 200, err = 0.25312967221012284\n",
      "iteration 201, err = 0.17379826569722823\n",
      "iteration 202, err = 0.28756511342924923\n",
      "iteration 203, err = 0.1798649877160993\n",
      "iteration 204, err = 0.16904516331218986\n",
      "iteration 205, err = 0.13881099628882512\n",
      "iteration 206, err = 0.25654837738672853\n",
      "iteration 207, err = 0.17684652816476185\n",
      "iteration 208, err = 0.27633764676736994\n",
      "iteration 209, err = 0.14681110387255725\n",
      "iteration 210, err = 0.23477948880743657\n",
      "iteration 211, err = 0.24709516478165244\n",
      "iteration 212, err = 0.28180321159101185\n",
      "iteration 213, err = 0.13160766684262795\n",
      "iteration 214, err = 0.15053387799988358\n",
      "iteration 215, err = 0.1740923739564184\n",
      "iteration 216, err = 0.24554036959243924\n",
      "iteration 217, err = 0.1296750157636822\n",
      "iteration 218, err = 0.2535261327228701\n",
      "iteration 219, err = 0.33174459714304727\n",
      "iteration 220, err = 0.1803077239391362\n",
      "iteration 221, err = 0.16241081962366874\n",
      "iteration 222, err = 0.1657918477610442\n",
      "iteration 223, err = 0.23874972255858906\n",
      "iteration 224, err = 0.26994914013837806\n",
      "iteration 225, err = 0.15196612533062567\n",
      "iteration 226, err = 0.16794325430807697\n",
      "iteration 227, err = 0.13466456323313408\n",
      "iteration 228, err = 0.15189688501450085\n",
      "iteration 229, err = 0.16399898868361767\n",
      "iteration 230, err = 0.23249925356090934\n",
      "iteration 231, err = 0.12666175149041914\n",
      "iteration 232, err = 0.23939575773594654\n",
      "iteration 233, err = 0.14003660943271062\n",
      "iteration 234, err = 0.21338118378046217\n",
      "iteration 235, err = 0.24331860265962518\n",
      "iteration 236, err = 0.26036113805119504\n",
      "iteration 237, err = 0.1233566122484943\n",
      "iteration 238, err = 0.17442669373812886\n",
      "iteration 239, err = 0.12578223000552943\n",
      "iteration 240, err = 0.1705053391085241\n",
      "iteration 241, err = 0.21749554325646078\n",
      "iteration 242, err = 0.16787002287983946\n",
      "iteration 243, err = 0.1510934731575274\n",
      "iteration 244, err = 0.12740668894358206\n",
      "iteration 245, err = 0.17001664318951384\n",
      "iteration 246, err = 0.12479604586020669\n",
      "iteration 247, err = 0.2484551240589092\n",
      "iteration 248, err = 0.16032144588512345\n",
      "iteration 249, err = 0.15077630295126115\n",
      "iteration 250, err = 0.2057855595827304\n",
      "iteration 251, err = 0.22845338125666742\n",
      "iteration 252, err = 0.12278708481201644\n",
      "iteration 253, err = 0.1338335108616161\n",
      "iteration 254, err = 0.13771493927096745\n",
      "iteration 255, err = 0.16138443223936005\n",
      "iteration 256, err = 0.22155068988659232\n",
      "iteration 257, err = 0.2488553595601343\n",
      "iteration 258, err = 0.15911562921056988\n",
      "iteration 259, err = 0.2732854417377415\n",
      "iteration 260, err = 0.16730065535404426\n",
      "iteration 261, err = 0.15161561462917642\n",
      "iteration 262, err = 0.256600604716229\n",
      "iteration 263, err = 0.17056665010058095\n",
      "iteration 264, err = 0.26114720235548494\n",
      "iteration 265, err = 0.13338826603892925\n",
      "iteration 266, err = 0.20734397528150603\n",
      "iteration 267, err = 0.2480443575221334\n",
      "iteration 268, err = 0.12063682360547062\n",
      "iteration 269, err = 0.13716948044810784\n",
      "iteration 270, err = 0.15584928347815932\n",
      "iteration 271, err = 0.12297678077674667\n",
      "iteration 272, err = 0.1622126495236204\n",
      "iteration 273, err = 0.17192030223148116\n",
      "iteration 274, err = 0.15178375846866057\n",
      "iteration 275, err = 0.25365890473020797\n",
      "iteration 276, err = 0.13026183984541975\n",
      "iteration 277, err = 0.12711704086595416\n",
      "iteration 278, err = 0.13913488817542655\n",
      "iteration 279, err = 0.14269049092900163\n",
      "iteration 280, err = 0.15127882328923734\n",
      "iteration 281, err = 0.21434060111575287\n",
      "iteration 282, err = 0.1187863897742363\n",
      "iteration 283, err = 0.2248666179964368\n",
      "iteration 284, err = 0.30411488572893647\n",
      "iteration 285, err = 0.3375394917477336\n",
      "iteration 286, err = 0.12177929189597578\n",
      "iteration 287, err = 0.179328671931619\n",
      "iteration 288, err = 0.11992738894295624\n",
      "iteration 289, err = 0.226511916149232\n",
      "iteration 290, err = 0.15936640014096107\n",
      "iteration 291, err = 0.1473068925543638\n",
      "iteration 292, err = 0.2503194890990424\n",
      "iteration 293, err = 0.1259712591297557\n",
      "iteration 294, err = 0.12459490517147953\n",
      "iteration 295, err = 0.15617953483362618\n",
      "iteration 296, err = 0.21380601897934168\n",
      "iteration 297, err = 0.2413598047386932\n",
      "iteration 298, err = 0.11614936130803116\n",
      "iteration 299, err = 0.1630369745065385\n",
      "iteration 300, err = 0.2158653305719774\n",
      "iteration 301, err = 0.14921582734858505\n",
      "iteration 302, err = 0.24947240482374927\n",
      "iteration 303, err = 0.16570290359546405\n",
      "iteration 304, err = 0.1355951672137782\n",
      "iteration 305, err = 0.12044596660005327\n",
      "iteration 306, err = 0.21781041316583497\n",
      "iteration 307, err = 0.1605544828181172\n",
      "iteration 308, err = 0.24641891853485517\n",
      "iteration 309, err = 0.12466632826823716\n",
      "iteration 310, err = 0.19064386777067135\n",
      "iteration 311, err = 0.2222073137155802\n",
      "iteration 312, err = 0.2454146541806274\n",
      "iteration 313, err = 0.11425750181499376\n",
      "iteration 314, err = 0.1283554368556763\n",
      "iteration 315, err = 0.146185732646345\n",
      "iteration 316, err = 0.2026421348818727\n",
      "iteration 317, err = 0.11415727320062667\n",
      "iteration 318, err = 0.21100098596128525\n",
      "iteration 319, err = 0.28913750027405866\n",
      "iteration 320, err = 0.16848227425483375\n",
      "iteration 321, err = 0.14734935378938435\n",
      "iteration 322, err = 0.13539390170167848\n",
      "iteration 323, err = 0.19371800541295003\n",
      "iteration 324, err = 0.22514719244541442\n",
      "iteration 325, err = 0.13253846698425967\n",
      "iteration 326, err = 0.13926049177501762\n",
      "iteration 327, err = 0.11696551071794983\n",
      "iteration 328, err = 0.13140537342641015\n",
      "iteration 329, err = 0.13669427990631422\n",
      "iteration 330, err = 0.19030912712849155\n",
      "iteration 331, err = 0.11115561073692215\n",
      "iteration 332, err = 0.20064752862465154\n",
      "iteration 333, err = 0.11781809137909068\n",
      "iteration 334, err = 0.1715941969066386\n",
      "iteration 335, err = 0.21965647975390815\n",
      "iteration 336, err = 0.22617034416751253\n",
      "iteration 337, err = 0.10921918001184955\n",
      "iteration 338, err = 0.16187972042770946\n",
      "iteration 339, err = 0.11093734996901819\n",
      "iteration 340, err = 0.15734196049837593\n",
      "iteration 341, err = 0.20379876961504373\n",
      "iteration 342, err = 0.14102941575833955\n",
      "iteration 343, err = 0.13523068297155139\n",
      "iteration 344, err = 0.11234537811245288\n",
      "iteration 345, err = 0.1453703604566716\n",
      "iteration 346, err = 0.1112581661156647\n",
      "iteration 347, err = 0.20957967424016238\n",
      "iteration 348, err = 0.14554176506318148\n",
      "iteration 349, err = 0.13221025178472925\n",
      "iteration 350, err = 0.18671900231343383\n",
      "iteration 351, err = 0.21452704525116398\n",
      "iteration 352, err = 0.11061669546472362\n",
      "iteration 353, err = 0.11712449623634652\n",
      "iteration 354, err = 0.12150038098171861\n",
      "iteration 355, err = 0.1373098503234838\n",
      "iteration 356, err = 0.18410462363605298\n",
      "iteration 357, err = 0.20920025530497216\n",
      "iteration 358, err = 0.13957098493308373\n",
      "iteration 359, err = 0.2394234118261869\n",
      "iteration 360, err = 0.15339222173732223\n",
      "iteration 361, err = 0.13674206007714315\n",
      "iteration 362, err = 0.22717936885297416\n",
      "iteration 363, err = 0.15619872367797105\n",
      "iteration 364, err = 0.2340757915841121\n",
      "iteration 365, err = 0.11604356050989655\n",
      "iteration 366, err = 0.1711973309279549\n",
      "iteration 367, err = 0.20760276490260302\n",
      "iteration 368, err = 0.10745375275491026\n",
      "iteration 369, err = 0.11916622136082637\n",
      "iteration 370, err = 0.1329288548245721\n",
      "iteration 371, err = 0.10944342642811013\n",
      "iteration 372, err = 0.14786727725059826\n",
      "iteration 373, err = 0.14731497000369098\n",
      "iteration 374, err = 0.13482025054105262\n",
      "iteration 375, err = 0.22261975280888083\n",
      "iteration 376, err = 0.11452913648653741\n",
      "iteration 377, err = 0.11282751934528963\n",
      "iteration 378, err = 0.12271648913822396\n",
      "iteration 379, err = 0.12648354124914554\n",
      "iteration 380, err = 0.12810409873159867\n",
      "iteration 381, err = 0.17752936938170574\n",
      "iteration 382, err = 0.10685245326305047\n",
      "iteration 383, err = 0.19018308484230714\n",
      "iteration 384, err = 0.26448218511075233\n",
      "iteration 385, err = 0.2997990628717706\n",
      "iteration 386, err = 0.10788634633297342\n",
      "iteration 387, err = 0.1587890824854612\n",
      "iteration 388, err = 0.10737846365746583\n",
      "iteration 389, err = 0.1910815648266435\n",
      "iteration 390, err = 0.14384916286780217\n",
      "iteration 391, err = 0.13314302734638786\n",
      "iteration 392, err = 0.2196985556127219\n",
      "iteration 393, err = 0.10852689050162252\n",
      "iteration 394, err = 0.11195491007230808\n",
      "iteration 395, err = 0.13209142891561632\n",
      "iteration 396, err = 0.17805427888636247\n",
      "iteration 397, err = 0.20343330429901144\n",
      "iteration 398, err = 0.10483100976683979\n",
      "iteration 399, err = 0.15084706739602707\n",
      "iteration 400, err = 0.18827660408746597\n",
      "iteration 401, err = 0.12820234814322057\n",
      "iteration 402, err = 0.21252250132570097\n",
      "iteration 403, err = 0.15085651119601728\n",
      "iteration 404, err = 0.11394890641586787\n",
      "iteration 405, err = 0.10790689662136498\n",
      "iteration 406, err = 0.1865341841109532\n",
      "iteration 407, err = 0.14549661652114515\n",
      "iteration 408, err = 0.21597053730078145\n",
      "iteration 409, err = 0.1101974547959304\n",
      "iteration 410, err = 0.15961474522071006\n",
      "iteration 411, err = 0.20247310944156524\n",
      "iteration 412, err = 0.21553469119283966\n",
      "iteration 413, err = 0.10251975204667672\n",
      "iteration 414, err = 0.11254960804056523\n",
      "iteration 415, err = 0.12510076385865437\n",
      "iteration 416, err = 0.1689374279934736\n",
      "iteration 417, err = 0.10262673543992815\n",
      "iteration 418, err = 0.1776420847937888\n",
      "iteration 419, err = 0.2477526233741609\n",
      "iteration 420, err = 0.1545004003156577\n",
      "iteration 421, err = 0.13453579091514423\n",
      "iteration 422, err = 0.1133882646885661\n",
      "iteration 423, err = 0.1578980719564862\n",
      "iteration 424, err = 0.18626365308177828\n",
      "iteration 425, err = 0.11514108180569842\n",
      "iteration 426, err = 0.11818888713659975\n",
      "iteration 427, err = 0.10391273669901596\n",
      "iteration 428, err = 0.11623223941218308\n",
      "iteration 429, err = 0.11504378149375025\n",
      "iteration 430, err = 0.15543888104963377\n",
      "iteration 431, err = 0.09913830059524122\n",
      "iteration 432, err = 0.16828762113148443\n",
      "iteration 433, err = 0.1014424661519441\n",
      "iteration 434, err = 0.13893300294822822\n",
      "iteration 435, err = 0.19850287043877096\n",
      "iteration 436, err = 0.1956457979677735\n",
      "iteration 437, err = 0.09784153151419274\n",
      "iteration 438, err = 0.15078613266257723\n",
      "iteration 439, err = 0.09889484131202435\n",
      "iteration 440, err = 0.1464755421854968\n",
      "iteration 441, err = 0.19382334182994967\n",
      "iteration 442, err = 0.11632079437589458\n",
      "iteration 443, err = 0.12440204473031669\n",
      "iteration 444, err = 0.09938650751401934\n",
      "iteration 445, err = 0.12434899750075373\n",
      "iteration 446, err = 0.0992604880525892\n",
      "iteration 447, err = 0.17591531332748345\n",
      "iteration 448, err = 0.13028121993565178\n",
      "iteration 449, err = 0.11726555087049167\n",
      "iteration 450, err = 0.17149719893640492\n",
      "iteration 451, err = 0.20334717980848216\n",
      "iteration 452, err = 0.10025674057436001\n",
      "iteration 453, err = 0.10294648081638483\n",
      "iteration 454, err = 0.10718858689137628\n",
      "iteration 455, err = 0.11719413411108222\n",
      "iteration 456, err = 0.1523102385656897\n",
      "iteration 457, err = 0.17348690645792492\n",
      "iteration 458, err = 0.12094918866273051\n",
      "iteration 459, err = 0.20366561025311936\n",
      "iteration 460, err = 0.1378495748559576\n",
      "iteration 461, err = 0.12276191885861949\n",
      "iteration 462, err = 0.19646362198768677\n",
      "iteration 463, err = 0.14087230673781165\n",
      "iteration 464, err = 0.20361870168486249\n",
      "iteration 465, err = 0.10211422874079328\n",
      "iteration 466, err = 0.14220848452820065\n",
      "iteration 467, err = 0.17144717253620473\n",
      "iteration 468, err = 0.0954887236738693\n",
      "iteration 469, err = 0.10409807964038659\n",
      "iteration 470, err = 0.112348515001939\n",
      "iteration 471, err = 0.09703063960834546\n",
      "iteration 472, err = 0.13377549743530207\n",
      "iteration 473, err = 0.12283670665223141\n",
      "iteration 474, err = 0.11787667376163863\n",
      "iteration 475, err = 0.1891913485799002\n",
      "iteration 476, err = 0.10115210909722201\n",
      "iteration 477, err = 0.09915608610287092\n",
      "iteration 478, err = 0.10764262043057542\n",
      "iteration 479, err = 0.11143994950933653\n",
      "iteration 480, err = 0.1076068050485387\n",
      "iteration 481, err = 0.14437920107263646\n",
      "iteration 482, err = 0.09487166893044519\n",
      "iteration 483, err = 0.15878947906437266\n",
      "iteration 484, err = 0.22233218213978762\n",
      "iteration 485, err = 0.25520271030733105\n",
      "iteration 486, err = 0.09504713862532872\n",
      "iteration 487, err = 0.14177550422149204\n",
      "iteration 488, err = 0.09485490362300629\n",
      "iteration 489, err = 0.15758348535887587\n",
      "iteration 490, err = 0.12667916430488627\n",
      "iteration 491, err = 0.11840491732970795\n",
      "iteration 492, err = 0.18759395896757525\n",
      "iteration 493, err = 0.09342068655606509\n",
      "iteration 494, err = 0.09931238308942573\n",
      "iteration 495, err = 0.10930796040260589\n",
      "iteration 496, err = 0.14390666667080426\n",
      "iteration 497, err = 0.1654404168789045\n",
      "iteration 498, err = 0.09250714373331881\n",
      "iteration 499, err = 0.13689429974801864\n",
      "iteration 500, err = 0.16104561961015937\n",
      "iteration 501, err = 0.10720651221793091\n",
      "iteration 502, err = 0.17321868677291577\n",
      "iteration 503, err = 0.13292666447855034\n",
      "iteration 504, err = 0.09511884508043254\n",
      "iteration 505, err = 0.09517266196777435\n",
      "iteration 506, err = 0.1557446165091146\n",
      "iteration 507, err = 0.12854715537745476\n",
      "iteration 508, err = 0.18222433386793052\n",
      "iteration 509, err = 0.09650157297091344\n",
      "iteration 510, err = 0.13201365858214734\n",
      "iteration 511, err = 0.18160484555558687\n",
      "iteration 512, err = 0.1846464306838307\n",
      "iteration 513, err = 0.08996273320857714\n",
      "iteration 514, err = 0.09713798270501176\n",
      "iteration 515, err = 0.10435604822223907\n",
      "iteration 516, err = 0.1363899524113277\n",
      "iteration 517, err = 0.08980909156926777\n",
      "iteration 518, err = 0.14604494240036744\n",
      "iteration 519, err = 0.2035153100455069\n",
      "iteration 520, err = 0.13649401796138122\n",
      "iteration 521, err = 0.11977194184063018\n",
      "iteration 522, err = 0.09346001896868769\n",
      "iteration 523, err = 0.12439383221130319\n",
      "iteration 524, err = 0.1474675425915086\n",
      "iteration 525, err = 0.09744764969760956\n",
      "iteration 526, err = 0.09813364348469678\n",
      "iteration 527, err = 0.09037947426278194\n",
      "iteration 528, err = 0.1010333338499069\n",
      "iteration 529, err = 0.09432917027791053\n",
      "iteration 530, err = 0.12219956326657634\n",
      "iteration 531, err = 0.08604912033553498\n",
      "iteration 532, err = 0.1373549737959603\n",
      "iteration 533, err = 0.08577689943210524\n",
      "iteration 534, err = 0.10929745873667777\n",
      "iteration 535, err = 0.17394495910231036\n",
      "iteration 536, err = 0.163281825177812\n",
      "iteration 537, err = 0.08494616571402001\n",
      "iteration 538, err = 0.1366545116566348\n",
      "iteration 539, err = 0.08578483181998951\n",
      "iteration 540, err = 0.13339193799867902\n",
      "iteration 541, err = 0.1812813724179187\n",
      "iteration 542, err = 0.0927348752790174\n",
      "iteration 543, err = 0.11365043069544932\n",
      "iteration 544, err = 0.08515260748068954\n",
      "iteration 545, err = 0.10311033721902656\n",
      "iteration 546, err = 0.08555425718746726\n",
      "iteration 547, err = 0.14256745504552298\n",
      "iteration 548, err = 0.11218095768199708\n",
      "iteration 549, err = 0.10144453062937084\n",
      "iteration 550, err = 0.15433662065663542\n",
      "iteration 551, err = 0.18880482568346277\n",
      "iteration 552, err = 0.08831545462358459\n",
      "iteration 553, err = 0.087677205227282\n",
      "iteration 554, err = 0.09140356129021952\n",
      "iteration 555, err = 0.09698885542814006\n",
      "iteration 556, err = 0.12130688347237267\n",
      "iteration 557, err = 0.13744991912033502\n",
      "iteration 558, err = 0.10135032233138488\n",
      "iteration 559, err = 0.16556419787435697\n",
      "iteration 560, err = 0.11843679682742497\n",
      "iteration 561, err = 0.10610510860831537\n",
      "iteration 562, err = 0.16245291331901623\n",
      "iteration 563, err = 0.12164349520990118\n",
      "iteration 564, err = 0.16879534006539865\n",
      "iteration 565, err = 0.08730689892544229\n",
      "iteration 566, err = 0.11467562416393552\n",
      "iteration 567, err = 0.1355212571120078\n",
      "iteration 568, err = 0.08154971945513775\n",
      "iteration 569, err = 0.08803484270117563\n",
      "iteration 570, err = 0.09138508135588591\n",
      "iteration 571, err = 0.0828638322626977\n",
      "iteration 572, err = 0.1164964256678556\n",
      "iteration 573, err = 0.09799310237070927\n",
      "iteration 574, err = 0.09847182574222246\n",
      "iteration 575, err = 0.1525850948339731\n",
      "iteration 576, err = 0.08649972657088786\n",
      "iteration 577, err = 0.0834072670073178\n",
      "iteration 578, err = 0.09055166291304274\n",
      "iteration 579, err = 0.0941913254043652\n",
      "iteration 580, err = 0.08713314588741389\n",
      "iteration 581, err = 0.11196832882709748\n",
      "iteration 582, err = 0.08053730437638632\n",
      "iteration 583, err = 0.12756785743156474\n",
      "iteration 584, err = 0.177406991711747\n",
      "iteration 585, err = 0.2048399771732696\n",
      "iteration 586, err = 0.08032769569558634\n",
      "iteration 587, err = 0.12331852158905156\n",
      "iteration 588, err = 0.08014977705020165\n",
      "iteration 589, err = 0.12358191197209929\n",
      "iteration 590, err = 0.10566881108375009\n",
      "iteration 591, err = 0.10016963535096762\n",
      "iteration 592, err = 0.15207980813442026\n",
      "iteration 593, err = 0.07769472890047456\n",
      "iteration 594, err = 0.08430365740595688\n",
      "iteration 595, err = 0.0864310505067116\n",
      "iteration 596, err = 0.10989666480610027\n",
      "iteration 597, err = 0.1262247652396872\n",
      "iteration 598, err = 0.07750331251584243\n",
      "iteration 599, err = 0.1181868176075551\n",
      "iteration 600, err = 0.13091818554790052\n",
      "iteration 601, err = 0.08564493166933382\n",
      "iteration 602, err = 0.1331141380024695\n",
      "iteration 603, err = 0.1104025170863448\n",
      "iteration 604, err = 0.07676471427124079\n",
      "iteration 605, err = 0.08025738388166279\n",
      "iteration 606, err = 0.12400586631571903\n",
      "iteration 607, err = 0.10769338978314515\n",
      "iteration 608, err = 0.14536175992680536\n",
      "iteration 609, err = 0.0810580974990574\n",
      "iteration 610, err = 0.10463522345857065\n",
      "iteration 611, err = 0.15537065229132987\n",
      "iteration 612, err = 0.1499781236450086\n",
      "iteration 613, err = 0.07493119149489263\n",
      "iteration 614, err = 0.07989890369246744\n",
      "iteration 615, err = 0.08291054501062903\n",
      "iteration 616, err = 0.10413969449320018\n",
      "iteration 617, err = 0.07450377686523428\n",
      "iteration 618, err = 0.11467968281797786\n",
      "iteration 619, err = 0.15779082835281874\n",
      "iteration 620, err = 0.11336356647113104\n",
      "iteration 621, err = 0.10082302403663991\n",
      "iteration 622, err = 0.07433528541870857\n",
      "iteration 623, err = 0.09305125055397101\n",
      "iteration 624, err = 0.10955485405752968\n",
      "iteration 625, err = 0.07894612191847165\n",
      "iteration 626, err = 0.07775786118250537\n",
      "iteration 627, err = 0.07496617088741457\n",
      "iteration 628, err = 0.08376936275158567\n",
      "iteration 629, err = 0.07422436867819936\n",
      "iteration 630, err = 0.09112460732990296\n",
      "iteration 631, err = 0.07108585428966908\n",
      "iteration 632, err = 0.10723409774766635\n",
      "iteration 633, err = 0.06972079345332671\n",
      "iteration 634, err = 0.08233787385531531\n",
      "iteration 635, err = 0.1435320810733679\n",
      "iteration 636, err = 0.12839459114060617\n",
      "iteration 637, err = 0.06986042523160513\n",
      "iteration 638, err = 0.11707474616102191\n",
      "iteration 639, err = 0.07071570395254784\n",
      "iteration 640, err = 0.11536705670978613\n",
      "iteration 641, err = 0.16105584934699668\n",
      "iteration 642, err = 0.0711472005473136\n",
      "iteration 643, err = 0.0998461676739346\n",
      "iteration 644, err = 0.06912179013408236\n",
      "iteration 645, err = 0.08120524172498794\n",
      "iteration 646, err = 0.06958327239303004\n",
      "iteration 647, err = 0.10937455950554371\n",
      "iteration 648, err = 0.09067739121867341\n",
      "iteration 649, err = 0.08321796586233228\n",
      "iteration 650, err = 0.13162697010185465\n",
      "iteration 651, err = 0.1657497817012664\n",
      "iteration 652, err = 0.07376071029483663\n",
      "iteration 653, err = 0.0705424704354505\n",
      "iteration 654, err = 0.07338557102205373\n",
      "iteration 655, err = 0.07621752871199114\n",
      "iteration 656, err = 0.09135947061442988\n",
      "iteration 657, err = 0.10221137375674895\n",
      "iteration 658, err = 0.08056849291104459\n",
      "iteration 659, err = 0.12708150019950398\n",
      "iteration 660, err = 0.09509365841813093\n",
      "iteration 661, err = 0.08584702782466891\n",
      "iteration 662, err = 0.1261177618819512\n",
      "iteration 663, err = 0.09802737779426299\n",
      "iteration 664, err = 0.13114349243043089\n",
      "iteration 665, err = 0.07044904747974004\n",
      "iteration 666, err = 0.08754124203848151\n",
      "iteration 667, err = 0.1006039670011721\n",
      "iteration 668, err = 0.0653532236751014\n",
      "iteration 669, err = 0.07012193341171342\n",
      "iteration 670, err = 0.07032871822462416\n",
      "iteration 671, err = 0.06654126960559852\n",
      "iteration 672, err = 0.09498721722456993\n",
      "iteration 673, err = 0.07383563914360969\n",
      "iteration 674, err = 0.07683097479758563\n",
      "iteration 675, err = 0.1149545738146459\n",
      "iteration 676, err = 0.06963326276274934\n",
      "iteration 677, err = 0.06558344018302897\n",
      "iteration 678, err = 0.07112548391942931\n",
      "iteration 679, err = 0.0742825073615842\n",
      "iteration 680, err = 0.06676339280836366\n",
      "iteration 681, err = 0.08165966578376979\n",
      "iteration 682, err = 0.06391585679174984\n",
      "iteration 683, err = 0.09665252534165981\n",
      "iteration 684, err = 0.13251321242101757\n",
      "iteration 685, err = 0.1531738082627101\n",
      "iteration 686, err = 0.06350599291176538\n",
      "iteration 687, err = 0.1008964742990079\n",
      "iteration 688, err = 0.06329182561603953\n",
      "iteration 689, err = 0.09052586932924038\n",
      "iteration 690, err = 0.08150978398451389\n",
      "iteration 691, err = 0.07842346697923581\n",
      "iteration 692, err = 0.11468344261292952\n",
      "iteration 693, err = 0.06088859785164638\n",
      "iteration 694, err = 0.06673127601968344\n",
      "iteration 695, err = 0.0643971447583361\n",
      "iteration 696, err = 0.07839953269599403\n",
      "iteration 697, err = 0.0892470043484306\n",
      "iteration 698, err = 0.0604057165995264\n",
      "iteration 699, err = 0.09434718264234432\n",
      "iteration 700, err = 0.09880650579583325\n",
      "iteration 701, err = 0.06428874540866253\n",
      "iteration 702, err = 0.09554691160446986\n",
      "iteration 703, err = 0.08461609345983112\n",
      "iteration 704, err = 0.05863090479146919\n",
      "iteration 705, err = 0.06311687451500005\n",
      "iteration 706, err = 0.0924449414621514\n",
      "iteration 707, err = 0.08354337149149417\n",
      "iteration 708, err = 0.10794955805553569\n",
      "iteration 709, err = 0.06356289752461275\n",
      "iteration 710, err = 0.07766390499961295\n",
      "iteration 711, err = 0.12297439040302727\n",
      "iteration 712, err = 0.11287402661718791\n",
      "iteration 713, err = 0.057903284141854756\n",
      "iteration 714, err = 0.06108204881196369\n",
      "iteration 715, err = 0.06168210038304306\n",
      "iteration 716, err = 0.07427568341093292\n",
      "iteration 717, err = 0.05737609430789516\n",
      "iteration 718, err = 0.0843929220350969\n",
      "iteration 719, err = 0.11402668177231928\n",
      "iteration 720, err = 0.08667063195594926\n",
      "iteration 721, err = 0.07819386783332241\n",
      "iteration 722, err = 0.05600544907559665\n",
      "iteration 723, err = 0.06562001438841504\n",
      "iteration 724, err = 0.0758908708655432\n",
      "iteration 725, err = 0.06000551274482842\n",
      "iteration 726, err = 0.05777852246093766\n",
      "iteration 727, err = 0.05794988041207839\n",
      "iteration 728, err = 0.06457883573576811\n",
      "iteration 729, err = 0.05527560613950009\n",
      "iteration 730, err = 0.06411868438557251\n",
      "iteration 731, err = 0.054806146959868726\n",
      "iteration 732, err = 0.07873278931201257\n",
      "iteration 733, err = 0.05337389990828804\n",
      "iteration 734, err = 0.05889698958765808\n",
      "iteration 735, err = 0.1087992224008951\n",
      "iteration 736, err = 0.09343842875164739\n",
      "iteration 737, err = 0.05336501506300313\n",
      "iteration 738, err = 0.09221758748559158\n",
      "iteration 739, err = 0.05420021631127748\n",
      "iteration 740, err = 0.0919398955917752\n",
      "iteration 741, err = 0.13121069093966536\n",
      "iteration 742, err = 0.0520099179272879\n",
      "iteration 743, err = 0.08134184839093077\n",
      "iteration 744, err = 0.05211876930277493\n",
      "iteration 745, err = 0.05958381298849938\n",
      "iteration 746, err = 0.05227848049427999\n",
      "iteration 747, err = 0.07805260396254819\n",
      "iteration 748, err = 0.06736170909954009\n",
      "iteration 749, err = 0.06306223797918781\n",
      "iteration 750, err = 0.10297642410155046\n",
      "iteration 751, err = 0.1327906823669224\n",
      "iteration 752, err = 0.056939065318873494\n",
      "iteration 753, err = 0.05245379263561281\n",
      "iteration 754, err = 0.05427056685488732\n",
      "iteration 755, err = 0.05570388097530416\n",
      "iteration 756, err = 0.06415794501608652\n",
      "iteration 757, err = 0.07055621255652966\n",
      "iteration 758, err = 0.05945874928259998\n",
      "iteration 759, err = 0.09074285491230517\n",
      "iteration 760, err = 0.06995793245145222\n",
      "iteration 761, err = 0.06353686855802138\n",
      "iteration 762, err = 0.09031632044265275\n",
      "iteration 763, err = 0.07216856589631589\n",
      "iteration 764, err = 0.09389633320626832\n",
      "iteration 765, err = 0.05229704668576547\n",
      "iteration 766, err = 0.06193583114536902\n",
      "iteration 767, err = 0.06914964433973854\n",
      "iteration 768, err = 0.04811873034279838\n",
      "iteration 769, err = 0.05138702596164002\n",
      "iteration 770, err = 0.050313762750650776\n",
      "iteration 771, err = 0.04912756492610422\n",
      "iteration 772, err = 0.07072899805661546\n",
      "iteration 773, err = 0.05172647852906917\n",
      "iteration 774, err = 0.05495565360969086\n",
      "iteration 775, err = 0.0797317206869466\n",
      "iteration 776, err = 0.05142908033307416\n",
      "iteration 777, err = 0.04724745479686532\n",
      "iteration 778, err = 0.05107160793301945\n",
      "iteration 779, err = 0.053469578593077244\n",
      "iteration 780, err = 0.04743792357574501\n",
      "iteration 781, err = 0.055383536216088054\n",
      "iteration 782, err = 0.0464538629222305\n",
      "iteration 783, err = 0.067697084271814\n",
      "iteration 784, err = 0.09132008755621385\n",
      "iteration 785, err = 0.10538781550621078\n",
      "iteration 786, err = 0.045931284875567274\n",
      "iteration 787, err = 0.07520978222861793\n",
      "iteration 788, err = 0.04571412312890008\n",
      "iteration 789, err = 0.06103812186803572\n",
      "iteration 790, err = 0.057108908487035495\n",
      "iteration 791, err = 0.05565115751601971\n",
      "iteration 792, err = 0.0790432777918688\n",
      "iteration 793, err = 0.0438446281248999\n",
      "iteration 794, err = 0.048115471788743217\n",
      "iteration 795, err = 0.04450007000469989\n",
      "iteration 796, err = 0.051838027723785006\n",
      "iteration 797, err = 0.058144936145572944\n",
      "iteration 798, err = 0.04300720109756633\n",
      "iteration 799, err = 0.06805028779527007\n",
      "iteration 800, err = 0.06797686349879849\n",
      "iteration 801, err = 0.04449167258137984\n",
      "iteration 802, err = 0.06337387348401066\n",
      "iteration 803, err = 0.05886828456541008\n",
      "iteration 804, err = 0.0413546385500655\n",
      "iteration 805, err = 0.045213909270626414\n",
      "iteration 806, err = 0.06334818414864757\n",
      "iteration 807, err = 0.05883253554504328\n",
      "iteration 808, err = 0.07347936439681096\n",
      "iteration 809, err = 0.04540412130019429\n",
      "iteration 810, err = 0.05290554853009736\n",
      "iteration 811, err = 0.08785209279675663\n",
      "iteration 812, err = 0.07724814696588489\n",
      "iteration 813, err = 0.04067768333242849\n",
      "iteration 814, err = 0.04250398529679509\n",
      "iteration 815, err = 0.04218243483633028\n",
      "iteration 816, err = 0.04890342796668633\n",
      "iteration 817, err = 0.04021672071516342\n",
      "iteration 818, err = 0.05709813763832019\n",
      "iteration 819, err = 0.075743806303724\n",
      "iteration 820, err = 0.05996208693953373\n",
      "iteration 821, err = 0.05475081811757671\n",
      "iteration 822, err = 0.03898773897158244\n",
      "iteration 823, err = 0.043141376032125894\n",
      "iteration 824, err = 0.04874118276851209\n",
      "iteration 825, err = 0.04182206514232858\n",
      "iteration 826, err = 0.0395254894784255\n",
      "iteration 827, err = 0.04081638210173386\n",
      "iteration 828, err = 0.04526089868657115\n",
      "iteration 829, err = 0.03811980695633708\n",
      "iteration 830, err = 0.042111020192343626\n",
      "iteration 831, err = 0.0385863184015257\n",
      "iteration 832, err = 0.05328974003646444\n",
      "iteration 833, err = 0.03754213970336549\n",
      "iteration 834, err = 0.03936505103206475\n",
      "iteration 835, err = 0.07441840220057491\n",
      "iteration 836, err = 0.06200550186991051\n",
      "iteration 837, err = 0.03710644492681816\n",
      "iteration 838, err = 0.06530098034233235\n",
      "iteration 839, err = 0.03778851920843648\n",
      "iteration 840, err = 0.06584572994189995\n",
      "iteration 841, err = 0.09543044141748122\n",
      "iteration 842, err = 0.035403490752344724\n",
      "iteration 843, err = 0.05953758970107176\n",
      "iteration 844, err = 0.03578135518338216\n",
      "iteration 845, err = 0.03993197313411664\n",
      "iteration 846, err = 0.03558810193068595\n",
      "iteration 847, err = 0.05095600185257943\n",
      "iteration 848, err = 0.045218744583207175\n",
      "iteration 849, err = 0.04320881491422765\n",
      "iteration 850, err = 0.07216775118151517\n",
      "iteration 851, err = 0.09469429833108504\n",
      "iteration 852, err = 0.03967652563195125\n",
      "iteration 853, err = 0.03535666503579657\n",
      "iteration 854, err = 0.03631431321729447\n",
      "iteration 855, err = 0.037078459897018176\n",
      "iteration 856, err = 0.041351473727288875\n",
      "iteration 857, err = 0.04469297692842585\n",
      "iteration 858, err = 0.039895292843426064\n",
      "iteration 859, err = 0.05926800120100194\n",
      "iteration 860, err = 0.04642283210862796\n",
      "iteration 861, err = 0.04230508095762372\n",
      "iteration 862, err = 0.0587298106922983\n",
      "iteration 863, err = 0.0477881649704691\n",
      "iteration 864, err = 0.06100640481705691\n",
      "iteration 865, err = 0.035025767515242105\n",
      "iteration 866, err = 0.03991581752933024\n",
      "iteration 867, err = 0.04345421973876888\n",
      "iteration 868, err = 0.03198048462465974\n",
      "iteration 869, err = 0.034018508018879096\n",
      "iteration 870, err = 0.032837508983682734\n",
      "iteration 871, err = 0.03273173022405774\n",
      "iteration 872, err = 0.0472632250918759\n",
      "iteration 873, err = 0.03311493627023457\n",
      "iteration 874, err = 0.0355274684629144\n",
      "iteration 875, err = 0.05027846543749779\n",
      "iteration 876, err = 0.03418895351814735\n",
      "iteration 877, err = 0.030705791281960784\n",
      "iteration 878, err = 0.03303420922041594\n",
      "iteration 879, err = 0.034617067237035205\n",
      "iteration 880, err = 0.030652249549048407\n",
      "iteration 881, err = 0.034475282052986625\n",
      "iteration 882, err = 0.03041607190956563\n",
      "iteration 883, err = 0.04311052158624135\n",
      "iteration 884, err = 0.05730420498840835\n",
      "iteration 885, err = 0.06595464205657145\n",
      "iteration 886, err = 0.029897815134571508\n",
      "iteration 887, err = 0.05009781463321151\n",
      "iteration 888, err = 0.029711973830465623\n",
      "iteration 889, err = 0.037453748535467096\n",
      "iteration 890, err = 0.035960226789871125\n",
      "iteration 891, err = 0.03537504379257624\n",
      "iteration 892, err = 0.049206638776442484\n",
      "iteration 893, err = 0.028441879135848978\n",
      "iteration 894, err = 0.031105557954659933\n",
      "iteration 895, err = 0.02804914870196446\n",
      "iteration 896, err = 0.031469406004850604\n",
      "iteration 897, err = 0.03471844532038977\n",
      "iteration 898, err = 0.02758721078100787\n",
      "iteration 899, err = 0.04380720166928285\n",
      "iteration 900, err = 0.04216097384672369\n",
      "iteration 901, err = 0.027935880489256788\n",
      "iteration 902, err = 0.03843025363404079\n",
      "iteration 903, err = 0.03681840588416887\n",
      "iteration 904, err = 0.02639760285986275\n",
      "iteration 905, err = 0.029044481006396804\n",
      "iteration 906, err = 0.03935117428161358\n",
      "iteration 907, err = 0.0371650428780653\n",
      "iteration 908, err = 0.04530966606464533\n",
      "iteration 909, err = 0.02909954477823522\n",
      "iteration 910, err = 0.03263891978021022\n",
      "iteration 911, err = 0.055981544649737126\n",
      "iteration 912, err = 0.047570130986214466\n",
      "iteration 913, err = 0.025644297633078423\n",
      "iteration 914, err = 0.02658841251391029\n",
      "iteration 915, err = 0.026128880884794033\n",
      "iteration 916, err = 0.029398597010192314\n",
      "iteration 917, err = 0.02532606000289307\n",
      "iteration 918, err = 0.03504046091880002\n",
      "iteration 919, err = 0.04575928801184362\n",
      "iteration 920, err = 0.03719895355037408\n",
      "iteration 921, err = 0.034262989792464345\n",
      "iteration 922, err = 0.024568555908274513\n",
      "iteration 923, err = 0.026069869688458102\n",
      "iteration 924, err = 0.028794549980742136\n",
      "iteration 925, err = 0.02629807482509555\n",
      "iteration 926, err = 0.024529365464803474\n",
      "iteration 927, err = 0.02581969989399121\n",
      "iteration 928, err = 0.028467562993479807\n",
      "iteration 929, err = 0.023873926358190963\n",
      "iteration 930, err = 0.02544251868715077\n",
      "iteration 931, err = 0.024432676689574974\n",
      "iteration 932, err = 0.0327553513627265\n",
      "iteration 933, err = 0.023814778983644768\n",
      "iteration 934, err = 0.024142227861553008\n",
      "iteration 935, err = 0.04562850661076008\n",
      "iteration 936, err = 0.03722146538907434\n",
      "iteration 937, err = 0.02318380964832842\n",
      "iteration 938, err = 0.04116720649442029\n",
      "iteration 939, err = 0.023643357819655333\n",
      "iteration 940, err = 0.0419076507408586\n",
      "iteration 941, err = 0.06134074058177523\n",
      "iteration 942, err = 0.02196276479347472\n",
      "iteration 943, err = 0.03859252181449543\n",
      "iteration 944, err = 0.02209802581404813\n",
      "iteration 945, err = 0.02415499214475224\n",
      "iteration 946, err = 0.021740391704210396\n",
      "iteration 947, err = 0.0301599702152689\n",
      "iteration 948, err = 0.02724574288718656\n",
      "iteration 949, err = 0.02652936646554952\n",
      "iteration 950, err = 0.04497369851449787\n",
      "iteration 951, err = 0.05970151863217173\n",
      "iteration 952, err = 0.02467527707904583\n",
      "iteration 953, err = 0.021404520773164883\n",
      "iteration 954, err = 0.021822506703948457\n",
      "iteration 955, err = 0.022231280705733852\n",
      "iteration 956, err = 0.024214517739873237\n",
      "iteration 957, err = 0.0257896932475597\n",
      "iteration 958, err = 0.024095115048340304\n",
      "iteration 959, err = 0.035082951265159854\n",
      "iteration 960, err = 0.027641476187649533\n",
      "iteration 961, err = 0.025221137042840222\n",
      "iteration 962, err = 0.034459237604912825\n",
      "iteration 963, err = 0.02834416197251288\n",
      "iteration 964, err = 0.03575046760258574\n",
      "iteration 965, err = 0.021035920830661324\n",
      "iteration 966, err = 0.023274827978671423\n",
      "iteration 967, err = 0.02483205465597698\n",
      "iteration 968, err = 0.01906693313189124\n",
      "iteration 969, err = 0.020219867806550818\n",
      "iteration 970, err = 0.0193675038300695\n",
      "iteration 971, err = 0.01955338954893759\n",
      "iteration 972, err = 0.028242378279235356\n",
      "iteration 973, err = 0.019214088184587464\n",
      "iteration 974, err = 0.020684107889133708\n",
      "iteration 975, err = 0.02872284095786981\n",
      "iteration 976, err = 0.02036257720445587\n",
      "iteration 977, err = 0.017934400835210747\n",
      "iteration 978, err = 0.019198699815669566\n",
      "iteration 979, err = 0.020117464311266865\n",
      "iteration 980, err = 0.017872312541621214\n",
      "iteration 981, err = 0.019564857886021476\n",
      "iteration 982, err = 0.01786926677222481\n",
      "iteration 983, err = 0.02483362063572931\n",
      "iteration 984, err = 0.03262105185026446\n",
      "iteration 985, err = 0.03744034374475949\n",
      "iteration 986, err = 0.017469104978243558\n",
      "iteration 987, err = 0.0297845523414816\n",
      "iteration 988, err = 0.017329097825434188\n",
      "iteration 989, err = 0.020881055790387067\n",
      "iteration 990, err = 0.020379136555844037\n",
      "iteration 991, err = 0.020175690673593066\n",
      "iteration 992, err = 0.027668708410890003\n",
      "iteration 993, err = 0.016551691565970347\n",
      "iteration 994, err = 0.0180122584794041\n",
      "iteration 995, err = 0.016027337508393743\n",
      "iteration 996, err = 0.017476580691399027\n",
      "iteration 997, err = 0.018991870876268805\n",
      "iteration 998, err = 0.015918533212987546\n",
      "iteration 999, err = 0.02524111250418649\n",
      "iteration 1000, err = 0.023606340607356734\n",
      "iteration 1001, err = 0.01586153307550396\n",
      "iteration 1002, err = 0.02126326418248484\n",
      "iteration 1003, err = 0.020750547234610877\n",
      "iteration 1004, err = 0.015177671606179815\n",
      "iteration 1005, err = 0.016727226152841855\n",
      "iteration 1006, err = 0.022128462653674904\n",
      "iteration 1007, err = 0.021098515619814923\n",
      "iteration 1008, err = 0.025305423861649405\n",
      "iteration 1009, err = 0.016745764372434288\n",
      "iteration 1010, err = 0.01823416227810811\n",
      "iteration 1011, err = 0.03195846823345529\n",
      "iteration 1012, err = 0.026437627838960136\n",
      "iteration 1013, err = 0.014529456870055685\n",
      "iteration 1014, err = 0.014979352849504433\n",
      "iteration 1015, err = 0.014640331432046662\n",
      "iteration 1016, err = 0.01611579295011771\n",
      "iteration 1017, err = 0.014347777908169452\n",
      "iteration 1018, err = 0.019500432180018776\n",
      "iteration 1019, err = 0.025154723237447506\n",
      "iteration 1020, err = 0.020780074974418746\n",
      "iteration 1021, err = 0.01925103799016625\n",
      "iteration 1022, err = 0.013961684771926816\n",
      "iteration 1023, err = 0.014407007600658819\n",
      "iteration 1024, err = 0.015620857070689715\n",
      "iteration 1025, err = 0.014908996747912153\n",
      "iteration 1026, err = 0.013787186746726791\n",
      "iteration 1027, err = 0.014685395473832394\n",
      "iteration 1028, err = 0.016105159216267498\n",
      "iteration 1029, err = 0.013522149598193107\n",
      "iteration 1030, err = 0.01406595428055821\n",
      "iteration 1031, err = 0.01391664980664482\n",
      "iteration 1032, err = 0.01826372673356833\n",
      "iteration 1033, err = 0.013597948719270026\n",
      "iteration 1034, err = 0.0135003588046656\n",
      "iteration 1035, err = 0.025252256934839266\n",
      "iteration 1036, err = 0.020294274248176175\n",
      "iteration 1037, err = 0.01304323530608331\n",
      "iteration 1038, err = 0.023254975926932476\n",
      "iteration 1039, err = 0.013306663119827318\n",
      "iteration 1040, err = 0.02384918442203649\n",
      "iteration 1041, err = 0.03512752724668786\n",
      "iteration 1042, err = 0.012354788866891502\n",
      "iteration 1043, err = 0.022279994204350527\n",
      "iteration 1044, err = 0.012314897997022138\n",
      "iteration 1045, err = 0.01323125202440123\n",
      "iteration 1046, err = 0.011984189080262582\n",
      "iteration 1047, err = 0.016255005207885843\n",
      "iteration 1048, err = 0.01484382911612361\n",
      "iteration 1049, err = 0.014693871052414886\n",
      "iteration 1050, err = 0.02515463646312488\n",
      "iteration 1051, err = 0.033637158576631195\n",
      "iteration 1052, err = 0.013780476632729936\n",
      "iteration 1053, err = 0.011708901237835665\n",
      "iteration 1054, err = 0.011861830372191755\n",
      "iteration 1055, err = 0.012068709313454369\n",
      "iteration 1056, err = 0.01292967969450536\n",
      "iteration 1057, err = 0.013616705906881884\n",
      "iteration 1058, err = 0.013175101416105024\n",
      "iteration 1059, err = 0.018911235309423454\n",
      "iteration 1060, err = 0.014899690659647101\n",
      "iteration 1061, err = 0.013594783501272663\n",
      "iteration 1062, err = 0.018381747902447707\n",
      "iteration 1063, err = 0.015210616193343345\n",
      "iteration 1064, err = 0.019043887920114938\n",
      "iteration 1065, err = 0.011433180980677289\n",
      "iteration 1066, err = 0.01236834934404726\n",
      "iteration 1067, err = 0.012993099982074728\n",
      "iteration 1068, err = 0.010288493431733938\n",
      "iteration 1069, err = 0.01088637681136334\n",
      "iteration 1070, err = 0.01038463903513564\n",
      "iteration 1071, err = 0.010567388303419068\n",
      "iteration 1072, err = 0.015260358771615044\n",
      "iteration 1073, err = 0.010170240661101473\n",
      "iteration 1074, err = 0.010953126356089672\n",
      "iteration 1075, err = 0.015000218905167596\n",
      "iteration 1076, err = 0.010982257801652738\n",
      "iteration 1077, err = 0.00951409603008228\n",
      "iteration 1078, err = 0.010139455371215734\n",
      "iteration 1079, err = 0.010619421918065211\n",
      "iteration 1080, err = 0.009480498219374129\n",
      "iteration 1081, err = 0.010185976049785645\n",
      "iteration 1082, err = 0.00952163331300664\n",
      "iteration 1083, err = 0.013057463792081463\n",
      "iteration 1084, err = 0.016999253305007855\n",
      "iteration 1085, err = 0.019461872243719112\n",
      "iteration 1086, err = 0.009272121257718096\n",
      "iteration 1087, err = 0.016023766185425593\n",
      "iteration 1088, err = 0.00917893329659514\n",
      "iteration 1089, err = 0.010690229966226839\n",
      "iteration 1090, err = 0.010539001172788857\n",
      "iteration 1091, err = 0.010476489752290423\n",
      "iteration 1092, err = 0.01423239298611377\n",
      "iteration 1093, err = 0.008745622303303734\n",
      "iteration 1094, err = 0.009471299972643444\n",
      "iteration 1095, err = 0.008374184365067823\n",
      "iteration 1096, err = 0.008947886396034896\n",
      "iteration 1097, err = 0.009605255444954709\n",
      "iteration 1098, err = 0.008364091479780796\n",
      "iteration 1099, err = 0.01322268915809914\n",
      "iteration 1100, err = 0.012097201057347904\n",
      "iteration 1101, err = 0.008234267628859916\n",
      "iteration 1102, err = 0.010840328235640835\n",
      "iteration 1103, err = 0.010690328146518362\n",
      "iteration 1104, err = 0.007950997593618014\n",
      "iteration 1105, err = 0.008760242394995734\n",
      "iteration 1106, err = 0.011399397562098066\n",
      "iteration 1107, err = 0.010923662172259508\n",
      "iteration 1108, err = 0.012960365566863521\n",
      "iteration 1109, err = 0.00877833283483873\n",
      "iteration 1110, err = 0.009342994626280217\n",
      "iteration 1111, err = 0.01661968024837175\n",
      "iteration 1112, err = 0.013461978168550407\n",
      "iteration 1113, err = 0.0075096924059639605\n",
      "iteration 1114, err = 0.007712864087578741\n",
      "iteration 1115, err = 0.007514701219246873\n",
      "iteration 1116, err = 0.008145119050410918\n",
      "iteration 1117, err = 0.007420281392227282\n",
      "iteration 1118, err = 0.009965957981912939\n",
      "iteration 1119, err = 0.012738179109888711\n",
      "iteration 1120, err = 0.010620903086976032\n",
      "iteration 1121, err = 0.009875642808227341\n",
      "iteration 1122, err = 0.007244196333301493\n",
      "iteration 1123, err = 0.007342848078826573\n",
      "iteration 1124, err = 0.007852107560552796\n",
      "iteration 1125, err = 0.0077241084945018825\n",
      "iteration 1126, err = 0.007103364062475434\n",
      "iteration 1127, err = 0.007621262796895685\n",
      "iteration 1128, err = 0.008320929906149544\n",
      "iteration 1129, err = 0.0070067320506735\n",
      "iteration 1130, err = 0.007174672503642503\n",
      "iteration 1131, err = 0.007232692265076402\n",
      "iteration 1132, err = 0.009351735718947913\n",
      "iteration 1133, err = 0.00708269831872573\n",
      "iteration 1134, err = 0.006943384135916294\n",
      "iteration 1135, err = 0.01282963894259539\n",
      "iteration 1136, err = 0.010198781198420746\n",
      "iteration 1137, err = 0.006709339655382768\n",
      "iteration 1138, err = 0.011986467549982645\n",
      "iteration 1139, err = 0.006843609925930306\n",
      "iteration 1140, err = 0.012361192773005791\n",
      "iteration 1141, err = 0.018281033710687525\n",
      "iteration 1142, err = 0.00637267062468691\n",
      "iteration 1143, err = 0.011674200721544556\n",
      "iteration 1144, err = 0.006290580018866215\n",
      "iteration 1145, err = 0.006666161262076482\n",
      "iteration 1146, err = 0.006063820914729599\n",
      "iteration 1147, err = 0.008098520178314443\n",
      "iteration 1148, err = 0.0074424741538901005\n",
      "iteration 1149, err = 0.0074707746052517995\n",
      "iteration 1150, err = 0.012874371604010625\n",
      "iteration 1151, err = 0.017292647169397296\n",
      "iteration 1152, err = 0.007039209667738455\n",
      "iteration 1153, err = 0.0058893746006070105\n",
      "iteration 1154, err = 0.005936409230324406\n",
      "iteration 1155, err = 0.006034185441195512\n",
      "iteration 1156, err = 0.006391397152507052\n",
      "iteration 1157, err = 0.006676187681036776\n",
      "iteration 1158, err = 0.006633307243179811\n",
      "iteration 1159, err = 0.009427255770941232\n",
      "iteration 1160, err = 0.007406693857796762\n",
      "iteration 1161, err = 0.006754066989665807\n",
      "iteration 1162, err = 0.009070762803627569\n",
      "iteration 1163, err = 0.007529706565165479\n",
      "iteration 1164, err = 0.009385022261362197\n",
      "iteration 1165, err = 0.005728229067583095\n",
      "iteration 1166, err = 0.0060912973219424795\n",
      "iteration 1167, err = 0.0063250566548984175\n",
      "iteration 1168, err = 0.005119437623130693\n",
      "iteration 1169, err = 0.005408479552996865\n",
      "iteration 1170, err = 0.005147954923500643\n",
      "iteration 1171, err = 0.005264613521833939\n",
      "iteration 1172, err = 0.007603972707344082\n",
      "iteration 1173, err = 0.004993465904339901\n",
      "iteration 1174, err = 0.005374109514703478\n",
      "iteration 1175, err = 0.007286964579147051\n",
      "iteration 1176, err = 0.005468144825777879\n",
      "iteration 1177, err = 0.004673069595839621\n",
      "iteration 1178, err = 0.00496189797765146\n",
      "iteration 1179, err = 0.005193626340608016\n",
      "iteration 1180, err = 0.004658722680892977\n",
      "iteration 1181, err = 0.004942101490074475\n",
      "iteration 1182, err = 0.004692224717425832\n",
      "iteration 1183, err = 0.00637824792406463\n",
      "iteration 1184, err = 0.008249591622902723\n",
      "iteration 1185, err = 0.009425205446105367\n",
      "iteration 1186, err = 0.004559356073071755\n",
      "iteration 1187, err = 0.007963291306314707\n",
      "iteration 1188, err = 0.004504385965012448\n",
      "iteration 1189, err = 0.005115965706076385\n",
      "iteration 1190, err = 0.005074959102882428\n",
      "iteration 1191, err = 0.005057824657089539\n",
      "iteration 1192, err = 0.006828263825386397\n",
      "iteration 1193, err = 0.004281754924769523\n",
      "iteration 1194, err = 0.004617851592585564\n",
      "iteration 1195, err = 0.004071886428850171\n",
      "iteration 1196, err = 0.004290109408720219\n",
      "iteration 1197, err = 0.00456268815298203\n",
      "iteration 1198, err = 0.004080787342716379\n",
      "iteration 1199, err = 0.006432760093277511\n",
      "iteration 1200, err = 0.005787316641427869\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 0], label = 0, pre = [[0.17292059]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n",
      "data = [1 1], label = 0, pre = [[0.10146324]]\n",
      "data = [1 0], label = 1, pre = [[0.89844188]]\n",
      "data = [0 1], label = 1, pre = [[1.01223095]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getDataSet():\n",
    "    \"\"\"\n",
    "    得到异或的训练集\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataSet = [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 1],\n",
    "        [1, 1, 0],\n",
    "        [1, 0, 1]\n",
    "    ]\n",
    "    return np.array(dataSet)\n",
    "\n",
    "\n",
    "def RBFNN(dataSet, eta, thresh):\n",
    "    \"\"\"\n",
    "    训练RBF网络\n",
    "    :param dataSet: 数据集\n",
    "    :param eta:     学习率\n",
    "    :param thresh:  错误率容忍度\n",
    "    :return: w,     隐层神经元对应的权重\n",
    "             c,     隐层神经元对应的中心\n",
    "            beta,   高斯径向基函数里的参数\n",
    "          errHistory  训练的历史错误率\n",
    "    \"\"\"\n",
    "    errHistory = []  # 记录每轮迭代的均方误差\n",
    "    y = dataSet[:, -1]\n",
    "    x = dataSet[:, :-1]\n",
    "    m, n = x.shape\n",
    "\n",
    "    # 隐层神经元数\n",
    "    t = 10\n",
    "    # 初始化c\n",
    "    c = np.random.rand(t, n)\n",
    "    # 初始化beta\n",
    "    beta = np.random.randn(1, t)\n",
    "    # 初始化w\n",
    "    w = np.random.rand(1, t)\n",
    "    err = errOfMeanSqur(dataSet, w, c, beta)\n",
    "    while err > thresh:\n",
    "        for i in range(m):\n",
    "            trainX = np.tile(x[i], (t, 1))\n",
    "            dist = ((trainX - c) ** 2).sum(axis=1).reshape(1, -1)  # 1 x 8\n",
    "            rho = np.exp(-beta * dist)  # 1 x 8\n",
    "            phi = np.dot(w, rho.T)\n",
    "\n",
    "            # 计算梯度\n",
    "            dBeta = -w * (phi - y[i]) * rho * dist\n",
    "            dw = (phi - y[i]) * rho\n",
    "\n",
    "            # 更新参数\n",
    "            beta -= eta * dBeta\n",
    "            w -= eta * dw\n",
    "\n",
    "            # 计算新的错误率\n",
    "            err = errOfMeanSqur(dataSet, w, c, beta)\n",
    "            errHistory.append(err)\n",
    "            print(f\"iteration {len(errHistory)}, err = {err}\")\n",
    "\n",
    "    return w, c, beta, errHistory\n",
    "\n",
    "\n",
    "def classify(data, w, c, beta):\n",
    "    \"\"\"\n",
    "    给定参数分类\n",
    "    :param data:\n",
    "    :param w:\n",
    "    :param c:\n",
    "    :param beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataArr = np.tile(data, (c.shape[0], 1))\n",
    "    dist = ((dataArr - c) ** 2).sum(axis=1)\n",
    "    rho = np.exp(-beta * dist).reshape(-1, 1)\n",
    "    return np.dot(w, rho)\n",
    "\n",
    "\n",
    "def errOfMeanSqur(dataSet, w, c, beta):\n",
    "    \"\"\"\n",
    "    计算平方误差\n",
    "    :param dataSet:\n",
    "    :param w:\n",
    "    :param c:\n",
    "    :param beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = dataSet[:, :-1]\n",
    "    y = dataSet[:, -1]\n",
    "    num = x.shape[0]\n",
    "    err = 0.0\n",
    "    for i in range(num):\n",
    "        yPre = classify(dataSet[i][:-1],  w, c, beta)\n",
    "        err += ((y[i] - yPre) ** 2) / 2.0\n",
    "\n",
    "    return (err / float(num))[0][0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # test RBFNN(dataSet, eta, thresh)\n",
    "    X = np.random.randint(0,2,(100,2))\n",
    "    y = np.logical_xor(X[:,0],X[:,1]).astype(int)\n",
    "    dataSet = np.hstack((X,y.reshape(-1,1)))\n",
    "    # dataSet = getDataSet()\n",
    "    w, c, beta, errHistory1 = RBFNN(dataSet, 0.1, 0.01)\n",
    "    plt.plot(np.arange(len(errHistory1)), errHistory1)\n",
    "    plt.show()\n",
    "\n",
    "    # 测试抑或\n",
    "    for data in dataSet:\n",
    "        pre = classify(data[:-1], w, c, beta)\n",
    "        print(f\"data = {data[:-1]}, label = {data[-1]}, pre = {pre}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "a = np.array([[1,2]])\n",
    "print(np.sum(a ** 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
