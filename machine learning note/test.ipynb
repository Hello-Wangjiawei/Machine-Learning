{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 载入数据\n",
    "dataset = np.loadtxt('../machine learning note/watermelon_3a.csv', delimiter=',')\n",
    "X = dataset[:,1:3]\n",
    "y = dataset[:,3]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 绘制散点图\n",
    "plt.scatter(X[y==0][:,0],X[y==0][:,1],color='red',s=100,label='Good')\n",
    "plt.scatter(X[y==1][:,0],X[y==1][:,1],color='blue',s=100,label='Bad')\n",
    "plt.xlabel('Density')\n",
    "plt.ylabel('Sugar')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "        \n",
    "# Sigmoid函数\n",
    "def sigmoid(X,beta):\n",
    "    return 1.0 / (1+np.exp(-np.dot(X,beta)))\n",
    "\n",
    "# 分布率\n",
    "def likelihood_sub(X,y,beta):\n",
    "    return -y * np.dot(beta,X.T) + np.log(1 + np.exp(np.dot(beta,X.T)))\n",
    "\n",
    "# 似然函数\n",
    "def likelihood(X,y,beta):\n",
    "    num = len(y)\n",
    "    sum = 0\n",
    "    for i in range(num):\n",
    "        sum += likelihood_sub(X[i],y[i],beta)\n",
    "    return sum\n",
    "\n",
    "# 梯度下降每一次参数更新，对应公式3.30\n",
    "def gradient_step(alpha,X,y,beta):\n",
    "    prediction  = sigmoid(X,beta)\n",
    "    error = prediction - y\n",
    "    gradientStep = alpha * np.dot(X.T,error)    \n",
    "    beta -= gradientStep\n",
    "    return beta\n",
    "\n",
    "# 批量梯度下降\n",
    "def gradient_descent(X,y,beta,alpha,num_iterations):   \n",
    "    for i in range(num_iterations):\n",
    "        beta = gradient_step(alpha,X,y,beta)\n",
    "        loss = likelihood(X,y,beta)\n",
    "        # print('Iteration:',i,'Loss:',loss)\n",
    "    return beta,loss\n",
    "\n",
    "# 随机梯度下降（每次只沿着一个方向下降）\n",
    "def stochastic_gradient_descent(X, y, beta, alpha, num_epochs):\n",
    "    sample_size = X.shape[0]\n",
    "    for i in range(num_epochs):\n",
    "        X, y = shuffle(X, y)  # 打乱数据顺序\n",
    "        for j in range(sample_size):\n",
    "            beta = gradient_step(alpha, X[j], y[j], beta)\n",
    "        loss = likelihood(X, y, beta)\n",
    "    return beta, loss\n",
    "\n",
    "# 预测函数\n",
    "def predict(X,beta):\n",
    "    sample_size = X.shape[0]\n",
    "    y = np.zeros(sample_size)\n",
    "    for i in range(sample_size):\n",
    "        if sigmoid(X[i],beta) > 0.5:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return y\n",
    "\n",
    "# 计算准确率\n",
    "def accuracy(y_true,y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "# 训练模型\n",
    "feature_size = X_train.shape[1]\n",
    "beta = np.zeros(feature_size + 1)\n",
    "alpha = 0.1\n",
    "num_iterations = 500\n",
    "X_train = np.c_[X_train,np.ones((X_train.shape[0],1))]\n",
    "X_test = np.c_[X_test,np.ones((X_test.shape[0],1))]\n",
    "beta,loss = gradient_descent(X_train,y_train,beta,alpha,num_iterations)\n",
    "print('beta:',beta)\n",
    "predictions = predict(X_test,beta)\n",
    "accuracy_score = accuracy(y_test,predictions)\n",
    "print('accuracy_score:',accuracy_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self,fit_intercept=True,method='batch',\n",
    "                 learning_rate=0.1,max_iter=1000,random_state=None):\n",
    "        \"\"\"\n",
    "        逻辑回归分类器\n",
    "        \n",
    "        参数:\n",
    "        - fit_intercept: 是否添加偏置项 (默认True)\n",
    "        - method: 优化方法 ['batch'(批量梯度下降), 'stochastic'(随机梯度下降)] (默认'batch')\n",
    "        - learning_rate: 学习率 (默认0.1)\n",
    "        - max_iter: 最大迭代次数 (默认500)\n",
    "        - random_state: 随机种子 (默认None)\n",
    "        \"\"\"\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.beta = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def add_intercept(self,X):\n",
    "        \"\"\"设置偏置项(X,1)\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            return np.c_[X,np.ones(X.shape[0])]\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        \"\"\"sigmoid函数\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def loss(self,X,y):\n",
    "        \"\"\"计算似然损失(3.27)\"\"\"\n",
    "        z = X @ self.beta\n",
    "        loss_terms = -y * z + np.log(1 + np.exp(-z))\n",
    "        return np.sum(loss_terms)\n",
    "\n",
    "    def batch_gradient_step(self,X,y):\n",
    "        \"\"\"批量梯度下降更新参数(3.30)\"\"\" \n",
    "        p = self.sigmoid(X @ self.beta)\n",
    "        gradent = X.T @ (p - y)\n",
    "        self.beta -= self.learning_rate * gradent\n",
    "\n",
    "    def stochastic_gradient_step(self,X,y):\n",
    "        \"\"\"随机梯度下降更新参数(一次随机更新一个方向)\"\"\"\n",
    "        for i in range(X.shape[0]):\n",
    "             p = self.sigmoid(X[i] @ self.beta)\n",
    "             gradent = X[i] * (p - y[i])\n",
    "             self.beta -= self.learning_rate * gradent\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \n",
    "        参数:\n",
    "        - X: 特征矩阵 (n_samples, n_features)\n",
    "        - y: 标签向量 (n_samples,)\n",
    "        \"\"\"\n",
    "\n",
    "        # 数据预处理(m个数据，n-1个特征)\n",
    "        X = self.add_intercept(X)\n",
    "        m,n = X.shape\n",
    "        self.beta = np.zeros(n)\n",
    "\n",
    "        # 设置随机种子\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # 选择优化方法\n",
    "        if self.method == 'batch':\n",
    "            optimizer = self.batch_gradient_step\n",
    "        elif self.method =='stochastic':\n",
    "            optimizer = self.stochastic_gradient_step\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'batch' or'stochastic'\")\n",
    "\n",
    "        # 训练模型\n",
    "        for epoch in range(self.max_iter):\n",
    "            # 随机梯度下降每次迭代后随机打乱数据\n",
    "            if self.method == 'stochastic':\n",
    "                X,y = shuffle(X,y)\n",
    "\n",
    "            # 执行一次梯度下降更新\n",
    "            optimizer(X,y)\n",
    "\n",
    "            # 记录损失\n",
    "            self.loss_history.append(self.loss(X,y))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        \"\"\"返回预测概率\"\"\"\n",
    "        X = self.add_intercept(X)\n",
    "        return self.sigmoid(X @ self.beta)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"返回预测类别\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "    def score(self,X,y):\n",
    "        \"\"\"计算准确率\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def plot_loss_curve(self):\n",
    "        \"\"\"绘制损失变化图\"\"\"\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_decision_boundary(self,X,y):\n",
    "        \"\"\"绘制决策边界\"\"\"\n",
    "        if X.shape[1] != 2:\n",
    "            raise ValueError(\"只支持二维特征可视化!\")\n",
    "        # 提取模型参数\n",
    "        w1, w2, b = self.beta[0], self.beta[1], self.beta[2]\n",
    "        \n",
    "        # 生成网格点坐标\n",
    "        x_min, x_max = X[:,0].min()-0.1, X[:,0].max()+0.1\n",
    "        y_min, y_max = X[:,1].min()-0.1, X[:,1].max()+0.1\n",
    "        xx = np.linspace(x_min, x_max, 100)\n",
    "        \n",
    "        # 计算决策边界直线方程：w1*x1 + w2*x2 + b = 0 → x2 = (-w1*x1 -b)/w2\n",
    "        decision_line = (-w1 * xx - b) / w2\n",
    "        \n",
    "        # 创建画布\n",
    "        plt.figure(figsize=(8,6))\n",
    "        \n",
    "        # 绘制原始数据点\n",
    "        plt.scatter(X[y==0][:,0], X[y==0][:,1], color='red', \n",
    "                    edgecolor='k', s=100, label='Bad')\n",
    "        plt.scatter(X[y==1][:,0], X[y==1][:,1], color='blue', \n",
    "                    edgecolor='k', s=100, label='Good')\n",
    "        \n",
    "        # 绘制决策边界\n",
    "        plt.plot(xx, decision_line, 'k--', lw=2, \n",
    "                label=f'Decision Boundary\\n{w1:.2f}x1 + {w2:.2f}x2 + {b:.2f} = 0')\n",
    "        \n",
    "        # 美化显示\n",
    "        plt.xlabel(\"Density\")\n",
    "        plt.ylabel(\"Sugar\")\n",
    "        plt.title(\"Decision Boundary\")\n",
    "        plt.legend(bbox_to_anchor=(1, 0.5), loc='center left')\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 加载数据\n",
    "    dataset = np.loadtxt('../machine learning note/watermelon_3a.csv', delimiter=',')\n",
    "    X = dataset[:, 1:3]\n",
    "    y = dataset[:, 3]\n",
    "\n",
    "    # 划分数据集(0.3的测试集)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # 初始化模型(批量梯度下降)\n",
    "    model = LogisticRegression(\n",
    "        method='batch',       \n",
    "        learning_rate=0.1,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 评估模型\n",
    "    print(\"模型参数:\", model.beta)\n",
    "    print(\"测试集准确率:\", model.score(X_test, y_test))\n",
    "\n",
    "    # 损失可视化\n",
    "    model.plot_loss_curve()\n",
    "\n",
    "    # 决策边界可视化\n",
    "    model.plot_decision_boundary(X_train, y_train)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = np.loadtxt('../machine learning note/watermelon_3a.csv',delimiter=',')\n",
    "X = dataset[:,1:3]\n",
    "y = dataset[:,3]\n",
    "\n",
    "\n",
    "# 计算均值\n",
    "mean1 = np.mean(X[y == 1],axis = 0)\n",
    "mean0 = np.mean(X[y == 0],axis = 0)\n",
    "\n",
    "print(mean1,mean0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算类内散度矩阵Sw(公式3.33)\n",
    "X0 = X[y == 0] - mean0\n",
    "X1 = X[y == 1] - mean1\n",
    "Sw = X0.T @ X0 + X1.T @ X1\n",
    "\n",
    "# 计算LDA(公式3.39)\n",
    "w = np.linalg.inv(Sw) @ (mean0 - mean1)\n",
    "\n",
    "# 绘制数据集\n",
    "plt.scatter(X[y == 1][:,0],X[y == 1][:,1],c='r',marker='o',label='正例')\n",
    "plt.scatter(X[y == 0][:,0],X[y == 0][:,1],c='b',marker='o',label='反例')\n",
    "plt.xlabel('dencity')\n",
    "plt.ylabel('suger')\n",
    "plt.title('LDA')\n",
    "\n",
    "# 计算每个数据点在投影方向上的投影点(降维)\n",
    "X_projected = X @ w\n",
    "\n",
    "# 创建一个用于投影的二维数组\n",
    "w_prime_2d = np.array([w[0], w[1]])\n",
    "X_projected_points = X_projected[:, np.newaxis] * w_prime_2d\n",
    "\n",
    "# 绘制投影点\n",
    "plt.scatter(X_projected_points[y == 1][:, 0], X_projected_points[y == 1][:, 1], c='b', s=50, alpha=0.5, label='正例投影')\n",
    "plt.scatter(X_projected_points[y == 0][:, 0], X_projected_points[y == 0][:, 1], c='r', s=50, alpha=0.5, label='反例投影')\n",
    "\n",
    "# 绘制从数据点到投影点的线段\n",
    "for i in range(len(X)):\n",
    "    plt.plot([X[i][0], X_projected_points[i][0]], [X[i][1], X_projected_points[i][1]], color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: {'模糊': '否', '清晰': {'密度': {'<=0.3815': '否', '>0.3815': '是'}}, '稍糊': {'触感': {'软粘': '是', '硬滑': '否'}}}\n",
      "c: {'触感': {'软粘': '是', '硬滑': '否'}}\n"
     ]
    }
   ],
   "source": [
    "a = {'纹理': {'模糊': '否', '清晰': {'密度': {'<=0.3815': '否', '>0.3815': '是'}}, '稍糊': {'触感': {'软粘': '是', '硬滑': '否'}}}}\n",
    "b = a['纹理']\n",
    "print(\"b:\", b)\n",
    "c = b['稍糊']\n",
    "print(\"c:\", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "a = '>100'\n",
    "b = a.strip('<=').strip('>')\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "#创建决策树\n",
    "def createTree(dataset, features):\n",
    "    '''\n",
    "    @brief: create a decision tree by using the ID3 algorithm\n",
    "    @param dataset: the dataset to be used for training\n",
    "    @param features: the features to be used for training\n",
    "    @return: the decision tree\n",
    "    '''\n",
    "    # 取出所有样本的标签\n",
    "    classList = [example[-1] for example in dataset]\n",
    "    # 如果所有样本的标签相同，则返回该标签\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 如果特征集为空，则返回出现次数最多的标签\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    # 选择最优特征进行数据集划分\n",
    "    bestfeatureIndex, bestValue = chooseBestFeatureToSplit(dataset)\n",
    "    bestFeatLabel = features[bestfeatureIndex]\n",
    "    \n",
    "    # 创建节点\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    # 使用副本避免修改原始列表\n",
    "    subfeatures = features.copy()  \n",
    "    # 连续特征\n",
    "    if type(bestValue).__name__ == 'float':\n",
    "        myTree[bestFeatLabel]['<=' + str(bestValue)] = createTree(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, True), subfeatures)\n",
    "        myTree[bestFeatLabel]['>' + str(bestValue)] = createTree(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, False), subfeatures)\n",
    "    # 离散特征\n",
    "    else:\n",
    "        # 去除当前特征\n",
    "        del subfeatures[bestfeatureIndex]  # 在副本中删除当前特征\n",
    "        # 取出当前特征的取值\n",
    "        featValue = [example[bestfeatureIndex] for example in dataset]\n",
    "        uniqueVals = set(featValue)\n",
    "        # 递归每一个特征值\n",
    "        for value in uniqueVals:\n",
    "            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset, bestfeatureIndex, value), subfeatures)\n",
    "    return myTree\n",
    "\n",
    "# 计算类别中出现次数最多的元素\n",
    "def majorityCnt(classList):\n",
    "    # 创建一个字典{类标签:出现次数}\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    # 降序排序[(类标签,出现次数),(),()]\n",
    "    sortedclassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedclassCount[0][0]\n",
    "\n",
    "# 选择最优特征进行数据集划分\n",
    "def chooseBestFeatureToSplit(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataset)\n",
    "    bestInfoGain = 0\n",
    "    bestFeature = -1\n",
    "    bestValue = 0\n",
    "    # 遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 取出第i个特征\n",
    "        featList = [example[i] for example in dataset]\n",
    "        # 连续特征\n",
    "        if type(featList[0]).__name__ == 'float':\n",
    "            # 排序\n",
    "            sortedfeatList = sorted(featList)\n",
    "            splitList = []\n",
    "            # 计算切分点\n",
    "            for j in range(len(sortedfeatList) - 1):\n",
    "                splitVal = (sortedfeatList[j] + sortedfeatList[j + 1]) / 2.0\n",
    "                splitList.append(splitVal)\n",
    "                \n",
    "            # 计算信息增益\n",
    "            for val in set(splitList):\n",
    "                newEntropy = 0\n",
    "                subDataSet1 = splitDataSetByValue(dataset, i, val, True)\n",
    "                subDataSet2 = splitDataSetByValue(dataset, i, val, False)\n",
    "                prob1 = len(subDataSet1) / float(len(dataset))\n",
    "                newEntropy += prob1 * calcShannonEnt(subDataSet1)\n",
    "                prob2 = len(subDataSet2) / float(len(dataset))\n",
    "                newEntropy += prob2 * calcShannonEnt(subDataSet2)\n",
    "                infoGain = baseEntropy - newEntropy\n",
    "                if (infoGain > bestInfoGain):\n",
    "                    bestInfoGain = infoGain\n",
    "                    bestFeature = i\n",
    "                    bestValue = val\n",
    "        else:\n",
    "            # 离散特征\n",
    "            uniqueVals = set(featList)\n",
    "            newEntropy = 0\n",
    "            # 遍历所有取值\n",
    "            for val in uniqueVals:\n",
    "                subDataSet = splitDataSet(dataset, i, val)\n",
    "                prob = len(subDataSet) / float(len(dataset))\n",
    "                newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "            infoGain = baseEntropy - newEntropy\n",
    "            if (infoGain > bestInfoGain):\n",
    "                bestInfoGain = infoGain\n",
    "                bestFeature = i\n",
    "                bestValue = None\n",
    "    return bestFeature, bestValue\n",
    "\n",
    "# 根据特征值划分数据集\n",
    "def splitDataSet(dataset, axis, val):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if featVec[axis] == val:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis + 1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# 根据特征值和方向划分数据集\n",
    "def splitDataSetByValue(dataset, axis, val, direction):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if direction:\n",
    "            if featVec[axis] <= val:\n",
    "                reducedFeatVec = featVec[:axis]\n",
    "                reducedFeatVec.extend(featVec[axis + 1:])\n",
    "                retDataSet.append(reducedFeatVec)\n",
    "        else:\n",
    "            if featVec[axis] > val:\n",
    "                reducedFeatVec = featVec[:axis]\n",
    "                reducedFeatVec.extend(featVec[axis + 1:])\n",
    "                retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# 计算数据集信息熵\n",
    "def calcShannonEnt(dataset):\n",
    "    numexamples = len(dataset)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataset:\n",
    "        currentlabel = featVec[-1]\n",
    "        if currentlabel not in labelCounts.keys():\n",
    "            labelCounts[currentlabel] = 0\n",
    "        labelCounts[currentlabel] += 1\n",
    "\n",
    "    shannonEnt = 0\n",
    "    for key in labelCounts:\n",
    "        prop = float(labelCounts[key]) / numexamples\n",
    "        shannonEnt -= prop * log(prop, 2)\n",
    "    return shannonEnt\n",
    "\n",
    "def predict(inputTree, features, testVec):\n",
    "    '''\n",
    "    @brief: predict the label of a test vector using a decision tree\n",
    "    @param inputTree: the decision tree to be used for prediction\n",
    "    @param features: the features to be used for training\n",
    "    @param testVec: the test vector to be predicted\n",
    "    @return: the predicted label of the test vector\n",
    "    '''\n",
    "    # 提取当前节点\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    # 提取当前节点下的子节点\n",
    "    secondDict = inputTree[firstStr]\n",
    "    # 获取当前节点的特征标签\n",
    "    featureIndex = features.index(firstStr)\n",
    "\n",
    "    for key in secondDict.keys():\n",
    "        # 处理连续特征（如 \"<=0.5\"）\n",
    "        if type(key).__name__ == 'str' and ('<=' in key or '>' in key):\n",
    "            # 移除字符串中的符号，取出阈值\n",
    "            threshold = float(key.replace('<=', '').replace('>', ''))\n",
    "            # 当前特征值小于等于阈值，则进入左子树\n",
    "            if key.startswith('<=') and testVec[featureIndex] <= threshold:\n",
    "                childTree = secondDict[key]\n",
    "                # 判断是否为内部节点，若是，则表示不是叶子节点，继续递归\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "            elif key.startswith('>') and testVec[featureIndex] > threshold:\n",
    "                childTree = secondDict[key]\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "        # 处理离散特征（如 \"硬滑\"）\n",
    "        else:\n",
    "            if testVec[featureIndex] == key:\n",
    "                childTree = secondDict[key]\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "    # 若未匹配任何分支\n",
    "    return \"未知类别\"  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 构建数据集\n",
    "    df = pd.DataFrame(pd.read_csv(\"../Data/watermelon3.0.csv\", encoding=\"ansi\"))\n",
    "    df.drop(labels=[\"编号\"], axis=1, inplace=True)  # 删除编号这一列，inplace=True表示直接在原对象修改\n",
    "    # 转化为列表\n",
    "    dataset = df.values.tolist()\n",
    "    # 打印原始数据\n",
    "    # for i in range(len(dataset)):\n",
    "    #     print(dataset[i])\n",
    "    # 标签\n",
    "    labels = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖率']\n",
    "    # 构建决策树\n",
    "    myTree = createTree(dataset, labels)\n",
    "    # 打印决策树\n",
    "    print(myTree)\n",
    "    # 测试数据\n",
    "    testVec = ['青绿','硬挺','清脆','稍糊','平坦','软粘',0.243,0.267]\n",
    "    # 预测结果\n",
    "    result = predict(myTree, labels, testVec)\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 424\u001b[0m\n\u001b[0;32m    417\u001b[0m xgTree \u001b[38;5;241m=\u001b[39m createTree(xgData, xgLabel, xgName, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m#print(xgTree)\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m#createPlot(xgTree)\u001b[39;00m\n\u001b[0;32m    420\u001b[0m  \n\u001b[0;32m    421\u001b[0m  \n\u001b[0;32m    422\u001b[0m  \n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m#使用自己的数据集测试函数\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m myData,myLabel,myName \u001b[38;5;241m=\u001b[39m createDataMine()\n\u001b[0;32m    425\u001b[0m myTree \u001b[38;5;241m=\u001b[39m createTree(myData,myLabel,myName,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m#print(myTree)\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m#createPlot(myTree)\u001b[39;00m\n\u001b[0;32m    428\u001b[0m  \n\u001b[0;32m    429\u001b[0m  \n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# 创建预剪枝决策树\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m, in \u001b[0;36mcreateDataMine\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateDataMine\u001b[39m():\n\u001b[1;32m---> 61\u001b[0m     raw_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     62\u001b[0m     data \u001b[38;5;241m=\u001b[39m raw_data\u001b[38;5;241m.\u001b[39mvalues[:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m#使用pandas.cut实现对数据的离散化\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    496\u001b[0m         io,\n\u001b[0;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32md:\\software\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1552\u001b[0m     )\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32md:\\software\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\software\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.xlsx'"
     ]
    }
   ],
   "source": [
    "import math     #导入一系列数学函数\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    " \n",
    "decisionNodeStyle = dict(boxstyle = \"sawtooth\", fc = \"0.8\")\n",
    "leafNodeStyle = dict(boxstyle = \"round4\", fc = \"0.8\")\n",
    "arrowArgs = dict(arrowstyle=\"<-\")\n",
    " \n",
    "# 设置显示中文字体\n",
    "mpl.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "# 设置正常显示符号\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    " \n",
    "#创建数据集\n",
    "def createDataLH():\n",
    "    data = np.array([['青年','否','否','一般']])\n",
    "    data = np.append(data, [['青年', '否', '否', '好']\n",
    "                            ,['青年', '是', '否', '好'] \n",
    "                            , ['青年', '是', '是', '一般']\n",
    "                            , ['青年', '否', '否', '一般']\n",
    "                            , ['中年', '否', '否', '一般']\n",
    "                            , ['中年', '否', '否', '好']\n",
    "                            , ['中年', '是', '是', '好']\n",
    "                            , ['中年', '否', '是', '非常好']\n",
    "                            , ['中年', '否', '是', '非常好']\n",
    "                            , ['老年', '否', '是', '非常好']\n",
    "                            , ['老年', '否', '是', '好']\n",
    "                            , ['老年', '是', '否', '好']\n",
    "                            , ['老年', '是', '否', '非常好']\n",
    "                            , ['老年', '否', '否', '一般']\n",
    "                           ], axis = 0)\n",
    "    label = np.array(['否', '否', '是', '是', '否', '否', '否', '是', '是', '是', '是', '是', '是', '是', '否'])\n",
    "    name = np.array(['年龄', '有工作', '有房子', '信贷情况'])\n",
    "    return data,label,name\n",
    " \n",
    "def createDataXG20():\n",
    "    data = np.array([['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑']\n",
    "                    , ['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑']\n",
    "                    , ['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑']\n",
    "                    , ['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑']\n",
    "                    , ['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑']\n",
    "                    , ['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘']\n",
    "                    , ['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘']\n",
    "                    , ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑']\n",
    "                    , ['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑']\n",
    "                    , ['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘']\n",
    "                    , ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑']\n",
    "                    , ['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘']\n",
    "                    , ['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑']\n",
    "                    , ['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑']\n",
    "                    , ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘']\n",
    "                    , ['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑']\n",
    "                    , ['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑']])\n",
    "    label = np.array(['是', '是', '是', '是', '是', '是', '是', '是', '否', '否', '否', '否', '否', '否', '否', '否', '否'])\n",
    "    name = np.array(['色泽', '根蒂', '敲声', '纹理', '脐部', '触感'])\n",
    "    return data,label,name\n",
    " \n",
    "def createDataMine():\n",
    "    raw_data = pd.read_excel(r'data.xlsx',header=0)\n",
    "    data = raw_data.values[:,1:5]\n",
    "    #使用pandas.cut实现对数据的离散化\n",
    "    data[:,0] = pd.cut(data[:,0],[0,300,800,1200,1400],labels=False)\n",
    "    data[:,1] = pd.cut(data[:,1],[0,1000,1300,1600,2000],labels=False)\n",
    "    data[:,2] = pd.cut(data[:,2],[0,5000,8000,10000,12000],labels=False)\n",
    "    myData = data[:,0:3]\n",
    "    myLabel = data[:,-1]\n",
    "    myData = myData.astype(str)\n",
    "    myLabel = myLabel.astype(str)\n",
    "    #print(myData.dtype)\n",
    "    #print(myLabel.dtype)\n",
    "    myName = [\"住宿费\",\"月平均花费\",\"家庭平均收入\"]\n",
    "    for i in range(myData.shape[0]):\n",
    "        for j in range(myData.shape[1]):\n",
    "            if(myData[i][j]=='1'):\n",
    "                myData[i][j]='低'\n",
    "            if(myData[i][j]=='2'):\n",
    "                myData[i][j]='中'\n",
    "            if(myData[i][j]=='3'):\n",
    "                myData[i][j]='高'\n",
    "    for k in range(len(myLabel)):\n",
    "        if(myLabel[k]=='0'):\n",
    "            myLabel[k]='否'\n",
    "        if(myLabel[k]=='1'):\n",
    "            myLabel[k]='是'\n",
    "    #print(myData)\n",
    "    #print(myLabel)\n",
    "    return myData,myLabel,myName\n",
    " \n",
    "def splitXgData20(xgData, xgLabel):\n",
    "    xgDataTrain = xgData[[0, 1, 2, 5, 6, 9, 13, 14, 15, 16],:]\n",
    "    xgDataTest = xgData[[3, 4, 7, 8, 10, 11, 12],:]\n",
    "    xgLabelTrain = xgLabel[[0, 1, 2, 5, 6, 9, 13, 14, 15, 16]]\n",
    "    xgLabelTest = xgLabel[[3, 4, 7, 8, 10, 11, 12]]\n",
    "    return xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest\n",
    " \n",
    "def splitMyData(myData,myLabel):\n",
    "    myDataTest = np.empty((0,3),dtype=int)\n",
    "    myLabelTest = np.empty((0),dtype=int)\n",
    "    index = 4\n",
    "    #print(myData.shape)\n",
    "    for i in range(int(myData.shape[0]*0.3)):\n",
    "        #print(index)\n",
    "        #print(myData[index,:])\n",
    "        myDataTest = np.append(myDataTest,[myData[index,:]],axis=0)\n",
    "        myData = np.delete(myData,index,axis=0)\n",
    "        myLabelTest = np.append(myLabelTest,myLabel[index])\n",
    "        myLabel = np.delete(myLabel,[index])\n",
    "        index += 2\n",
    "    #print(myData.shape[0])\n",
    "    #print(myDataTest)\n",
    "    #print(myLabelTest)\n",
    "    return myData,myLabel,myDataTest,myLabelTest\n",
    " \n",
    " \n",
    " \n",
    "# 定义一个常用函数 用来求numpy array中数值等于某值的元素数量\n",
    "equalNums = lambda x,y: 0 if x is None else x[x==y].size\n",
    " \n",
    " \n",
    "# 定义计算信息熵的函数\n",
    "def singleEntropy(x):\n",
    "    \"\"\"计算一个输入序列的信息熵\"\"\"\n",
    "    # 转换为 numpy 矩阵\n",
    "    x = np.asarray(x)\n",
    "    # 取所有不同值\n",
    "    xValues = set(x)\n",
    "    # 计算熵值\n",
    "    entropy = 0\n",
    "    for xValue in xValues:\n",
    "        p = equalNums(x, xValue) / x.size \n",
    "        entropy -= p * math.log(p, 2)\n",
    "    return entropy\n",
    "    \n",
    "    \n",
    "# 定义计算条件信息熵的函数\n",
    "def conditionnalEntropy(feature, y):\n",
    "    \"\"\"计算 某特征feature 条件下y的信息熵\"\"\"\n",
    "    # 转换为numpy \n",
    "    feature = np.asarray(feature)\n",
    "    y = np.asarray(y)\n",
    "    # 取特征的不同值\n",
    "    featureValues = set(feature)\n",
    "    # 计算熵值 \n",
    "    entropy = 0\n",
    "    for feat in featureValues:\n",
    "        # 解释：feature == feat 是得到取feature中所有元素值等于feat的元素的索引（类似这样理解）\n",
    "        #       y[feature == feat] 是取y中 feature元素值等于feat的元素索引的 y的元素的子集\n",
    "        p = equalNums(feature, feat) / feature.size \n",
    "        entropy += p * singleEntropy(y[feature == feat])\n",
    "    return entropy\n",
    "    \n",
    "    \n",
    "# 定义信息增益\n",
    "def infoGain(feature, y):\n",
    "    return singleEntropy(y) - conditionnalEntropy(feature, y)\n",
    " \n",
    " \n",
    "# 定义信息增益率\n",
    "def infoGainRatio(feature, y):\n",
    "    return 0 if singleEntropy(feature) == 0 else infoGain(feature, y) / singleEntropy(feature)\n",
    " \n",
    "'''\n",
    "# 使用李航数据测试函数 p62\n",
    "lhData, lhLabel, lhName = createDataLH()\n",
    "print(\"书中H(D)为0.971，函数结果：\" + str(round(singleEntropy(lhLabel), 3)))  \n",
    "print(\"书中g(D, A1)为0.083，函数结果：\" + str(round(infoGain(lhData[:,0] ,lhLabel), 3)))  \n",
    "print(\"书中g(D, A2)为0.324，函数结果：\" + str(round(infoGain(lhData[:,1] ,lhLabel), 3)))  \n",
    "print(\"书中g(D, A3)为0.420，函数结果：\" + str(round(infoGain(lhData[:,2] ,lhLabel), 3)))  \n",
    "print(\"书中g(D, A4)为0.363，函数结果：\" + str(round(infoGain(lhData[:,3] ,lhLabel), 3)))  \n",
    "# 测试正常，与书中结果一致\n",
    "# 使用西瓜数据测试函数  p75-p77\n",
    "xgData, xgLabel, xgName = createDataXG20()\n",
    "print(\"书中Ent(D)为0.998，函数结果：\" + str(round(singleEntropy(xgLabel), 4)))  \n",
    "print(\"书中Gain(D, 色泽)为0.109，函数结果：\" + str(round(infoGain(xgData[:,0] ,xgLabel), 4)))  \n",
    "print(\"书中Gain(D, 根蒂)为0.143，函数结果：\" + str(round(infoGain(xgData[:,1] ,xgLabel), 4)))  \n",
    "print(\"书中Gain(D, 敲声)为0.141，函数结果：\" + str(round(infoGain(xgData[:,2] ,xgLabel), 4)))  \n",
    "print(\"书中Gain(D, 纹理)为0.381，函数结果：\" + str(round(infoGain(xgData[:,3] ,xgLabel), 4)))  \n",
    "print(\"书中Gain(D, 脐部)为0.289，函数结果：\" + str(round(infoGain(xgData[:,4] ,xgLabel), 4)))  \n",
    "print(\"书中Gain(D, 触感)为0.006，函数结果：\" + str(round(infoGain(xgData[:,5] ,xgLabel), 4)))\n",
    "'''\n",
    " \n",
    "# 特征选取\n",
    "def bestFeature(data, labels, method = 'id3'):\n",
    "    assert method in ['id3', 'c45'], \"method 须为id3或c45\"\n",
    "    data = np.asarray(data)\n",
    "    labels = np.asarray(labels)\n",
    "    # 根据输入的method选取 评估特征的方法：id3 -> 信息增益; c45 -> 信息增益率\n",
    "    def calcEnt(feature, labels):\n",
    "        if method == 'id3':\n",
    "            return infoGain(feature, labels)\n",
    "        elif method == 'c45' :\n",
    "            return infoGainRatio(feature, labels)\n",
    "    # 特征数量  即 data 的列数量\n",
    "    featureNum = data.shape[1]\n",
    "    # 计算最佳特征\n",
    "    bestEnt = 0 \n",
    "    bestFeat = -1\n",
    "    for feature in range(featureNum):\n",
    "        ent = calcEnt(data[:, feature], labels)\n",
    "        if ent >= bestEnt:\n",
    "            bestEnt = ent \n",
    "            bestFeat = feature\n",
    "        # print(\"feature \" + str(feature + 1) + \" ent: \" + str(ent)+ \"\\t bestEnt: \" + str(bestEnt))\n",
    "    return bestFeat, bestEnt \n",
    " \n",
    " \n",
    "# 根据特征及特征值分割原数据集  删除data中的feature列，并根据feature列中的值分割 data和label\n",
    "def splitFeatureData(data, labels, feature):\n",
    "    \"\"\"feature 为特征列的索引\"\"\"\n",
    "    # 取特征列\n",
    "    features = np.asarray(data)[:,feature]\n",
    "    # 数据集中删除特征列\n",
    "    data = np.delete(np.asarray(data), feature, axis = 1)\n",
    "    # 标签\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    uniqFeatures = set(features)\n",
    "    dataSet = {}\n",
    "    labelSet = {}\n",
    "    for feat in uniqFeatures:\n",
    "        dataSet[feat] = data[features == feat]\n",
    "        labelSet[feat] = labels[features == feat]\n",
    "    return dataSet, labelSet\n",
    "    \n",
    "    \n",
    "# 多数投票 \n",
    "def voteLabel(labels):\n",
    "    uniqLabels = list(set(labels))\n",
    "    labels = np.asarray(labels)\n",
    " \n",
    "    finalLabel = 0\n",
    "    labelNum = []\n",
    "    for label in uniqLabels:\n",
    "        # 统计每个标签值得数量\n",
    "        labelNum.append(equalNums(labels, label))\n",
    "    # 返回数量最大的标签\n",
    "    return uniqLabels[labelNum.index(max(labelNum))]\n",
    " \n",
    " \n",
    "# 创建决策树\n",
    "def createTree(data, labels, names, method = 'id3'):\n",
    "    data = np.asarray(data)\n",
    "    labels = np.asarray(labels)\n",
    "    names = np.asarray(names)\n",
    "    # 如果结果为单一结果\n",
    "    if len(set(labels)) == 1: \n",
    "        return labels[0] \n",
    "    # 如果没有待分类特征\n",
    "    elif data.size == 0: \n",
    "        return voteLabel(labels)\n",
    "    # 其他情况则选取特征 \n",
    "    bestFeat, bestEnt = bestFeature(data, labels, method = method)\n",
    "    # 取特征名称\n",
    "    bestFeatName = names[bestFeat]\n",
    "    # 从特征名称列表删除已取得特征名称\n",
    "    names = np.delete(names, [bestFeat])\n",
    "    # 根据选取的特征名称创建树节点\n",
    "    decisionTree = {bestFeatName: {}}\n",
    "    # 根据最优特征进行分割\n",
    "    dataSet, labelSet = splitFeatureData(data, labels, bestFeat)\n",
    "    # 对最优特征的每个特征值所分的数据子集进行计算\n",
    "    for featValue in dataSet.keys():\n",
    "        decisionTree[bestFeatName][featValue] = createTree(dataSet.get(featValue), labelSet.get(featValue), names, method)\n",
    "    return decisionTree \n",
    " \n",
    " \n",
    "# 树信息统计 叶子节点数量 和 树深度\n",
    "def getTreeSize(decisionTree):\n",
    "    nodeName = list(decisionTree.keys())[0]\n",
    "    nodeValue = decisionTree[nodeName]\n",
    "    leafNum = 0\n",
    "    treeDepth = 0 \n",
    "    leafDepth = 0\n",
    "    for val in nodeValue.keys():\n",
    "        if type(nodeValue[val]) == dict:\n",
    "            leafNum += getTreeSize(nodeValue[val])[0]\n",
    "            leafDepth = 1 + getTreeSize(nodeValue[val])[1] \n",
    "        else :\n",
    "            leafNum += 1 \n",
    "            leafDepth = 1 \n",
    "        treeDepth = max(treeDepth, leafDepth)\n",
    "    return leafNum, treeDepth \n",
    " \n",
    " \n",
    "# 使用模型对其他数据分类\n",
    "def dtClassify(decisionTree, rowData, names):\n",
    "    names = list(names)\n",
    "    # 获取特征\n",
    "    feature = list(decisionTree.keys())[0]\n",
    "    # 决策树对于该特征的值的判断字段\n",
    "    featDict = decisionTree[feature]\n",
    "    # 获取特征的列\n",
    "    feat = names.index(feature)\n",
    "    # 获取数据该特征的值\n",
    "    featVal = rowData[feat]\n",
    "    # 根据特征值查找结果，如果结果是字典说明是子树，调用本函数递归\n",
    "    if featVal in featDict.keys():\n",
    "        if type(featDict[featVal]) == dict:\n",
    "            classLabel = dtClassify(featDict[featVal], rowData, names)\n",
    "        else:\n",
    "            classLabel = featDict[featVal] \n",
    "    return classLabel\n",
    " \n",
    "#获取叶节点的数目和树的层数\n",
    "def getNumLeafs(tree):\n",
    "    numLeafs = 0\n",
    "    #获取第一个节点的分类特征\n",
    "    firstFeat = list(tree.keys())[0]\n",
    "    #得到firstFeat特征下的决策树（以字典方式表示）\n",
    "    secondDict = tree[firstFeat]\n",
    "    #遍历firstFeat下的每个节点\n",
    "    for key in secondDict.keys():\n",
    "        #如果节点类型为字典，说明该节点下仍然是一棵树，此时递归调用getNumLeafs\n",
    "        if type(secondDict[key]).__name__== 'dict':\n",
    "            numLeafs += getNumLeafs(secondDict[key])\n",
    "        #否则该节点为叶节点\n",
    "        else:\n",
    "            numLeafs += 1\n",
    "    return numLeafs\n",
    " \n",
    "#获取决策树深度\n",
    "def getTreeDepth(tree):\n",
    "    maxDepth = 0\n",
    "    #获取第一个节点分类特征\n",
    "    firstFeat = list(tree.keys())[0]\n",
    "    #得到firstFeat特征下的决策树（以字典方式表示）\n",
    "    secondDict = tree[firstFeat]\n",
    "    #遍历firstFeat下的每个节点，返回子树中的最大深度\n",
    "    for key in secondDict.keys():\n",
    "        #如果节点类型为字典，说明该节点下仍然是一棵树，此时递归调用getTreeDepth，获取该子树深度\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:\n",
    "            thisDepth = 1\n",
    "        if thisDepth > maxDepth:\n",
    "            maxDepth = thisDepth\n",
    "    return maxDepth\n",
    " \n",
    " \n",
    "    #画出决策树\n",
    "def createPlot(tree):\n",
    "    # 定义一块画布，背景为白色\n",
    "    fig = plt.figure(1, facecolor='white')\n",
    "    # 清空画布\n",
    "    fig.clf()\n",
    "    # 不显示x、y轴刻度\n",
    "    xyticks = dict(xticks=[], yticks=[])\n",
    "    # frameon：是否绘制坐标轴矩形\n",
    "    createPlot.pTree = plt.subplot(111, frameon=False, **xyticks)\n",
    "    # 计算决策树叶子节点个数\n",
    "    plotTree.totalW = float(getNumLeafs(tree))\n",
    "    # 计算决策树深度\n",
    "    plotTree.totalD = float(getTreeDepth(tree))\n",
    "    # 最近绘制的叶子节点的x坐标\n",
    "    plotTree.xOff = -0.5 / plotTree.totalW\n",
    "    # 当前绘制的深度：y坐标\n",
    "    plotTree.yOff = 1.0\n",
    "    # （0.5,1.0）为根节点坐标\n",
    "    plotTree(tree, (0.5, 1.0), '')\n",
    "    plt.show()\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "# nodeText:要显示的文本；centerPt：文本中心点，即箭头所在的点；parentPt：指向文本的点；nodeType:节点属性\n",
    "# ha='center'，va='center':水平、垂直方向中心对齐；bbox：方框属性\n",
    "# arrowprops：箭头属性\n",
    "# xycoords，textcoords选择坐标系；axes fraction-->0,0是轴域左下角，1,1是右上角\n",
    "def plotNode(nodeText, centerPt, parentPt, nodeType):\n",
    "    createPlot.pTree.annotate(nodeText, xy=parentPt, xycoords=\"axes fraction\",\n",
    "                              xytext=centerPt, textcoords='axes fraction',\n",
    "                              va='center', ha='center', bbox=nodeType, arrowprops=arrowArgs)\n",
    "def plotMidText(centerPt, parentPt, midText):\n",
    "    xMid = (parentPt[0] - centerPt[0]) / 2.0 + centerPt[0]\n",
    "    yMid = (parentPt[1] - centerPt[1]) / 2.0 + centerPt[1]\n",
    "    createPlot.pTree.text(xMid, yMid, midText)\n",
    " \n",
    "def plotTree(tree, parentPt, nodeTxt):\n",
    "    #计算叶子节点个数\n",
    "    numLeafs = getNumLeafs(tree)\n",
    "    #获取第一个节点特征\n",
    "    firstFeat = list(tree.keys())[0]\n",
    "    #计算当前节点的x坐标\n",
    "    centerPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)\n",
    "    #绘制当前节点\n",
    "    plotMidText(centerPt,parentPt,nodeTxt)\n",
    "    plotNode(firstFeat,centerPt,parentPt,decisionNodeStyle)\n",
    "    secondDict = tree[firstFeat]\n",
    "    #计算绘制深度\n",
    "    plotTree.yOff -= 1.0/plotTree.totalD\n",
    "    for key in secondDict.keys():\n",
    "        #如果当前节点的子节点不是叶子节点，则递归\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            plotTree(secondDict[key],centerPt,str(key))\n",
    "        #如果当前节点的子节点是叶子节点，则绘制该叶节点\n",
    "        else:\n",
    "            #plotTree.xOff在绘制叶节点坐标的时候才会发生改变\n",
    "            plotTree.xOff += 1.0/plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff,plotTree.yOff),centerPt,leafNodeStyle)\n",
    "            plotMidText((plotTree.xOff,plotTree.yOff),centerPt,str(key))\n",
    "    plotTree.yOff += 1.0/plotTree.totalD\n",
    " \n",
    " \n",
    " \n",
    "# 使用李航数据测试函数 p62\n",
    "lhData, lhLabel, lhName = createDataLH()\n",
    "lhTree = createTree(lhData, lhLabel, lhName, method = 'id3')\n",
    "#print(lhTree)\n",
    "#createPlot(lhTree)\n",
    " \n",
    " \n",
    " \n",
    "# 使用西瓜数据测试函数  p75-p77\n",
    "xgData, xgLabel, xgName = createDataXG20()\n",
    "xgTree = createTree(xgData, xgLabel, xgName, method = 'id3')\n",
    "#print(xgTree)\n",
    "#createPlot(xgTree)\n",
    " \n",
    " \n",
    " \n",
    "#使用自己的数据集测试函数\n",
    "myData,myLabel,myName = createDataMine()\n",
    "myTree = createTree(myData,myLabel,myName,method='id3')\n",
    "#print(myTree)\n",
    "#createPlot(myTree)\n",
    " \n",
    " \n",
    "# 创建预剪枝决策树\n",
    "def createTreePrePruning(dataTrain, labelTrain, dataTest, labelTest, names, method = 'id3'):\n",
    "    \"\"\"\n",
    "    预剪枝 需要使用测试数据对每次的划分进行评估\n",
    "         策略说明：原本如果某节点划分前后的测试结果没有提升，根据奥卡姆剃刀原则将不进行划分（即执行剪枝），但考虑到这种策略容易造成欠拟合，\n",
    "                   且不能排除后续划分有进一步提升的可能，因此，没有提升仍保留划分，即不剪枝\n",
    "         另外：周志华的书上评估的是某一个节点划分前后对该层所有数据综合评估，如评估对脐部 凹陷下色泽是否划分，\n",
    "               书上取的色泽划分前的精度是71.4%(5/7)，划分后的精度是57.1%(4/7)，都是脐部下三个特征（凹陷，稍凹，平坦）所有的数据的精度，计算也不易\n",
    "               而我觉得实际计算时，只对当前节点下的数据划分前后进行评估即可，如脐部凹陷时有三个测试样本，\n",
    "               三个样本色泽划分前的精度是2/3=66.7%，色泽划分后的精度是1/3=33.3%，因此判断不划分\n",
    "    \"\"\"\n",
    "    trainData = np.asarray(dataTrain)\n",
    "    labelTrain = np.asarray(labelTrain)\n",
    "    testData = np.asarray(dataTest)\n",
    "    labelTest = np.asarray(labelTest)\n",
    "    names = np.asarray(names)\n",
    "    # 如果结果为单一结果\n",
    "    if len(set(labelTrain)) == 1: \n",
    "        return labelTrain[0] \n",
    "    # 如果没有待分类特征\n",
    "    elif trainData.size == 0: \n",
    "        return voteLabel(labelTrain)\n",
    "    # 其他情况则选取特征 \n",
    "    bestFeat, bestEnt = bestFeature(dataTrain, labelTrain, method = method)\n",
    "    # 取特征名称\n",
    "    bestFeatName = names[bestFeat]\n",
    "    # 从特征名称列表删除已取得特征名称\n",
    "    names = np.delete(names, [bestFeat])\n",
    "    # 根据最优特征进行分割\n",
    "    dataTrainSet, labelTrainSet = splitFeatureData(dataTrain, labelTrain, bestFeat)\n",
    " \n",
    "    # 预剪枝评估\n",
    "    # 划分前的分类标签\n",
    "    labelTrainLabelPre = voteLabel(labelTrain)\n",
    "    labelTrainRatioPre = equalNums(labelTrain, labelTrainLabelPre) / labelTrain.size\n",
    "    # 划分后的精度计算 \n",
    "    if dataTest is not None: \n",
    "        dataTestSet, labelTestSet = splitFeatureData(dataTest, labelTest, bestFeat)\n",
    "        # 划分前的测试标签正确比例\n",
    "        labelTestRatioPre = equalNums(labelTest, labelTrainLabelPre) / labelTest.size\n",
    "        # 划分后 每个特征值的分类标签正确的数量\n",
    "        labelTrainEqNumPost = 0\n",
    "        for val in labelTrainSet.keys():\n",
    "            labelTrainEqNumPost += equalNums(labelTestSet.get(val), voteLabel(labelTrainSet.get(val))) + 0.0\n",
    "        # 划分后 正确的比例\n",
    "        labelTestRatioPost = labelTrainEqNumPost / labelTest.size \n",
    "    \n",
    "    # 如果没有评估数据 但划分前的精度等于最小值0.5 则继续划分\n",
    "    if dataTest is None and labelTrainRatioPre == 0.5:\n",
    "        decisionTree = {bestFeatName: {}}\n",
    "        for featValue in dataTrainSet.keys():\n",
    "            decisionTree[bestFeatName][featValue] = createTreePrePruning(dataTrainSet.get(featValue), labelTrainSet.get(featValue)\n",
    "                                      , None, None, names, method)\n",
    "    elif dataTest is None:\n",
    "        return labelTrainLabelPre \n",
    "    # 如果划分后的精度相比划分前的精度下降, 则直接作为叶子节点返回\n",
    "    elif labelTestRatioPost < labelTestRatioPre:\n",
    "        return labelTrainLabelPre\n",
    "    else :\n",
    "        # 根据选取的特征名称创建树节点\n",
    "        decisionTree = {bestFeatName: {}}\n",
    "        # 对最优特征的每个特征值所分的数据子集进行计算\n",
    "        for featValue in dataTrainSet.keys():\n",
    "            decisionTree[bestFeatName][featValue] = createTreePrePruning(dataTrainSet.get(featValue), labelTrainSet.get(featValue)\n",
    "                                      , dataTestSet.get(featValue), labelTestSet.get(featValue)\n",
    "                                      , names, method)\n",
    "    return decisionTree \n",
    " \n",
    " \n",
    "# 创建决策树 带预划分标签\n",
    "def createTreeWithLabel(data, labels, names, method = 'id3'):\n",
    "    data = np.asarray(data)\n",
    "    labels = np.asarray(labels)\n",
    "    names = np.asarray(names)\n",
    "    # 如果不划分的标签为\n",
    "    votedLabel = voteLabel(labels)\n",
    "    # 如果结果为单一结果\n",
    "    if len(set(labels)) == 1: \n",
    "        return votedLabel \n",
    "    # 如果没有待分类特征\n",
    "    elif data.size == 0: \n",
    "        return votedLabel\n",
    "    # 其他情况则选取特征 \n",
    "    bestFeat, bestEnt = bestFeature(data, labels, method = method)\n",
    "    # 取特征名称\n",
    "    bestFeatName = names[bestFeat]\n",
    "    # 从特征名称列表删除已取得特征名称\n",
    "    names = np.delete(names, [bestFeat])\n",
    "    # 根据选取的特征名称创建树节点 划分前的标签votedPreDivisionLabel=_vpdl\n",
    "    decisionTree = {bestFeatName: {\"_vpdl\": votedLabel}}\n",
    "    # 根据最优特征进行分割\n",
    "    dataSet, labelSet = splitFeatureData(data, labels, bestFeat)\n",
    "    # 对最优特征的每个特征值所分的数据子集进行计算\n",
    "    for featValue in dataSet.keys():\n",
    "        decisionTree[bestFeatName][featValue] = createTreeWithLabel(dataSet.get(featValue), labelSet.get(featValue), names, method)\n",
    "    return decisionTree \n",
    " \n",
    " \n",
    "# 将带预划分标签的tree转化为常规的tree\n",
    "# 函数中进行的copy操作，原因见有道笔记 【YL20190621】关于Python中字典存储修改的思考\n",
    "def convertTree(labeledTree):\n",
    "    labeledTreeNew = labeledTree.copy()\n",
    "    nodeName = list(labeledTree.keys())[0]\n",
    "    labeledTreeNew[nodeName] = labeledTree[nodeName].copy()\n",
    "    for val in list(labeledTree[nodeName].keys()):\n",
    "        if val == \"_vpdl\": \n",
    "            labeledTreeNew[nodeName].pop(val)\n",
    "        elif type(labeledTree[nodeName][val]) == dict:\n",
    "            labeledTreeNew[nodeName][val] = convertTree(labeledTree[nodeName][val])\n",
    "    return labeledTreeNew\n",
    " \n",
    " \n",
    "# 后剪枝 训练完成后决策节点进行替换评估  这里可以直接对xgTreeTrain进行操作\n",
    "def treePostPruning(labeledTree, dataTest, labelTest, names):\n",
    "    newTree = labeledTree.copy()\n",
    "    dataTest = np.asarray(dataTest)\n",
    "    labelTest = np.asarray(labelTest)\n",
    "    names = np.asarray(names)\n",
    "    # 取决策节点的名称 即特征的名称\n",
    "    featName = list(labeledTree.keys())[0]\n",
    "    #print(\"\\n当前节点：\" + featName)\n",
    "    # 取特征的列\n",
    "    featCol = np.argwhere(names==featName)[0][0]\n",
    "    names = np.delete(names, [featCol])\n",
    "    #print(\"当前节点划分的数据维度：\" + str(names))\n",
    "    #print(\"当前节点划分的数据：\" )\n",
    "    #print(dataTest)\n",
    "    #print(labelTest)\n",
    "    # 该特征下所有值的字典\n",
    "    newTree[featName] = labeledTree[featName].copy()\n",
    "    featValueDict = newTree[featName]\n",
    "    featPreLabel = featValueDict.pop(\"_vpdl\")\n",
    "    #print(\"当前节点预划分标签：\" + featPreLabel)\n",
    "    # 是否为子树的标记\n",
    "    subTreeFlag = 0\n",
    "    # 分割测试数据 如果有数据 则进行测试或递归调用  np的array我不知道怎么判断是否None, 用is None是错的\n",
    "    dataFlag = 1 if sum(dataTest.shape) > 0 else 0\n",
    "    if dataFlag == 1:\n",
    "        # print(\"当前节点有划分数据！\")\n",
    "        dataTestSet, labelTestSet = splitFeatureData(dataTest, labelTest, featCol)\n",
    "    for featValue in featValueDict.keys():\n",
    "        # print(\"当前节点属性 {0} 的子节点：{1}\".format(featValue ,str(featValueDict[featValue])))\n",
    "        if dataFlag == 1 and type(featValueDict[featValue]) == dict:\n",
    "            subTreeFlag = 1 \n",
    "            # 如果是子树则递归\n",
    "            newTree[featName][featValue] = treePostPruning(featValueDict[featValue], dataTestSet.get(featValue), labelTestSet.get(featValue), names)\n",
    "            # 如果递归后为叶子 则后续进行评估\n",
    "            if type(featValueDict[featValue]) != dict:\n",
    "                subTreeFlag = 0 \n",
    "            \n",
    "        # 如果没有数据  则转换子树\n",
    "        if dataFlag == 0 and type(featValueDict[featValue]) == dict: \n",
    "            subTreeFlag = 1 \n",
    "            # print(\"当前节点无划分数据！直接转换树：\"+str(featValueDict[featValue]))\n",
    "            newTree[featName][featValue] = convertTree(featValueDict[featValue])\n",
    "            # print(\"转换结果：\" + str(convertTree(featValueDict[featValue])))\n",
    "    # 如果全为叶子节点， 评估需要划分前的标签，这里思考两种方法，\n",
    "    #     一是，不改变原来的训练函数，评估时使用训练数据对划分前的节点标签重新打标\n",
    "    #     二是，改进训练函数，在训练的同时为每个节点增加划分前的标签，这样可以保证评估时只使用测试数据，避免再次使用大量的训练数据\n",
    "    #     这里考虑第二种方法 写新的函数 createTreeWithLabel，当然也可以修改createTree来添加参数实现\n",
    "    if subTreeFlag == 0:\n",
    "        ratioPreDivision = equalNums(labelTest, featPreLabel) / labelTest.size\n",
    "        equalNum = 0\n",
    "        for val in labelTestSet.keys():\n",
    "            equalNum += equalNums(labelTestSet[val], featValueDict[val])\n",
    "        ratioAfterDivision = equalNum / labelTest.size \n",
    "        # print(\"当前节点预划分标签的准确率：\" + str(ratioPreDivision))\n",
    "        # print(\"当前节点划分后的准确率：\" + str(ratioAfterDivision))\n",
    "        # 如果划分后的测试数据准确率低于划分前的，则划分无效，进行剪枝，即使节点等于预划分标签\n",
    "        # 注意这里取的是小于，如果有需要 也可以取 小于等于\n",
    "        if ratioAfterDivision < ratioPreDivision:\n",
    "            newTree = featPreLabel \n",
    "    return newTree\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "# 将西瓜数据2.0分割为测试集和训练集\n",
    "xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest = splitXgData20(xgData, xgLabel)\n",
    "# 生成不剪枝的树\n",
    "xgTreeTrain = createTree(xgDataTrain, xgLabelTrain, xgName, method = 'id3')\n",
    "# 生成预剪枝的树\n",
    "xgTreePrePruning = createTreePrePruning(xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest, xgName, method = 'id3')\n",
    "# 画剪枝前的树\n",
    "#print(\"剪枝前的树\")\n",
    "#createPlot(xgTreeTrain)\n",
    "# 画剪枝后的树\n",
    "#print(\"剪枝后的树\")\n",
    "#createPlot(xgTreePrePruning)\n",
    " \n",
    " \n",
    "#将自己的数据分割为测试集和训练集\n",
    "myDataTrain, myLabelTrain, myDataTest, myLabelTest = splitMyData(myData,myLabel)\n",
    "# 生成不剪枝的树\n",
    "myTreeTrain = createTree(myDataTrain, myLabelTrain, myName, method='id3')\n",
    "# 生成预剪枝的树\n",
    "myTreePrePruning = createTreePrePruning(myDataTrain, myLabelTrain, myDataTest, myLabelTest, myName, method='id3')\n",
    "# 画剪枝前的树\n",
    "#print(\"剪枝前的树\")\n",
    "#createPlot(myTreeTrain)\n",
    "# 画剪枝后的树\n",
    "#print(\"剪枝后的树\")\n",
    "#createPlot(myTreePrePruning)\n",
    " \n",
    " \n",
    "# 书中的树结构 p81 p83\n",
    " \n",
    "xgTreeBeforePostPruning = {\"脐部\": {\"_vpdl\": \"是\"\n",
    "                                   , '凹陷': {'色泽':{\"_vpdl\": \"是\", '青绿': '是', '乌黑': '是', '浅白': '否'}}\n",
    "                                   , '稍凹': {'根蒂':{\"_vpdl\": \"是\"\n",
    "                                                  , '稍蜷': {'色泽': {\"_vpdl\": \"是\"\n",
    "                                                                  , '青绿': '是'\n",
    "                                                                  , '乌黑': {'纹理': {\"_vpdl\": \"是\"\n",
    "                                                                               , '稍糊': '是', '清晰': '否', '模糊': '是'}}\n",
    "                                                                  , '浅白': '是'}}\n",
    "                                                  , '蜷缩': '否'\n",
    "                                                  , '硬挺': '是'}}\n",
    "                                   , '平坦': '否'}}\n",
    " \n",
    "#xgTreeBeforePostPruning = createTreeWithLabel(xgDataTrain, xgLabelTrain, xgName, method='id3')\n",
    "#print(xgTreeBeforePostPruning)\n",
    "xgTreePostPruning = treePostPruning(xgTreeBeforePostPruning, xgDataTest, xgLabelTest, xgName)\n",
    "createPlot(convertTree(xgTreeBeforePostPruning))\n",
    "createPlot(xgTreePostPruning)\n",
    " \n",
    " \n",
    "myTreeBeforePostPruning = createTreeWithLabel(myDataTrain, myLabelTrain, myName, method='id3')\n",
    "#print(myTreeBeforePostPruning)\n",
    "myTreePostPruning = treePostPruning(myTreeBeforePostPruning, myDataTest, myLabelTest, myName)\n",
    "createPlot(convertTree(myTreeBeforePostPruning))\n",
    "createPlot(myTreePostPruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'色泽': {'浅白': '坏瓜', '青绿': {'敲声': {'沉闷': '坏瓜', '浊响': '好瓜', '清脆': '坏瓜'}}, '乌黑': {'根蒂': {'硬挺': '好瓜', '蜷缩': '好瓜', '稍蜷': {'纹理': {'清晰': '坏瓜', '模糊': '好瓜', '稍糊': '好瓜'}}}}}}\n",
      "base accuracy = 0.42857142857142855\n",
      "base accuracy = 0.5\n",
      "base accuracy = 0.5\n",
      "{'色泽': {'浅白': '坏瓜', '青绿': '好瓜', '乌黑': '好瓜'}}\n",
      "full tree's train accuracy = 1.0,test accuracy = 0.2857142857142857\n",
      "\n",
      "pre pruning tree's train accuracy = 1.0,test accuracy = 0.5714285714285714\n",
      "\n",
      "post pruning tree's train accuracy = 1.0,test accuracy = 0.5714285714285714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import operator\n",
    "\n",
    "# 特征字典，后面用到了好多次，干脆当全局变量了\n",
    "featureDic = {\n",
    "    '色泽': ['浅白', '青绿', '乌黑'],\n",
    "    '根蒂': ['硬挺', '蜷缩', '稍蜷'],\n",
    "    '敲声': ['沉闷', '浊响', '清脆'],\n",
    "    '纹理': ['清晰', '模糊', '稍糊'],\n",
    "    '脐部': ['凹陷', '平坦', '稍凹'],\n",
    "    '触感': ['硬滑', '软粘']}\n",
    "\n",
    "# ***********************画图***********************\n",
    "# **********************start***********************\n",
    "# 详情参见机器学习实战决策树那一章\n",
    "\n",
    "# 定义文本框和箭头格式\n",
    "decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")\n",
    "leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"<-\")\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  # 没有这句话汉字都是口口\n",
    "# mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题\n",
    "\n",
    "\n",
    "def plotMidText(cntrPt, parentPt, txtString):\n",
    "    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]\n",
    "    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, fontsize=20)\n",
    "\n",
    "\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):  # 绘制带箭头的注解\n",
    "    createPlot.ax1.annotate(nodeTxt,\n",
    "                            xy=parentPt,\n",
    "                            xycoords=\"axes fraction\",\n",
    "                            xytext=centerPt,\n",
    "                            textcoords=\"axes fraction\",\n",
    "                            va=\"center\",\n",
    "                            ha=\"center\",\n",
    "                            bbox=nodeType,\n",
    "                            arrowprops=arrow_args,\n",
    "                            fontsize=20)\n",
    "\n",
    "\n",
    "def getNumLeafs(myTree):  # 获取叶节点的数目\n",
    "    numLeafs = 0\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            numLeafs += getNumLeafs(secondDict[key])\n",
    "        else:\n",
    "            numLeafs += 1\n",
    "    return numLeafs\n",
    "\n",
    "\n",
    "def getTreeDepth(myTree):  # 获取树的层数\n",
    "    maxDepth = 0\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:\n",
    "            thisDepth = 1\n",
    "        if thisDepth > maxDepth: maxDepth = thisDepth\n",
    "    return maxDepth\n",
    "\n",
    "\n",
    "def plotTree(myTree, parentPt, nodeTxt):\n",
    "    numLeafs = getNumLeafs(myTree)\n",
    "    getTreeDepth(myTree)\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW,\n",
    "              plotTree.yOff)\n",
    "    plotMidText(cntrPt, parentPt, nodeTxt)\n",
    "    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n",
    "    secondDict = myTree[firstStr]\n",
    "    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            plotTree(secondDict[key], cntrPt, str(key))\n",
    "        else:\n",
    "            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff),\n",
    "                     cntrPt, leafNode)\n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n",
    "    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD\n",
    "\n",
    "\n",
    "def createPlot(inTree):\n",
    "    fig = plt.figure(1, figsize=(600, 30), facecolor='white')\n",
    "    fig.clf()\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)\n",
    "    plotTree.totalW = float(getNumLeafs(inTree))\n",
    "    plotTree.totalD = float(getTreeDepth(inTree))\n",
    "    plotTree.xOff = -0.5 / plotTree.totalW\n",
    "    plotTree.yOff = 1.0\n",
    "    plotTree(inTree, (0.5, 1.0), '')\n",
    "    plt.show()\n",
    "# ***********************画图***********************\n",
    "# ***********************end************************\n",
    "\n",
    "\n",
    "def getDataSet():\n",
    "    \"\"\"\n",
    "    get watermelon data set 3.0 alpha.\n",
    "    :return: 训练集合剪枝集以及特征列表。\n",
    "    \"\"\"\n",
    "    # 也可以直接从\n",
    "    dataSet = [\n",
    "        ['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '好瓜'],\n",
    "        ['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '好瓜'],\n",
    "        ['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '好瓜'],\n",
    "        ['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '好瓜'],\n",
    "        ['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '好瓜'],\n",
    "        ['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '好瓜'],\n",
    "        ['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘', '好瓜'],\n",
    "        ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑', '好瓜'],\n",
    "        ['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', '坏瓜'],\n",
    "        ['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘', '坏瓜'],\n",
    "        ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', '坏瓜'],\n",
    "        ['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘', '坏瓜'],\n",
    "        ['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑', '坏瓜'],\n",
    "        ['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', '坏瓜'],\n",
    "        ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '坏瓜'],\n",
    "        ['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑', '坏瓜'],\n",
    "        ['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑', '坏瓜']\n",
    "    ]\n",
    "\n",
    "    features = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感']\n",
    "\n",
    "    # #得到特征值字典，本来用这个生成的特征字典，还是直接当全局变量方便\n",
    "    # featureDic = {}\n",
    "    # for i in range(len(features)):\n",
    "    #     featureList = [example[i] for example in dataSet]\n",
    "    #     uniqueFeature = list(set(featureList))\n",
    "    #     featureDic[features[i]] = uniqueFeature\n",
    "\n",
    "    # 每种特征的属性个数\n",
    "    numList = []  # [3, 3, 3, 3, 3, 2]\n",
    "    for i in range(len(features)):\n",
    "        numList.append(len(featureDic[features[i]]))\n",
    "\n",
    "    # # 编码，把文字替换成数字。用1、2、3表示同种特征的不同类型\n",
    "    # newDataSet = []\n",
    "    # for dataVec in dataSet:  # 第一每一个数据\n",
    "    #     dataNum = dataVec[-1]  # 保存数据中类别部分\n",
    "    #     newData = []\n",
    "    #     for i in range(len(dataVec) - 1):  # 值为字符的每一列\n",
    "    #         for j in range(numList[i]):  # 对应列的特征的每一类\n",
    "    #             if dataVec[i] == featureDic[features[i]][j]:\n",
    "    #                 newData.append(j + 1)\n",
    "    #     newData.append(dataNum)  # 编码好的部分和原来的数值部分合并\n",
    "    #     newDataSet.append(newData)\n",
    "\n",
    "    newDataSet = np.array(dataSet)\n",
    "    # 得到训练数据集\n",
    "    trainIndex = [0, 1, 2, 5, 6, 9, 13, 14, 15, 16]\n",
    "    trainDataSet = newDataSet[trainIndex]\n",
    "    # 得到剪枝数据集\n",
    "    pruneIndex = [3 ,4, 7, 8, 10, 11, 12]\n",
    "    pruneDataSet = newDataSet[pruneIndex]\n",
    "\n",
    "    return np.array(dataSet), trainDataSet, pruneDataSet, features\n",
    "\n",
    "\n",
    "def calGini(dataArr):\n",
    "    \"\"\"\n",
    "    calculate information entropy.\n",
    "    :param dataArr:\n",
    "    :param classArr:\n",
    "    :return: Gini\n",
    "    \"\"\"\n",
    "    numEntries = dataArr.shape[0]\n",
    "    classArr = dataArr[:, -1]\n",
    "    uniqueClass = list(set(classArr))\n",
    "    Gini = 1.0\n",
    "    for c in uniqueClass:\n",
    "        Gini -= (len(dataArr[dataArr[:, -1] == c]) / float(numEntries)) ** 2\n",
    "    return Gini\n",
    "\n",
    "\n",
    "def splitDataSet(dataSet, ax, value):\n",
    "    \"\"\"\n",
    "    按照给点的属性ax和其中一种取值value来划分数据。\n",
    "    当属性类型为标称数据时，返回一个属性值都为value的数据集。\n",
    "    input:\n",
    "        dataSet: 输入数据集，形状为(m,n)表示m个数据，前n-1列个属性，最后一列为类型。\n",
    "        ax：属性类型\n",
    "        value: 标称型时为1、2、3等。数值型为形如0.123的数。\n",
    "    return：\n",
    "        标称型dataSet返回第ax个属性中值为value组成的集合\n",
    "    \"\"\"\n",
    "    return np.delete(dataSet[dataSet[:, ax] == value], ax, axis=1)\n",
    "\n",
    "\n",
    "def calSplitGin(dataSet, ax, labels):\n",
    "    \"\"\"\n",
    "    计算给定数据dataSet在属性ax上的基尼指数。\n",
    "    input：\n",
    "        dataSet：输入数据集，形状为(m,n)表示m个数据，前n-1列个属性，最后一列为类型。\n",
    "        labelList：属性列表，如['色泽', '根蒂', '敲声', '纹理', '脐部', '触感']\n",
    "        ax: 选择用来计算信息增益的属性。0表示第一个属性，1表示第二个属性等。\n",
    "    return：\n",
    "        Gini:基尼指数\n",
    "    \"\"\"\n",
    "    newGini = 0.0  # 划分完数据后的基尼指数\n",
    "    # 对每一种属性\n",
    "    for j in featureDic[ax]:\n",
    "        axIndex = labels.index(ax)\n",
    "        subDataSet = splitDataSet(dataSet, axIndex, j)\n",
    "        prob = len(subDataSet) / float(len(dataSet))\n",
    "        if prob != 0:  # prob为0意味着dataSet的ax属性中，没有第j+1种值\n",
    "            newGini += prob * calGini(subDataSet)\n",
    "    return newGini\n",
    "\n",
    "\n",
    "def chooseBestSplit(dataSet, labelList):\n",
    "    \"\"\"\n",
    "    得到基尼指数最小的属性作为最有划分属性。\n",
    "    input:\n",
    "        dataSet\n",
    "        labelList\n",
    "    return:\n",
    "        bestFeature: 使得到最大增益划分的属性。\n",
    "    \"\"\"\n",
    "    bestGain = 1\n",
    "    bestFeature = -1\n",
    "    n = dataSet.shape[1]\n",
    "    # 对每一个特征\n",
    "    for i in range(n - 1):\n",
    "        newGini = calSplitGin(dataSet, labelList[i], labelList)\n",
    "        if newGini < bestGain:\n",
    "            bestFeature = i\n",
    "            bestGain = newGini\n",
    "\n",
    "    return bestFeature\n",
    "\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount:\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    # classCount.items()将字典的key-value对变成元组对，如{'a':1, 'b':2} -> [('a',1),('b',2)]\n",
    "    # operator.itemgetter(1)按照第二个元素次序进行排序\n",
    "    # reverse=True表示从大大到小。[('b',2), ('a',1)]\n",
    "    sortedClassCount = sorted(classCount.items(),\n",
    "                              key=operator.itemgetter(1),\n",
    "                              reverse=True)\n",
    "    return sortedClassCount[0][0]   # 返回第0个元组的第0个值\n",
    "\n",
    "\n",
    "def createTree(dataSet, labels):\n",
    "    \"\"\"\n",
    "    通过信息增益递归创造一颗决策树。\n",
    "    input:\n",
    "        labels\n",
    "        dataSet\n",
    "    return:\n",
    "        myTree: 返回一个存有树的字典\n",
    "    \"\"\"\n",
    "    classList = dataSet[:, -1]\n",
    "    # 如果基尼指数为0，即D中样本全属于同一类别，返回\n",
    "    if calGini(dataSet) == 0:\n",
    "        return dataSet[0][-1]\n",
    "    # 属性值为空，只剩下类标签\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    # 得到增益最大划分的属性、值\n",
    "    bestFeat = chooseBestSplit(dataSet, labels)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel: {}}  # 创建字典，即树的节点。\n",
    "    # 生成子树的时候要将已遍历的属性删去。数值型不要删除。\n",
    "    labelsCopy = labels[:]\n",
    "    del (labelsCopy[bestFeat])\n",
    "    uniqueVals = featureDic[bestFeatLabel]  # 最好的特征的类别列表\n",
    "    for value in uniqueVals:  # 标称型的属性值有几种，就要几个子树。\n",
    "        # Python中列表作为参数类型时，是按照引用传递的，要保证同一节点的子节点能有相同的参数。\n",
    "        subLabels = labelsCopy[:]  # subLabels = 注意要用[:]，不然还是引用\n",
    "        subDataSet = splitDataSet(dataSet, bestFeat, value)\n",
    "        if len(subDataSet) != 0:\n",
    "            myTree[bestFeatLabel][value] = createTree(subDataSet, subLabels)\n",
    "        else:\n",
    "            # 计算D中样本最多的类\n",
    "            myTree[bestFeatLabel][value] = majorityCnt(classList)\n",
    "\n",
    "    return myTree\n",
    "\n",
    "\n",
    "def classify(data, featLabels, Tree):\n",
    "    \"\"\"\n",
    "    通过决策树对一条数据分类\n",
    "    :param featLabels:\n",
    "    :param data:\n",
    "    :param Tree:\n",
    "    :return: 分类\n",
    "    \"\"\"\n",
    "    firstStr = list(Tree.keys())[0]  # 父节点\n",
    "    secondDict = Tree[firstStr]  # 父节点下的子树，即子字典\n",
    "    featIndex = featLabels.index(firstStr)  # 当前属性标识的位置\n",
    "    classLabel = \"\"\n",
    "    for key in secondDict.keys():  # 遍历该属性下的不同类\n",
    "        if data[featIndex] == key:  # 如果数据中找到了匹配的属性类别\n",
    "            # 如果不是叶子节点，继续向下遍历\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(data, featLabels, secondDict[key])\n",
    "            # 如果是叶子节点，返回该叶子节点的类型\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel\n",
    "\n",
    "\n",
    "def calAccuracy(dataSet, labels, Tree):\n",
    "    \"\"\"\n",
    "    计算已有决策树的精度\n",
    "    :param dataSet:\n",
    "    :param labels: ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感']\n",
    "    :param Tree:\n",
    "    :return: 决策树精度\n",
    "    \"\"\"\n",
    "    cntCorrect = 0\n",
    "    size = len(dataSet)\n",
    "    for i in range(size):\n",
    "        pre = classify(dataSet[i], labels, Tree)\n",
    "        if pre == dataSet[i][-1]:\n",
    "            cntCorrect += 1\n",
    "    return cntCorrect / float(size)\n",
    "\n",
    "\n",
    "def cntAccNums(dataSet, pruneSet):\n",
    "    \"\"\"\n",
    "    用于剪枝，用dataSet中多数的类作为节点类，计算pruneSet中有多少类是被分类正确的，然后返回正确\n",
    "    分类的数目。\n",
    "    :param dataSet: 训练集\n",
    "    :param pruneSet: 测试集\n",
    "    :return: 正确分类的数目\n",
    "    \"\"\"\n",
    "    nodeClass = majorityCnt(dataSet[:, -1])\n",
    "    rightCnt = 0\n",
    "    for vect in pruneSet:\n",
    "        if vect[-1] == nodeClass:\n",
    "            rightCnt += 1\n",
    "    return rightCnt\n",
    "\n",
    "\n",
    "def prePruning(dataSet, pruneSet, labels):\n",
    "    \"\"\"\n",
    "    每到一个节点要划分的时候：\n",
    "    1. 用这个节点上数据投票得出这个节点的类，即是\"好瓜\"还是\"坏瓜\"。\n",
    "    2. 用这个投票出来的类计算测试集中正确的点数。\n",
    "    3. 尝试计算一个节点向下划分时测试点的正确数。假如，当前属性为\"脐部\"，有三种\"凹陷\",\n",
    "    \"稍凹\",\"平坦\"，则可将训练集和测试集按照这三种属性值分为三部分，分别计算分类正确的点数并求和。\n",
    "    4 若尝试划分得到的正确点数少于不划分时得到的正确点数，则返回不划分时节点的类，否则继续划分。\n",
    "    :param dataSet: 训练数据集\n",
    "    :param pruneSet: 预剪枝数据集\n",
    "    :param labels:  属性标签\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    classList = dataSet[:, -1]\n",
    "\n",
    "    if calGini(dataSet) == 0:\n",
    "        return dataSet[0][-1]\n",
    "\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    # 获取最好特征\n",
    "    bestFeat = chooseBestSplit(dataSet, labels)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    # 计算初始正确率\n",
    "    baseRightNums = cntAccNums(dataSet, pruneSet)\n",
    "    # 得到最好划分属性取值\n",
    "    features = featureDic[bestFeatLabel]\n",
    "    # 计算尝试划分节点时的正确率\n",
    "    splitRightNums = 0.0\n",
    "    for value in features:\n",
    "        # 每个属性取值得到的子集\n",
    "        subDataSet = splitDataSet(dataSet, bestFeat, value)\n",
    "        if len(subDataSet) != 0:\n",
    "            # 把用来剪枝的子集也按照相应属性值划分下去\n",
    "            subPruneSet = splitDataSet(pruneSet, bestFeat, value)\n",
    "            splitRightNums += cntAccNums(subDataSet, subPruneSet)\n",
    "            print(splitRightNums)\n",
    "    if baseRightNums < splitRightNums:  # 如果不划分的正确点数少于尝试划分的点数，则继续划分。\n",
    "        myTree = {bestFeatLabel: {}}\n",
    "    else:\n",
    "        return majorityCnt(dataSet[:, -1])  # 否则，返回不划分时投票得到的类\n",
    "\n",
    "    # 以下代码和不预剪枝的代码大致相同，一点不同在于每次测试集也要参与划分。\n",
    "    for value in features:\n",
    "        subLabels = labels[:]\n",
    "        subDataSet = splitDataSet(dataSet, bestFeat, value)\n",
    "        subPruneSet = splitDataSet(pruneSet, bestFeat, value)\n",
    "        if len(subDataSet) != 0:\n",
    "            myTree[bestFeatLabel][value] = prePruning(subDataSet, subPruneSet, subLabels)\n",
    "        else:\n",
    "            # 计算D中样本最多的类\n",
    "            myTree[bestFeatLabel][value] = majorityCnt(classList)\n",
    "    return myTree\n",
    "\n",
    "\n",
    "def postPruning(dataSet, pruneSet, labels):\n",
    "    \"\"\"\n",
    "    后剪枝的思想就是，在决策树每一条分支到达叶子节点时，分别计算剪枝和不剪枝时，位于该节点上的\n",
    "    测试数据，被正确判定的数量孰大孰小，以此为依据来决定是否剪枝。\n",
    "    :param dataSet:\n",
    "    :param pruneSet:\n",
    "    :param labels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    classList = dataSet[:, -1]\n",
    "    # 如果基尼指数为0，即D中样本全属于同一类别，返回\n",
    "    if calGini(dataSet) == 0:\n",
    "        return dataSet[0][-1]\n",
    "    # 属性值为空，只剩下类标签\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    # 得到增益最大划分的属性、值\n",
    "    bestFeat = chooseBestSplit(dataSet, labels)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel: {}}  # 创建字典，即树的节点。\n",
    "    # 生成子树的时候要将已遍历的属性删去。数值型不要删除。\n",
    "    labelsCopy = labels[:]\n",
    "    del (labelsCopy[bestFeat])\n",
    "    uniqueVals = featureDic[bestFeatLabel]  # 最好的特征的类别列表\n",
    "    for value in uniqueVals:  # 标称型的属性值有几种，就要几个子树。\n",
    "        # Python中列表作为参数类型时，是按照引用传递的，要保证同一节点的子节点能有相同的参数。\n",
    "        subLabels = labelsCopy[:]  # subLabels = 注意要用[:]，不然还是引用\n",
    "        subPrune = splitDataSet(pruneSet, bestFeat, value)\n",
    "        subDataSet = splitDataSet(dataSet, bestFeat, value)\n",
    "        if len(subDataSet) != 0:\n",
    "            myTree[bestFeatLabel][value] = postPruning(subDataSet, subPrune, subLabels)\n",
    "        else:\n",
    "            # 计算D中样本最多的类\n",
    "            myTree[bestFeatLabel][value] = majorityCnt(classList)\n",
    "\n",
    "    # 后剪枝，如果到达叶子节点，尝试剪枝。\n",
    "    # 计算未剪枝时，测试集的正确数\n",
    "    numNoPrune = 0.0\n",
    "    for value in uniqueVals:\n",
    "        subDataSet = splitDataSet(dataSet, bestFeat, value)\n",
    "        if len(subDataSet) != 0:\n",
    "            subPrune = splitDataSet(pruneSet, bestFeat, value)\n",
    "            numNoPrune += cntAccNums(subDataSet, subPrune)\n",
    "    # 计算剪枝后，测试集正确数\n",
    "    numPrune = cntAccNums(dataSet, pruneSet)\n",
    "    # 比较决定是否剪枝, 如果剪枝后该节点上测试集的正确数变多了，则剪枝。\n",
    "    if numNoPrune < numPrune:\n",
    "        return majorityCnt(dataSet[:, -1])  # 直接返回节点上训练数据的多数类为节点类。\n",
    "\n",
    "    return myTree\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataSet, trainData, pruneData, labelList = getDataSet()\n",
    "    # 用训练集训练一颗树并画图\n",
    "    myTree = createTree(trainData, labelList)\n",
    "    print(myTree)\n",
    "    # createPlot(myTree)\n",
    "    # 画预剪枝树\n",
    "    preTree = prePruning(trainData, pruneData, labelList)\n",
    "    # createPlot(preTree)\n",
    "    # 画后剪枝树\n",
    "    postPTree = postPruning(trainData, pruneData, labelList)\n",
    "    print(preTree)\n",
    "    # createPlot(postPTree)\n",
    "    # 计算未剪枝的精度\n",
    "    print(f\"full tree's train accuracy = {calAccuracy(trainData, labelList, myTree)},\"\n",
    "          f\"test accuracy = {calAccuracy(pruneData, labelList, myTree)}\\n\")\n",
    "    # 计算预剪枝精度\n",
    "    print(f\"pre pruning tree's train accuracy = {calAccuracy(trainData, labelList, myTree)},\"\n",
    "          f\"test accuracy = {calAccuracy(pruneData, labelList, preTree)}\\n\")\n",
    "    # 计算后剪枝精度\n",
    "    print(f\"post pruning tree's train accuracy = {calAccuracy(trainData, labelList, myTree)},\"\n",
    "          f\"test accuracy = {calAccuracy(pruneData, labelList, postPTree)}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
