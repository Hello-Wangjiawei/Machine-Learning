{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3421d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是']\n",
      "['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是']\n",
      "['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是']\n",
      "['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是']\n",
      "['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是']\n",
      "['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '是']\n",
      "['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘', '是']\n",
      "['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑', '是']\n",
      "['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', '否']\n",
      "['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘', '否']\n",
      "['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', '否']\n",
      "['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘', '否']\n",
      "['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑', '否']\n",
      "['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', '否']\n",
      "['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘', '否']\n",
      "['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑', '否']\n",
      "['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑', '否']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 导入数据\n",
    "# 构建数据集\n",
    "df = pd.DataFrame(pd.read_csv(\"../Data/watermelon2.0.csv\", encoding=\"ansi\"))\n",
    "df.drop(labels=[\"编号\"], axis=1, inplace=True)  # 删除编号这一列，inplace=True表示直接在原对象修改\n",
    "# 转化为列表\n",
    "dataset = df.values.tolist()\n",
    "# 第4，5，8，9，11，12，13行作为测试集\n",
    "dataset_test = [dataset[i-1] for i in [4,5,8,9,11,12,13]]\n",
    "\n",
    "# 其余作为训练集\n",
    "dataset_train = [dataset[i-1] for i in range(len(dataset)) if i not in [4,5,8,9,11,12,13]]\n",
    "\n",
    "# 打印数据集\n",
    "for i in dataset:\n",
    "    print(i)\n",
    "    \n",
    "# 属性\n",
    "features = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1231ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 计算某个特征下某个取值的基尼系数\n",
    "def Gini_index(dataset):\n",
    "    '''\n",
    "    @brief:calculate the Gini index of a dataset\n",
    "    @param dataset: the dataset to be calculated\n",
    "    @return: the Gini index of the dataset\n",
    "    '''\n",
    "    num_samples  = len(dataset)\n",
    "    if(num_samples == 0):\n",
    "        return 0\n",
    "    # 统计该取值下对应每个标签的数量\n",
    "    label_count = {}\n",
    "    for sample in dataset:\n",
    "        if(sample[-1] not in label_count):\n",
    "            label_count[sample[-1]] = 0\n",
    "        label_count[sample[-1]] += 1\n",
    "    \n",
    "    # 计算Gini指数\n",
    "    Gini = 1.0\n",
    "    for i in label_count:\n",
    "        Pk = label_count[i] / num_samples\n",
    "        Gini -= Pk**2\n",
    "\n",
    "    return Gini\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f38f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "# 创建决策树（不含裁剪）\n",
    "def createTree(dataset, features):\n",
    "    '''\n",
    "    @brief: create a decision tree by using the ID3 algorithm\n",
    "    @param dataset: the dataset to be used for training\n",
    "    @param features: the features to be used for training\n",
    "    @return: the decision tree\n",
    "    '''\n",
    "    # 取出所有样本的标签\n",
    "    classList = [example[-1] for example in dataset]\n",
    "    # 如果所有样本的标签相同，则返回该标签\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 如果特征集为空，则返回出现次数最多的标签\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    # 选择最优特征进行数据集划分\n",
    "    bestfeatureIndex, bestValue = chooseBestFeatureToSplit(dataset)\n",
    "    bestFeatLabel = features[bestfeatureIndex]\n",
    "\n",
    "    # 创建节点\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    # 使用副本避免修改原始列表\n",
    "    subfeatures = features.copy()  \n",
    "    # 连续特征\n",
    "    if type(bestValue).__name__ == 'float':\n",
    "        myTree[bestFeatLabel]['<=' + str(bestValue)] = createTree(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, True), subfeatures)\n",
    "        myTree[bestFeatLabel]['>' + str(bestValue)] = createTree(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, False), subfeatures)\n",
    "    # 离散特征\n",
    "    else:\n",
    "        # 去除当前特征\n",
    "        del subfeatures[bestfeatureIndex] \n",
    "        # 取出当前特征的取值\n",
    "        featValue = [example[bestfeatureIndex] for example in dataset]\n",
    "        uniqueVals = set(featValue)\n",
    "        # 递归每一个特征值\n",
    "        for value in uniqueVals:\n",
    "            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset, bestfeatureIndex, value), subfeatures)\n",
    "\n",
    "    return myTree\n",
    "\n",
    "#创建决策树(预裁剪)\n",
    "def createTree_prepruning(dataset, features, dataset_test):\n",
    "    '''\n",
    "    @brief: create a decision tree by using the ID3 algorithm\n",
    "    @param dataset: the dataset to be used for training\n",
    "    @param features: the features to be used for training\n",
    "    @return: the decision tree\n",
    "    '''\n",
    "    # 取出所有样本的标签\n",
    "    classList = [example[-1] for example in dataset]\n",
    "    # 如果所有样本的标签相同，则返回该标签\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 如果特征集为空，则返回出现次数最多的标签\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    \n",
    "    # 计算不分裂时的准确率（叶节点多数类）\n",
    "    majority_class = majorityCnt(classList)\n",
    "    accuracy_before = sum(1 for ex in dataset_test if ex[-1] == majority_class) / len(dataset_test)\n",
    "\n",
    "    # 选择最优特征进行数据集划分\n",
    "    bestfeatureIndex, bestValue = chooseBestFeatureToSplit(dataset)\n",
    "    bestFeatLabel = features[bestfeatureIndex]\n",
    "\n",
    "    # 计算分裂后的准确率\n",
    "    # subdataset_test  = [ex for ex in dataset_test if ex[bestfeatureIndex] == bestValue]\n",
    "    accuracy_after = sum(1 for ex in subdataset_test if ex[-1] == majority_class) / len(subdataset_test) if subdataset_test else 0\n",
    "\n",
    "    # 创建节点\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    # 使用副本避免修改原始列表\n",
    "    subfeatures = features.copy()  \n",
    "\n",
    "    # 连续特征\n",
    "    if type(bestValue).__name__ == 'float':\n",
    "        myTree[bestFeatLabel]['<=' + str(bestValue)] = createTree_prepruning(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, True), subfeatures,dataset_test)\n",
    "        myTree[bestFeatLabel]['>' + str(bestValue)] = createTree_prepruning(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, False), subfeatures,dataset_test)\n",
    "    # 离散特征\n",
    "    else:\n",
    "        # 去除当前特征\n",
    "        del subfeatures[bestfeatureIndex] \n",
    "        # 取出当前特征的取值\n",
    "        featValue = [example[bestfeatureIndex] for example in dataset]\n",
    "        uniqueVals = set(featValue)\n",
    "        # 递归每一个特征值\n",
    "        for value in uniqueVals:\n",
    "            myTree[bestFeatLabel][value] = createTree_prepruning(splitDataSet(dataset, bestfeatureIndex, value), subfeatures,dataset_test)\n",
    "    # 计算分裂后的准确率\n",
    "    accuracy_after = calculateAccuracy(myTree, dataset_test)\n",
    "    if accuracy_after > accuracy_before:\n",
    "        return myTree\n",
    "    return majority_class\n",
    "    \n",
    "def postPrune(tree, train_data, test_data, features):\n",
    "    '''\n",
    "    @brief: post-prune the decision tree\n",
    "    @param tree: the decision tree to be pruned\n",
    "    @param train_data: the training dataset \n",
    "    @param test_data: the testing dataset\n",
    "    @param features: the features to be used for training\n",
    "    @return: the pruned decision tree\n",
    "    '''\n",
    "    if not isinstance(tree, dict):  # 到达叶节点，无需剪枝\n",
    "        return tree\n",
    "    \n",
    "    # 深度优先遍历子树\n",
    "    current_feat = list(tree.keys())[0]\n",
    "    sub_tree = tree[current_feat]\n",
    "    feat_idx = features.index(current_feat)\n",
    "    \n",
    "    # 递归剪枝所有子节点\n",
    "    for key in list(sub_tree.keys()):\n",
    "        # 处理连续特征（如 \"<=0.5\"）\n",
    "        if isinstance(key, str) and ('<=' in key or '>' in key):\n",
    "            # 提取阈值\n",
    "            threshold = float(key.split('=')[1])\n",
    "            # 划分训练集用于计算多数类\n",
    "            subset_train = [ex for ex in train_data if (ex[feat_idx] <= threshold and key.startswith('<=')) or (ex[feat_idx] > threshold and key.startswith('>'))]\n",
    "            # 递归剪枝子节点\n",
    "            sub_tree[key] = postPrune(sub_tree[key], subset_train, test_data, features)\n",
    "        # 处理离散特征（如 \"硬滑\"）\n",
    "        else:\n",
    "            subset_train = [ex for ex in train_data if ex[feat_idx] == key]\n",
    "            sub_tree[key] = postPrune(sub_tree[key], subset_train, test_data, features)\n",
    "    \n",
    "    # 尝试剪枝当前节点\n",
    "    accuracy_before = calculateAccuracy(tree, test_data)\n",
    "    \n",
    "    # 计算当前节点下训练集的多数类\n",
    "    class_list = [ex[-1] for ex in train_data]\n",
    "    majority_class = majorityCnt(class_list)\n",
    "    \n",
    "    # 计算替换为叶节点后的准确率\n",
    "    accuracy_after = sum(1 for ex in test_data if ex[-1] == majority_class) / len(test_data) if test_data else 0\n",
    "    \n",
    "    # 若剪枝后准确率不下降，则剪枝\n",
    "    if accuracy_after >= accuracy_before:\n",
    "        return majority_class\n",
    "    else:\n",
    "        return tree\n",
    "    \n",
    "# 计算类别中出现次数最多的元素\n",
    "def majorityCnt(classList):\n",
    "    # 创建一个字典{类标签:出现次数}\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    # 降序排序[(类标签,出现次数),(),()]\n",
    "    sortedclassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedclassCount[0][0]\n",
    "\n",
    "# 选择最优特征进行数据集划分\n",
    "def chooseBestFeatureToSplit(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    minGini = 1\n",
    "    bestFeature = -1\n",
    "    bestValue = 0\n",
    "    # 遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 取出第i个特征\n",
    "        featList = [example[i] for example in dataset]\n",
    "        # 连续特征\n",
    "        if type(featList[0]).__name__ == 'float':\n",
    "            # 排序\n",
    "            sortedfeatList = sorted(featList)\n",
    "            splitList = []\n",
    "            # 计算切分点\n",
    "            for j in range(len(sortedfeatList) - 1):\n",
    "                splitVal = (sortedfeatList[j] + sortedfeatList[j + 1]) / 2.0\n",
    "                splitList.append(splitVal)\n",
    "            # 计算Gini指数\n",
    "            for val in set(splitList):\n",
    "                Gini = 0\n",
    "                subDataSet1 = splitDataSetByValue(dataset, i, val, True)\n",
    "                subDataSet2 = splitDataSetByValue(dataset, i, val, False)\n",
    "                prob1 = len(subDataSet1) / float(len(dataset))\n",
    "                Gini1 = Gini_index(subDataSet1)\n",
    "                prob2 = len(subDataSet2) / float(len(dataset))\n",
    "                Gini2 = Gini_index(subDataSet2)\n",
    "                Gini += prob1 * Gini1 + prob2 * Gini2\n",
    "                \n",
    "                if (Gini < minGini):\n",
    "                    minGini = Gini\n",
    "                    bestFeature = i\n",
    "                    bestValue = val\n",
    "        # 离散特征\n",
    "        else:\n",
    "            uniqueVals = set(featList)\n",
    "            Gini = 0\n",
    "            # 遍历所有取值\n",
    "            for val in uniqueVals:\n",
    "                subDataSet = splitDataSet(dataset, i, val)\n",
    "                prob = len(subDataSet) / float(len(dataset))\n",
    "                Gini += prob * Gini_index(subDataSet)\n",
    "            \n",
    "            if (Gini < minGini):\n",
    "                minGini = Gini\n",
    "                bestFeature = i\n",
    "                bestValue = None\n",
    "    return bestFeature, bestValue\n",
    "\n",
    "# 根据特征值划分数据集\n",
    "def splitDataSet(dataset, axis, val):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if featVec[axis] == val:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis + 1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# 根据特征值和方向划分数据集\n",
    "def splitDataSetByValue(dataset, axis, val, direction):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if direction:\n",
    "            if featVec[axis] <= val:\n",
    "                reducedFeatVec = featVec[:axis]\n",
    "                reducedFeatVec.extend(featVec[axis + 1:])\n",
    "                retDataSet.append(reducedFeatVec)\n",
    "        else:\n",
    "            if featVec[axis] > val:\n",
    "                reducedFeatVec = featVec[:axis]\n",
    "                reducedFeatVec.extend(featVec[axis + 1:])\n",
    "                retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def predict(inputTree, features, testVec):\n",
    "    '''\n",
    "    @brief: predict the label of a test vector using a decision tree\n",
    "    @param inputTree: the decision tree to be used for prediction\n",
    "    @param features: the features to be used for training\n",
    "    @param testVec: the test vector to be predicted\n",
    "    @return: the predicted label of the test vector\n",
    "    '''\n",
    "    # 若为叶节点，则返回标签\n",
    "    if not isinstance(inputTree, dict):\n",
    "        return inputTree\n",
    "    \n",
    "    # 提取当前节点\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    # 提取当前节点下的子节点\n",
    "    secondDict = inputTree[firstStr]\n",
    "    # 获取当前节点的特征标签\n",
    "    featureIndex = features.index(firstStr)\n",
    "\n",
    "    for key in secondDict.keys():\n",
    "        # 处理连续特征（如 \"<=0.5\"）\n",
    "        if type(key).__name__ == 'str' and ('<=' in key or '>' in key):\n",
    "            # 移除字符串中的符号，取出阈值\n",
    "            threshold = float(key.replace('<=', '').replace('>', ''))\n",
    "            # 当前特征值小于等于阈值，则进入左子树\n",
    "            if key.startswith('<=') and testVec[featureIndex] <= threshold:\n",
    "                childTree = secondDict[key]\n",
    "                # 判断是否为内部节点，若是，则表示不是叶子节点，继续递归\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "            elif key.startswith('>') and testVec[featureIndex] > threshold:\n",
    "                childTree = secondDict[key]\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "        # 处理离散特征（如 \"硬滑\"）\n",
    "        else:\n",
    "            if testVec[featureIndex] == key:\n",
    "                childTree = secondDict[key]\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "    # 若未匹配任何分支\n",
    "    return \"未知类别\"  \n",
    "\n",
    "# 计算准确率\n",
    "def calculateAccuracy(inputTree,testData):\n",
    "    current_accuracy = 0\n",
    "    for sample in testData:\n",
    "        # 去除标签\n",
    "        testVec = sample[:-1]\n",
    "        # 预测\n",
    "        prediction = predict(inputTree, features, testVec)\n",
    "        # 计算准确率\n",
    "        if prediction == sample[-1]:\n",
    "            current_accuracy += 1\n",
    "    return current_accuracy / float(len(testData))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc96379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完整决策树:\n",
      "{'色泽': {'乌黑': {'根蒂': {'蜷缩': '是', '稍蜷': {'纹理': {'稍糊': '是', '清晰': '否'}}}}, '浅白': '否', '青绿': {'敲声': {'沉闷': '否', '浊响': '是', '清脆': '否'}}}}\n",
      "完整决策树的准确率: 0.2857142857142857\n",
      "预剪枝决策树:\n",
      "否\n",
      "预剪枝决策树的准确率: 0.5714285714285714\n",
      "后剪枝决策树:\n",
      "否\n",
      "后剪枝决策树的准确率: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# 创建决策树\n",
    "myWholeTree = createTree(dataset_train,features)\n",
    "# 打印决策树\n",
    "print(\"完整决策树:\")\n",
    "print(myWholeTree)\n",
    "# 计算准确率\n",
    "accuracy = calculateAccuracy(myWholeTree,dataset_test)\n",
    "print(\"完整决策树的准确率:\",accuracy)\n",
    "\n",
    "myPrepruningTree = createTree_prepruning(dataset_train,features,dataset_test)\n",
    "print(\"预剪枝决策树:\")\n",
    "print(myPrepruningTree)\n",
    "accuracy = calculateAccuracy(myPrepruningTree,dataset_test)\n",
    "print(\"预剪枝决策树的准确率:\",accuracy) \n",
    "\n",
    "myPostpruningTree = postPrune(myWholeTree,dataset_train,dataset_test,features)\n",
    "print(\"后剪枝决策树:\")\n",
    "print(myPostpruningTree)\n",
    "accuracy = calculateAccuracy(myPostpruningTree,dataset_test)\n",
    "print(\"后剪枝决策树的准确率:\",accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89f79a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'色泽': {'乌黑': {'根蒂': {'蜷缩': '是', '稍蜷': {'纹理': {'稍糊': '是', '清晰': '否'}}}}, '浅白': '否', '青绿': {'敲声': {'沉闷': '否', '浊响': '是', '清脆': '否'}}}}\n",
      "accuracy: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CART:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, Dataset, features):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \n",
    "        参数:\n",
    "        - Dataset: 训练数据集\n",
    "        - features: 特征列表 \n",
    "        \"\"\"\n",
    "        self.tree = self.createTree(Dataset, features)\n",
    "\n",
    "    def createTree(self, Dataset, features):\n",
    "        \"\"\" \n",
    "        创建决策树\n",
    "        \n",
    "        参数:\n",
    "        - Dataset: 训练数据集\n",
    "        - features: 特征列表\n",
    "        \n",
    "        返回:\n",
    "        - 决策树\n",
    "        \"\"\"\n",
    "        # 提取标签\n",
    "        classList = [example[-1] for example in Dataset]\n",
    "\n",
    "        # 如果所有样本的标签相同，则返回该标签\n",
    "        if len(set(classList)) == 1:\n",
    "            return classList[0]\n",
    "        \n",
    "        # 如果特征集为空，则返回出现次数最多的标签\n",
    "        if len(Dataset[0]) == 1:\n",
    "            return self.majorityCnt(classList)\n",
    "        \n",
    "        # 选择最优特征\n",
    "        bestFeatureIndex, bestSplitValue = self.chooseBestFeature(Dataset)\n",
    "        bestFeatureLabel = features[bestFeatureIndex]\n",
    "\n",
    "        # 创建节点\n",
    "        tree = {bestFeatureLabel: {}}\n",
    "        # 使用副本避免修改原始列表\n",
    "        subfeatures = features.copy()\n",
    "        # 删除当前特征\n",
    "        del subfeatures[bestFeatureIndex]\n",
    "        # 连续特征\n",
    "        if type(bestSplitValue).__name__ == 'float':\n",
    "            tree[bestFeatureLabel]['<=' + str(bestSplitValue)] = self.createTree(self.splitDataSetByValue(Dataset, bestFeatureIndex, bestSplitValue, False), subfeatures)\n",
    "            tree[bestFeatureLabel]['>' + str(bestSplitValue)] = self.createTree(self.splitDataSetByValue(Dataset, bestFeatureIndex, bestSplitValue, True), subfeatures)\n",
    "        # 离散特征\n",
    "        else:\n",
    "            # 取出当前特征的取值\n",
    "            featValue = [example[bestFeatureIndex] for example in Dataset]\n",
    "            uniqueVals = set(featValue)\n",
    "            # 遍历所有取值,开始递归\n",
    "            for value in uniqueVals:\n",
    "                subDataset = self.splitDataSet(Dataset, bestFeatureIndex, value)\n",
    "                tree[bestFeatureLabel][value] = self.createTree(subDataset, subfeatures)\n",
    "        return tree\n",
    "\n",
    "    def majorityCnt(self, classList):\n",
    "        \"\"\"返回最多的标签\"\"\"\n",
    "        # 统计标签出现的次数\n",
    "        label_count = {}\n",
    "        for label in classList:\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        # 降序排序[(类标签,出现次数),(),()]\n",
    "        sortedclassCount = sorted(label_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        return sortedclassCount[0][0]\n",
    "\n",
    "    def chooseBestFeature(self,Dataset):\n",
    "        \"\"\"通过信息增益选择最优特征\"\"\"\n",
    "        featureNum = len(Dataset[0]) - 1\n",
    "        bestGini = 1.0\n",
    "        bestSplitValue = 0\n",
    "        bestFeatureIndex = -1\n",
    "\n",
    "        # 遍历所有特征\n",
    "        for i in range(featureNum):\n",
    "            # 提取当前特征下的取值\n",
    "            featureValues = [example[i] for example in Dataset]\n",
    "            # 连续特征\n",
    "            if type(featureValues[0]).__name__ == 'float':\n",
    "                # 对特征值进行排序\n",
    "                sortedFeatureValues = sorted(featureValues)\n",
    "                # 计算分割值（取相邻两个取值的中点）\n",
    "                splitList = []\n",
    "                for j in range(len(sortedFeatureValues) - 1):\n",
    "                    splitList.append((sortedFeatureValues[j] + sortedFeatureValues[j + 1]) / 2.0)\n",
    "                # 遍历所有分割值,相当于做二分类\n",
    "                for splitValue in splitList:\n",
    "                    currentEntropy = 0.0\n",
    "                    subDataset1 = self.splitDataSetByValue(Dataset, i, splitValue, True)\n",
    "                    subDataset2 = self.splitDataSetByValue(Dataset, i, splitValue, False)\n",
    "                    prob1 = len(subDataset1) / float(len(Dataset))\n",
    "                    prob2 = len(subDataset2) / float(len(Dataset))\n",
    "                    currentEntropy  = prob1 * self.calculateGini(subDataset1) + prob2 * self.calculateGini(subDataset2)\n",
    "                    # 比较Gini指数，数值越小，纯度越高\n",
    "                    if (currentGini < bestGini):\n",
    "                        bestGini = currentGini\n",
    "                        bestFeatureIndex = i\n",
    "                        bestSplitValue = splitValue\n",
    "            # 离散特征\n",
    "            else:\n",
    "                uniqueValues = set(featureValues)\n",
    "                currentGini = 0.0\n",
    "                # 遍历所有取值,计算Gini指数\n",
    "                for value in uniqueValues:\n",
    "                    subDataset = self.splitDataSet(Dataset, i, value)\n",
    "                    prob = len(subDataset) / len(Dataset)\n",
    "                    currentGini += prob * self.calculateGini(subDataset)\n",
    "                # 比较Gini指数\n",
    "                if (currentGini < bestGini):\n",
    "                    bestGini = currentGini\n",
    "                    bestFeatureIndex = i\n",
    "                    bestSplitValue = None\n",
    "        \n",
    "        return bestFeatureIndex, bestSplitValue\n",
    "\n",
    "    def calculateGini(self, Dataset):\n",
    "        \"\"\"计算Gini指数,公式(4.5)\"\"\"\n",
    "        sample_num = len(Dataset)\n",
    "        # 统计标签出现的次数\n",
    "        label_count = {}\n",
    "        for featVec in Dataset:\n",
    "            label = featVec[-1]\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        \n",
    "        # 计算信息熵\n",
    "        Gini_index = 1.0\n",
    "        for count in label_count.values():\n",
    "            prob = float(count) / sample_num\n",
    "            Gini_index -= prob ** 2\n",
    "\n",
    "        return Gini_index\n",
    "\n",
    "    def splitDataSet(self,Dataset, axis, val):\n",
    "        '''\n",
    "        根据特征索引i和离散特征值value将数据集切分\n",
    "\n",
    "        参数:\n",
    "        - Dataset: 训练数据集\n",
    "        - axis: 特征索引\n",
    "        - val: 特征值\n",
    "\n",
    "        返回:\n",
    "        - 切分后的子集\n",
    "        '''\n",
    "        subDataset = []\n",
    "        # 遍历每一行\n",
    "        for featVec in Dataset:\n",
    "            if featVec[axis] == val:\n",
    "                reducedFeature = featVec[:axis]\n",
    "                reducedFeature.extend(featVec[axis + 1:])\n",
    "                subDataset.append(reducedFeature)\n",
    "        return subDataset\n",
    "\n",
    "    def splitDataSetByValue(self, Dataset, axis, val, isAbove):\n",
    "        '''\n",
    "        根据特征索引i和连续特征值value将数据集切分\n",
    "\n",
    "        参数:\n",
    "        - Dataset: 训练数据集\n",
    "        - axis: 特征索引\n",
    "        - val: 特征值\n",
    "        - isAbove: True表示大于value,False表示小于等于value\n",
    "\n",
    "        返回:\n",
    "        - 切分后的子集\n",
    "        '''\n",
    "        subDataset = []\n",
    "        # 遍历每一行\n",
    "        for featVec in Dataset:\n",
    "            if isAbove and featVec[axis] > val:\n",
    "                reducedFeature = featVec[:axis]\n",
    "                reducedFeature.extend(featVec[axis + 1:])\n",
    "                subDataset.append(reducedFeature)\n",
    "            elif not isAbove and featVec[axis] <= val:\n",
    "                reducedFeature = featVec[:axis]\n",
    "                reducedFeature.extend(featVec[axis + 1:])\n",
    "                subDataset.append(reducedFeature)\n",
    "        return subDataset\n",
    "\n",
    "    def predict(self, inputTree,features, testVec):\n",
    "        '''\n",
    "        预测测试数据集\n",
    "\n",
    "        参数:\n",
    "        - inputTree: 训练好的决策树\n",
    "        - features: 特征列表\n",
    "        - testVec: 测试数据集\n",
    "\n",
    "        返回:\n",
    "        - 预测结果\n",
    "        '''\n",
    "        # 提取当前节点(每个决策树节点只有一个特征标签)\n",
    "        firstStr = list(inputTree.keys())[0]\n",
    "        # 提取当前节点下的子节点\n",
    "        secondDict = inputTree[firstStr]\n",
    "        # 获取当前节点的特征标签序号\n",
    "        featureIndex = features.index(firstStr)\n",
    "\n",
    "        # 遍历每个子节点\n",
    "        for key in secondDict.keys():\n",
    "            # 连续特征\n",
    "            if type(key).__name__ == 'str' and ('<=' in key or '>' in key):\n",
    "                # 去除字符串中的符号，取出阈值\n",
    "                threshold = float(key.strip('<=').strip('>'))\n",
    "                # 判断测试数据是否满足阈值\n",
    "                if key.startswith('<=') and testVec[featureIndex] <= threshold:\n",
    "                    childTree = secondDict[key]\n",
    "                    # 判断当前是不是叶节点,如果不是，继续递归\n",
    "                    if isinstance(childTree, dict):\n",
    "                        return self.predict(childTree,features, testVec)\n",
    "                    else:\n",
    "                        return childTree\n",
    "                elif key.startswith('>') and testVec[featureIndex] > threshold:\n",
    "                    childTree = secondDict[key]\n",
    "                    # 判断当前是不是叶节点\n",
    "                    if isinstance(childTree, dict):\n",
    "                        return self.predict(childTree,features, testVec)\n",
    "                    else:\n",
    "                        return childTree\n",
    "            # 离散特征\n",
    "            else:\n",
    "                # 判断测试数据是否满足取值\n",
    "                if testVec[featureIndex] == key:\n",
    "                    childTree = secondDict[key]\n",
    "                    # 判断当前是不是叶节点\n",
    "                    if isinstance(childTree, dict):\n",
    "                        return self.predict(childTree,features, testVec)\n",
    "                    else:\n",
    "                        return childTree\n",
    "        return \"Unknown !\"\n",
    "        \n",
    "    def printTree(self):\n",
    "        \"\"\"打印决策树\"\"\"\n",
    "        print(self.tree)\n",
    "\n",
    "    def calculateAccuracy(self, dataset_test, features):\n",
    "        \"\"\"计算准确率\"\"\"\n",
    "        correct_count = 0\n",
    "        for example in dataset_test:\n",
    "            label = example[-1]\n",
    "            predict_label = self.predict(self.tree, features, example)\n",
    "            if label == predict_label:\n",
    "                correct_count += 1\n",
    "        accuracy = float(correct_count) / len(dataset_test)\n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CART = CART()\n",
    "    CART.fit(dataset_train,features)\n",
    "    CART.printTree()\n",
    "    accuracy = CART.calculateAccuracy(dataset_test,features)\n",
    "    print(\"accuracy:\",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
