{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7fe7ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'纹理': {'模糊': '否', '清晰': {'密度': {'<=0.3815': '否', '>0.3815': '是'}}, '稍糊': {'触感': {'硬滑': '否', '软粘': '是'}}}}\n",
      "预测结果: 是\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "class ID3:\n",
    "    def __init__(self,pruning = None):\n",
    "        self.tree = None\n",
    "        self.pruning = pruning\n",
    "\n",
    "    def fit(self, dataset_train,dataset_test,features):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \n",
    "        参数:\n",
    "        - dataset_train: 训练数据集\n",
    "        - dataset_test: 测试数据集\n",
    "        - features: 特征列表 \n",
    "        \"\"\"\n",
    "\n",
    "        if self.pruning == None:\n",
    "            self.tree = self.createTree(dataset_train, features)\n",
    "        elif self.pruning == 'prepruning':\n",
    "            self.tree = self.prepruning(dataset_train,dataset_test, features)\n",
    "        elif self.pruning == 'postpruning':\n",
    "            self.tree = self.postpruning(dataset_train,dataset_test, features)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pruning method: %s\" % self.pruning)\n",
    "\n",
    "    def createTree(self, Dataset, features):\n",
    "        \"\"\" \n",
    "        创建决策树\n",
    "        \n",
    "        参数:\n",
    "        - Dataset: 训练数据集\n",
    "        - features: 特征列表\n",
    "        \n",
    "        返回:\n",
    "        - 决策树\n",
    "        \"\"\"\n",
    "        # 提取标签\n",
    "        classList = [example[-1] for example in Dataset]\n",
    "\n",
    "        # 如果所有样本的标签相同，则返回该标签\n",
    "        if len(set(classList)) == 1:\n",
    "            return classList[0]\n",
    "        \n",
    "        # 如果特征集为空，则返回出现次数最多的标签\n",
    "        if len(Dataset[0]) == 1:\n",
    "            return self.majorityCnt(classList)\n",
    "        \n",
    "        # 选择最优特征\n",
    "        bestFeatureIndex, bestSplitValue = self.chooseBestFeature(Dataset)\n",
    "        bestFeatureLabel = features[bestFeatureIndex]\n",
    "\n",
    "        # 创建节点\n",
    "        tree = {bestFeatureLabel: {}}\n",
    "        # 使用副本避免修改原始列表\n",
    "        subfeatures = features.copy()\n",
    "        # 删除当前特征\n",
    "        del subfeatures[bestFeatureIndex]\n",
    "        # 连续特征\n",
    "        if type(bestSplitValue).__name__ == 'float':\n",
    "            tree[bestFeatureLabel]['<=' + str(bestSplitValue)] = self.createTree(self.splitDataSetByValue(Dataset, bestFeatureIndex, bestSplitValue, False), subfeatures)\n",
    "            tree[bestFeatureLabel]['>' + str(bestSplitValue)] = self.createTree(self.splitDataSetByValue(Dataset, bestFeatureIndex, bestSplitValue, True), subfeatures)\n",
    "        # 离散特征\n",
    "        else:\n",
    "            # 取出当前特征的取值\n",
    "            featValue = [example[bestFeatureIndex] for example in Dataset]\n",
    "            uniqueVals = set(featValue)\n",
    "            # 遍历所有取值,开始递归\n",
    "            for value in uniqueVals:\n",
    "                subDataset = self.splitDataSet(Dataset, bestFeatureIndex, value)\n",
    "                tree[bestFeatureLabel][value] = self.createTree(subDataset, subfeatures)\n",
    "        return tree\n",
    "    \n",
    "    def prepruning(self, dataset_train, dataset_test, features):\n",
    "        \"\"\" \n",
    "        创建决策树(预裁剪)\n",
    "        \n",
    "        参数:\n",
    "        - dataset_train: 训练数据集\n",
    "        - dataset_test: 测试数据集\n",
    "        - features: 特征列表\n",
    "        \n",
    "        返回:\n",
    "        - 决策树\n",
    "        \"\"\"\n",
    "        # 取出所有样本的标签\n",
    "        classList = [example[-1] for example in dataset_train]\n",
    "        # 如果所有样本的标签相同，则返回该标签\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "        # 如果特征集为空，则返回出现次数最多的标签\n",
    "        if len(dataset_train[0]) == 1:\n",
    "            return self.majorityCnt(classList)\n",
    "        \n",
    "        # 计算不分裂时的准确率（叶节点多数类）\n",
    "        majority_class = self.majorityCnt(classList)\n",
    "        # print(\"majority_class:\", majority_class)\n",
    "        accuracy_before = sum(1 for ex in dataset_test if ex[-1] == majority_class) / len(dataset_test)\n",
    "        # print(\"accuracy_before:\", accuracy_before)\n",
    "\n",
    "        # 选择最优特征进行数据集划分\n",
    "        bestfeatureIndex, bestValue = self.chooseBestFeature(dataset_train)\n",
    "        bestFeatLabel = features[bestfeatureIndex]\n",
    "\n",
    "        # 计算分裂后的准确率\n",
    "        # 连续特征\n",
    "        if type(bestValue).__name__ == 'float':\n",
    "            splitRightNums = 0\n",
    "            # 将训练集划分为左右两部分，计算出每个分叉的多数类\n",
    "            subLeftDataset = self.splitDataSetByValue(dataset_train, bestfeatureIndex, bestValue, False)\n",
    "            subRightDataset = self.splitDataSetByValue(dataset_train, bestfeatureIndex, bestValue, True)\n",
    "            majority_class_left = self.majorityCnt([ex[-1] for ex in subLeftDataset])\n",
    "            majority_class_right = self.majorityCnt([ex[-1] for ex in subRightDataset])\n",
    "            # 将测试集划分为左右两部分，计算出每个分叉的正确分类个数\n",
    "            subLeftDataset_test = self.splitDataSetByValue(dataset_test, bestfeatureIndex, bestValue, False)\n",
    "            subRightDataset_test = self.splitDataSetByValue(dataset_test, bestfeatureIndex, bestValue, True)\n",
    "            splitRightNums = sum(1 for ex in subLeftDataset_test if ex[-1] == majority_class_left) + \\\n",
    "                            sum(1 for ex in subRightDataset_test if ex[-1] == majority_class_right)\n",
    "            accuracy_after = splitRightNums / len(dataset_test)\n",
    "        # 离散特征\n",
    "        else:\n",
    "            splitRightNums = 0\n",
    "            featValue = [example[bestfeatureIndex] for example in dataset_train]\n",
    "            uniqueVals = set(featValue) \n",
    "            # 统计分裂后每一个分叉中的正确分类个数\n",
    "            for value in uniqueVals:\n",
    "                subDataSet = self.splitDataSet(dataset_train, bestfeatureIndex, value)\n",
    "                majority_class_sub = self.majorityCnt([ex[-1] for ex in subDataSet])\n",
    "                subDataSet_test = self.splitDataSet(dataset_test, bestfeatureIndex, value)\n",
    "                splitRightNums += sum(1 for ex in subDataSet_test if ex[-1] == majority_class_sub)\n",
    "            accuracy_after = splitRightNums / len(dataset_test)\n",
    "            # print(\"accuracy_after:\", accuracy_after)\n",
    "\n",
    "        # 比较准确率，决定是否继续分裂(根据奥卡姆剃刀准则，准确率相等时不分裂)\n",
    "        if (accuracy_after > accuracy_before):\n",
    "            myTree = {bestFeatLabel: {}}\n",
    "        else:\n",
    "        # 返回分裂前最多的标签\n",
    "            return majority_class\n",
    "\n",
    "        # 使用副本避免修改原始列表\n",
    "        subfeatures = features.copy()  \n",
    "        del subfeatures[bestfeatureIndex]\n",
    "\n",
    "        # 连续特征\n",
    "        if type(bestValue).__name__ == 'float':\n",
    "\n",
    "            myTree[bestFeatLabel]['<=' + str(bestValue)] = self.prepruning(subLeftDataset, subLeftDataset_test, subfeatures)\n",
    "            myTree[bestFeatLabel]['>' + str(bestValue)] = self.prepruning(subRightDataset, subRightDataset_test, subfeatures)\n",
    "        # 离散特征\n",
    "        else:\n",
    "            # 递归每一个特征值\n",
    "            for value in uniqueVals:\n",
    "                subDataSet = self.splitDataSet(dataset_train, bestfeatureIndex, value)\n",
    "                # 测试集也需要划分\n",
    "                subDataSet_test = self.splitDataSet(dataset_test, bestfeatureIndex, value)\n",
    "                myTree[bestFeatLabel][value] = self.prepruning(subDataSet, subDataSet_test,subfeatures)\n",
    "                \n",
    "        return myTree\n",
    "    \n",
    "    def postpruning(self, dataset_train, dataset_test, features):\n",
    "        \"\"\" \n",
    "        创建决策树(后裁剪)\n",
    "        \n",
    "        参数:\n",
    "        - dataset_train: 训练数据集\n",
    "        - dataset_test: 测试数据集\n",
    "        - features: 特征列表\n",
    "        \n",
    "        返回:\n",
    "        - 决策树\n",
    "        \"\"\"\n",
    "        # 取出所有样本的标签\n",
    "        classList = [example[-1] for example in dataset_train]\n",
    "        # 如果所有样本的标签相同，则返回该标签\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "        # 如果特征集为空，则返回出现次数最多的标签\n",
    "        if len(dataset_train[0]) == 1:\n",
    "            return self.majorityCnt(classList)\n",
    "\n",
    "        # 选择最优特征进行数据集划分\n",
    "        bestfeatureIndex, bestValue = self.chooseBestFeature(dataset_train)\n",
    "        bestFeatLabel = features[bestfeatureIndex]\n",
    "\n",
    "        # 创建节点\n",
    "        myTree = {bestFeatLabel: {}}\n",
    "\n",
    "        # 使用副本避免修改原始列表\n",
    "        subfeatures = features.copy()  \n",
    "        del subfeatures[bestfeatureIndex]\n",
    "\n",
    "        # 连续特征\n",
    "        if type(bestValue).__name__ == 'float':\n",
    "            subLeftDataset = self.splitDataSetByValue(dataset_train, bestfeatureIndex, bestValue, False)\n",
    "            subRightDataset = self.splitDataSetByValue(dataset_train, bestfeatureIndex, bestValue, True)\n",
    "            subLeftDataset_test = self.splitDataSetByValue(dataset_test, bestfeatureIndex, bestValue, False)\n",
    "            subRightDataset_test = self.splitDataSetByValue(dataset_test, bestfeatureIndex, bestValue, True)\n",
    "            myTree[bestFeatLabel]['<=' + str(bestValue)] = self.postpruning(subLeftDataset, subLeftDataset_test, subfeatures)\n",
    "            myTree[bestFeatLabel]['>' + str(bestValue)] = self.postpruning(subRightDataset, subRightDataset_test, subfeatures)\n",
    "        # 离散特征\n",
    "        else:\n",
    "            # 递归每一个特征值\n",
    "            featValue = [example[bestfeatureIndex] for example in dataset_train]\n",
    "            uniqueVals = set(featValue) \n",
    "            for value in uniqueVals:\n",
    "                subDataSet = self.splitDataSet(dataset_train, bestfeatureIndex, value)\n",
    "                # 测试集也需要划分\n",
    "                subDataSet_test = self.splitDataSet(dataset_test, bestfeatureIndex, value)\n",
    "                myTree[bestFeatLabel][value] = self.postpruning(subDataSet, subDataSet_test,subfeatures)\n",
    "\n",
    "        # 在每一条分支到达叶节点时，开始后裁剪(避免生成完整的树后再进行裁剪，提高效率)\n",
    "        # 计算未裁剪树的准确率\n",
    "        unpruneRightNums = 0\n",
    "        # 连续特征\n",
    "        if type(bestValue).__name__ == 'float':\n",
    "            majority_class_left = self.majorityCnt([ex[-1] for ex in subLeftDataset])\n",
    "            majority_class_right = self.majorityCnt([ex[-1] for ex in subRightDataset])\n",
    "            unpruneRightNums = sum(1 for ex in dataset_test if ex[-1] == majority_class_left) + \\\n",
    "                            sum(1 for ex in dataset_test if ex[-1] == majority_class_right)\n",
    "        # 离散特征\n",
    "        else:\n",
    "            for value in uniqueVals:\n",
    "                subDataSet = self.splitDataSet(dataset_train, bestfeatureIndex, value)\n",
    "                majority_class_sub = self.majorityCnt([ex[-1] for ex in subDataSet])\n",
    "                subDataSet_test = self.splitDataSet(dataset_test, bestfeatureIndex, value)\n",
    "                unpruneRightNums += sum(1 for ex in subDataSet_test if ex[-1] == majority_class_sub)\n",
    "        accuracy_before = unpruneRightNums / len(dataset_test)\n",
    "        # 计算裁剪后的准确率\n",
    "        majority_class = self.majorityCnt(classList)\n",
    "        accuracy_after = sum(1 for ex in dataset_test if ex[-1] == majority_class) / len(dataset_test)\n",
    "        # 比较准确率，决定是否继续分裂(根据奥卡姆剃刀准则，准确率相等时裁剪)\n",
    "        if (accuracy_after >= accuracy_before):\n",
    "            return majority_class\n",
    "        return myTree\n",
    "\n",
    "    def majorityCnt(self, classList):\n",
    "        \"\"\"返回最多的标签\"\"\"\n",
    "        # 统计标签出现的次数\n",
    "        label_count = {}\n",
    "        for label in classList:\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        # 降序排序[(类标签,出现次数),(),()]\n",
    "        sortedclassCount = sorted(label_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        return sortedclassCount[0][0]\n",
    "\n",
    "    def chooseBestFeature(self,Dataset):\n",
    "        \"\"\"通过信息增益选择最优特征\"\"\"\n",
    "        featureNum = len(Dataset[0]) - 1\n",
    "        bestInfoGain = 0.0\n",
    "        bestSplitValue = 0\n",
    "        bestFeatureIndex = -1\n",
    "        baseEntropy = self.calculateEntropy(Dataset)\n",
    "\n",
    "        # 遍历所有特征\n",
    "        for i in range(featureNum):\n",
    "            # 提取当前特征下的取值\n",
    "            featureValues = [example[i] for example in Dataset]\n",
    "            # 连续特征\n",
    "            if type(featureValues[0]).__name__ == 'float':\n",
    "                # 对特征值进行排序\n",
    "                sortedFeatureValues = sorted(featureValues)\n",
    "                # 计算分割值（取相邻两个取值的中点）\n",
    "                splitList = []\n",
    "                for j in range(len(sortedFeatureValues) - 1):\n",
    "                    splitList.append((sortedFeatureValues[j] + sortedFeatureValues[j + 1]) / 2.0)\n",
    "                # 遍历所有分割值,相当于做二分类\n",
    "                for splitValue in splitList:\n",
    "                    currentEntropy = 0.0\n",
    "                    subDataset1 = self.splitDataSetByValue(Dataset, i, splitValue, True)\n",
    "                    subDataset2 = self.splitDataSetByValue(Dataset, i, splitValue, False)\n",
    "                    prob1 = len(subDataset1) / float(len(Dataset))\n",
    "                    prob2 = len(subDataset2) / float(len(Dataset))\n",
    "                    currentEntropy  = prob1 * self.calculateEntropy(subDataset1) + prob2 * self.calculateEntropy(subDataset2)\n",
    "                    # 计算信息增益\n",
    "                    infoGain = baseEntropy - currentEntropy\n",
    "                    if (infoGain > bestInfoGain):\n",
    "                        bestInfoGain = infoGain\n",
    "                        bestFeatureIndex = i\n",
    "                        bestSplitValue = splitValue\n",
    "            # 离散特征\n",
    "            else:\n",
    "                uniqueValues = set(featureValues)\n",
    "                currentEntropy = 0.0\n",
    "                # 遍历所有取值\n",
    "                for value in uniqueValues:\n",
    "                    subDataset = self.splitDataSet(Dataset, i, value)\n",
    "                    prob = len(subDataset) / len(Dataset)\n",
    "                    currentEntropy += prob * self.calculateEntropy(subDataset)\n",
    "                # 计算信息增益\n",
    "                infoGain = baseEntropy - currentEntropy\n",
    "                if (infoGain >= bestInfoGain):\n",
    "                    bestInfoGain = infoGain\n",
    "                    bestFeatureIndex = i\n",
    "                    bestSplitValue = None\n",
    "        \n",
    "        return bestFeatureIndex, bestSplitValue\n",
    "\n",
    "    def calculateEntropy(self, Dataset):\n",
    "        \"\"\"计算信息熵,公式(4.1)\"\"\"\n",
    "        sample_num = len(Dataset)\n",
    "        # 统计标签出现的次数\n",
    "        label_count = {}\n",
    "        for featVec in Dataset:\n",
    "            label = featVec[-1]\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        \n",
    "        # 计算信息熵\n",
    "        entropy = 0.0\n",
    "        for count in label_count.values():\n",
    "            prob = float(count) / sample_num\n",
    "            entropy -= prob * np.log2(prob)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def splitDataSet(self,Dataset, axis, val):\n",
    "        '''\n",
    "        根据特征索引i和离散特征值value将数据集切分\n",
    "\n",
    "        参数:\n",
    "        - Dataset: 训练数据集\n",
    "        - axis: 特征索引\n",
    "        - val: 特征值\n",
    "\n",
    "        返回:\n",
    "        - 切分后的子集\n",
    "        '''\n",
    "        subDataset = []\n",
    "        # 遍历每一行\n",
    "        for featVec in Dataset:\n",
    "            if featVec[axis] == val:\n",
    "                reducedFeature = featVec[:axis]\n",
    "                reducedFeature.extend(featVec[axis + 1:])\n",
    "                subDataset.append(reducedFeature)\n",
    "        return subDataset\n",
    "\n",
    "    def splitDataSetByValue(self, Dataset, axis, val, isAbove):\n",
    "        '''\n",
    "        根据特征索引i和连续特征值value将数据集切分\n",
    "\n",
    "        参数:\n",
    "        - Dataset: 训练数据集\n",
    "        - axis: 特征索引\n",
    "        - val: 特征值\n",
    "        - isAbove: True表示大于value,False表示小于等于value\n",
    "\n",
    "        返回:\n",
    "        - 切分后的子集\n",
    "        '''\n",
    "        subDataset = []\n",
    "        # 遍历每一行\n",
    "        for featVec in Dataset:\n",
    "            if isAbove and featVec[axis] > val:\n",
    "                reducedFeature = featVec[:axis]\n",
    "                reducedFeature.extend(featVec[axis + 1:])\n",
    "                subDataset.append(reducedFeature)\n",
    "            elif not isAbove and featVec[axis] <= val:\n",
    "                reducedFeature = featVec[:axis]\n",
    "                reducedFeature.extend(featVec[axis + 1:])\n",
    "                subDataset.append(reducedFeature)\n",
    "        return subDataset\n",
    "\n",
    "    def predict(self, inputTree,features, testVec):\n",
    "        '''\n",
    "        预测测试数据集\n",
    "\n",
    "        参数:\n",
    "        - inputTree: 训练好的决策树\n",
    "        - features: 特征列表\n",
    "        - testVec: 测试数据集\n",
    "\n",
    "        返回:\n",
    "        - 预测结果\n",
    "        '''\n",
    "        # 提取当前节点(每个决策树节点只有一个特征标签)\n",
    "        firstStr = list(inputTree.keys())[0]\n",
    "        # 提取当前节点下的子节点\n",
    "        secondDict = inputTree[firstStr]\n",
    "        # 获取当前节点的特征标签序号\n",
    "        featureIndex = features.index(firstStr)\n",
    "\n",
    "        # 遍历每个子节点\n",
    "        for key in secondDict.keys():\n",
    "            # 连续特征\n",
    "            if type(key).__name__ == 'str' and ('<=' in key or '>' in key):\n",
    "                # 去除字符串中的符号，取出阈值\n",
    "                threshold = float(key.strip('<=').strip('>'))\n",
    "                # 判断测试数据是否满足阈值\n",
    "                if key.startswith('<=') and testVec[featureIndex] <= threshold:\n",
    "                    childTree = secondDict[key]\n",
    "                    # 判断当前是不是叶节点,如果不是，继续递归\n",
    "                    if isinstance(childTree, dict):\n",
    "                        return self.predict(childTree,features, testVec)\n",
    "                    else:\n",
    "                        return childTree\n",
    "                elif key.startswith('>') and testVec[featureIndex] > threshold:\n",
    "                    childTree = secondDict[key]\n",
    "                    # 判断当前是不是叶节点\n",
    "                    if isinstance(childTree, dict):\n",
    "                        return self.predict(childTree,features, testVec)\n",
    "                    else:\n",
    "                        return childTree\n",
    "            # 离散特征\n",
    "            else:\n",
    "                # 判断测试数据是否满足取值\n",
    "                if testVec[featureIndex] == key:\n",
    "                    childTree = secondDict[key]\n",
    "                    # 判断当前是不是叶节点\n",
    "                    if isinstance(childTree, dict):\n",
    "                        return self.predict(childTree,features, testVec)\n",
    "                    else:\n",
    "                        return childTree\n",
    "        return \"Unknown !\"\n",
    "        \n",
    "    def printTree(self):\n",
    "        \"\"\"打印决策树\"\"\"\n",
    "        print(self.tree)\n",
    "\n",
    "    def calculateAccuracy(self, dataset_test, features):\n",
    "        \"\"\"计算准确率\"\"\"\n",
    "        correct_count = 0\n",
    "        for example in dataset_test:\n",
    "            label = example[-1]\n",
    "            predict_label = self.predict(self.tree, features, example)\n",
    "            if label == predict_label:\n",
    "                correct_count += 1\n",
    "        accuracy = float(correct_count) / len(dataset_test)\n",
    "        return accuracy\n",
    "      \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 加载数据\n",
    "    df = pd.DataFrame(pd.read_csv(\"../Data/watermelon3.0.csv\", encoding=\"ansi\"))\n",
    "    df.drop(labels=[\"编号\"], axis=1, inplace=True)  # 删除编号这一列，inplace=True表示直接在原对象修改\n",
    "    # 转化为列表\n",
    "    dataset = df.values.tolist()\n",
    "    # 特征列表\n",
    "    features = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖率']\n",
    "    # 创建决策树\n",
    "    ID3_model = ID3()\n",
    "    ID3_model.fit(dataset,None,features)\n",
    "    # 打印决策树\n",
    "    ID3_model.printTree()\n",
    "    # 测试数据\n",
    "    test_data = ['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.6, 0.3]\n",
    "    # 预测结果\n",
    "    result = ID3_model.predict(ID3_model.tree, features, test_data)\n",
    "    print(\"预测结果:\", result)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43db7da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'花瓣长度': {'<=2.5999999999999996': 0.0, '>2.5999999999999996': {'花瓣宽度': {'<=1.7': {'花萼宽度': {'<=2.6': 1.0, '>2.6': 1.0}}, '>1.7': 2.0}}}}\n",
      "准确度： 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载鸢尾花\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=30)\n",
    "# 合并特征和标签\n",
    "dataset_train = np.c_[X_train, y_train]\n",
    "dataset_test = np.c_[X_test, y_test]\n",
    "# 转化为列表\n",
    "dataset_train = dataset_train.tolist()\n",
    "dataset_test = dataset_test.tolist()\n",
    "# 特征列表\n",
    "features = ['花萼长度','花萼宽度','花瓣长度','花瓣宽度']\n",
    "# 训练\n",
    "ID3_model = ID3(pruning='postpruning')\n",
    "ID3_model.fit(dataset_train,dataset_test,features)\n",
    "# 打印\n",
    "ID3_model.printTree()\n",
    "# 计算准确度\n",
    "accuracy = ID3_model.calculateAccuracy(dataset_test,features)\n",
    "print(\"准确度：\",accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
