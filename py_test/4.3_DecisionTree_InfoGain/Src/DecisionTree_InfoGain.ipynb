{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5b68d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'纹理': {'模糊': '否', '清晰': {'密度': {'<=0.3815': '否', '>0.3815': '是'}}, '稍糊': {'触感': {'软粘': '是', '硬滑': '否'}}}}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "#创建决策树\n",
    "def createTree(dataset, features):\n",
    "    '''\n",
    "    @brief: create a decision tree by using the ID3 algorithm\n",
    "    @param dataset: the dataset to be used for training\n",
    "    @param features: the features to be used for training\n",
    "    @return: the decision tree\n",
    "    '''\n",
    "    # 取出所有样本的标签\n",
    "    classList = [example[-1] for example in dataset]\n",
    "    # 如果所有样本的标签相同，则返回该标签\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 如果特征集为空，则返回出现次数最多的标签\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    # 选择最优特征进行数据集划分\n",
    "    bestfeatureIndex, bestValue = chooseBestFeatureToSplit(dataset)\n",
    "    bestFeatLabel = features[bestfeatureIndex]\n",
    "    \n",
    "    # 创建节点\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    # 使用副本避免修改原始列表\n",
    "    subfeatures = features.copy()  \n",
    "    # 连续特征\n",
    "    if type(bestValue).__name__ == 'float':\n",
    "        myTree[bestFeatLabel]['<=' + str(bestValue)] = createTree(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, True), subfeatures)\n",
    "        myTree[bestFeatLabel]['>' + str(bestValue)] = createTree(splitDataSetByValue(dataset, bestfeatureIndex, bestValue, False), subfeatures)\n",
    "    # 离散特征\n",
    "    else:\n",
    "        # 去除当前特征\n",
    "        del subfeatures[bestfeatureIndex]  # 在副本中删除当前特征\n",
    "        # 取出当前特征的取值\n",
    "        featValue = [example[bestfeatureIndex] for example in dataset]\n",
    "        uniqueVals = set(featValue)\n",
    "        # 递归每一个特征值\n",
    "        for value in uniqueVals:\n",
    "            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset, bestfeatureIndex, value), subfeatures)\n",
    "    return myTree\n",
    "\n",
    "# 计算类别中出现次数最多的元素\n",
    "def majorityCnt(classList):\n",
    "    # 创建一个字典{类标签:出现次数}\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    # 降序排序[(类标签,出现次数),(),()]\n",
    "    sortedclassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedclassCount[0][0]\n",
    "\n",
    "# 选择最优特征进行数据集划分\n",
    "def chooseBestFeatureToSplit(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataset)\n",
    "    bestInfoGain = 0\n",
    "    bestFeature = -1\n",
    "    bestValue = 0\n",
    "    # 遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 取出第i个特征\n",
    "        featList = [example[i] for example in dataset]\n",
    "        # 连续特征\n",
    "        if type(featList[0]).__name__ == 'float':\n",
    "            # 排序\n",
    "            sortedfeatList = sorted(featList)\n",
    "            splitList = []\n",
    "            # 计算切分点\n",
    "            for j in range(len(sortedfeatList) - 1):\n",
    "                splitVal = (sortedfeatList[j] + sortedfeatList[j + 1]) / 2.0\n",
    "                splitList.append(splitVal)\n",
    "            # 计算信息增益\n",
    "            for val in set(splitList):\n",
    "                newEntropy = 0\n",
    "                subDataSet1 = splitDataSetByValue(dataset, i, val, True)\n",
    "                subDataSet2 = splitDataSetByValue(dataset, i, val, False)\n",
    "                prob1 = len(subDataSet1) / float(len(dataset))\n",
    "                newEntropy += prob1 * calcShannonEnt(subDataSet1)\n",
    "                prob2 = len(subDataSet2) / float(len(dataset))\n",
    "                newEntropy += prob2 * calcShannonEnt(subDataSet2)\n",
    "                infoGain = baseEntropy - newEntropy\n",
    "                if (infoGain > bestInfoGain):\n",
    "                    bestInfoGain = infoGain\n",
    "                    bestFeature = i\n",
    "                    bestValue = val\n",
    "        else:\n",
    "            # 离散特征\n",
    "            uniqueVals = set(featList)\n",
    "            newEntropy = 0\n",
    "            # 遍历所有取值\n",
    "            for val in uniqueVals:\n",
    "                subDataSet = splitDataSet(dataset, i, val)\n",
    "                prob = len(subDataSet) / float(len(dataset))\n",
    "                newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "            infoGain = baseEntropy - newEntropy\n",
    "            if (infoGain > bestInfoGain):\n",
    "                bestInfoGain = infoGain\n",
    "                bestFeature = i\n",
    "                bestValue = None\n",
    "    return bestFeature, bestValue\n",
    "\n",
    "# 根据特征值划分数据集\n",
    "def splitDataSet(dataset, axis, val):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if featVec[axis] == val:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis + 1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# 根据特征值和方向划分数据集\n",
    "def splitDataSetByValue(dataset, axis, val, direction):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if direction:\n",
    "            if featVec[axis] <= val:\n",
    "                reducedFeatVec = featVec[:axis]\n",
    "                reducedFeatVec.extend(featVec[axis + 1:])\n",
    "                retDataSet.append(reducedFeatVec)\n",
    "        else:\n",
    "            if featVec[axis] > val:\n",
    "                reducedFeatVec = featVec[:axis]\n",
    "                reducedFeatVec.extend(featVec[axis + 1:])\n",
    "                retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# 计算数据集信息熵\n",
    "def calcShannonEnt(dataset):\n",
    "    numexamples = len(dataset)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataset:\n",
    "        currentlabel = featVec[-1]\n",
    "        if currentlabel not in labelCounts.keys():\n",
    "            labelCounts[currentlabel] = 0\n",
    "        labelCounts[currentlabel] += 1\n",
    "\n",
    "    shannonEnt = 0\n",
    "    for key in labelCounts:\n",
    "        prop = float(labelCounts[key]) / numexamples\n",
    "        shannonEnt -= prop * log(prop, 2)\n",
    "    return shannonEnt\n",
    "\n",
    "def predict(inputTree, features, testVec):\n",
    "    '''\n",
    "    @brief: predict the label of a test vector using a decision tree\n",
    "    @param inputTree: the decision tree to be used for prediction\n",
    "    @param features: the features to be used for training\n",
    "    @param testVec: the test vector to be predicted\n",
    "    @return: the predicted label of the test vector\n",
    "    '''\n",
    "    # 提取当前节点\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    # 提取当前节点下的子节点\n",
    "    secondDict = inputTree[firstStr]\n",
    "    # 获取当前节点的特征标签\n",
    "    featureIndex = features.index(firstStr)\n",
    "\n",
    "    for key in secondDict.keys():\n",
    "        # 处理连续特征（如 \"<=0.5\"）\n",
    "        if type(key).__name__ == 'str' and ('<=' in key or '>' in key):\n",
    "            # 移除字符串中的符号，取出阈值\n",
    "            threshold = float(key.replace('<=', '').replace('>', ''))\n",
    "            # 当前特征值小于等于阈值，则进入左子树\n",
    "            if key.startswith('<=') and testVec[featureIndex] <= threshold:\n",
    "                childTree = secondDict[key]\n",
    "                # 判断是否为内部节点，若是，则表示不是叶子节点，继续递归\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "            elif key.startswith('>') and testVec[featureIndex] > threshold:\n",
    "                childTree = secondDict[key]\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "        # 处理离散特征（如 \"硬滑\"）\n",
    "        else:\n",
    "            if testVec[featureIndex] == key:\n",
    "                childTree = secondDict[key]\n",
    "                if isinstance(childTree, dict):\n",
    "                    return predict(childTree, features, testVec)\n",
    "                else:\n",
    "                    return childTree\n",
    "    # 若未匹配任何分支\n",
    "    return \"未知类别\"  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 构建数据集\n",
    "    df = pd.DataFrame(pd.read_csv(\"../Data/watermelon3.0.csv\", encoding=\"ansi\"))\n",
    "    df.drop(labels=[\"编号\"], axis=1, inplace=True)  # 删除编号这一列，inplace=True表示直接在原对象修改\n",
    "    # 转化为列表\n",
    "    dataset = df.values.tolist()\n",
    "    # 打印原始数据\n",
    "    # for i in range(len(dataset)):\n",
    "    #     print(dataset[i])\n",
    "    # 标签\n",
    "    labels = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖率']\n",
    "    # 构建决策树\n",
    "    myTree = createTree(dataset, labels)\n",
    "    # 打印决策树\n",
    "    print(myTree)\n",
    "    # 测试数据\n",
    "    testVec = ['青绿','硬挺','清脆','稍糊','平坦','软粘',0.243,0.267]\n",
    "    # 预测结果\n",
    "    result = predict(myTree, labels, testVec)\n",
    "    # print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
